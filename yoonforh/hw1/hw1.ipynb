{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Assignments\n",
    "1. Homework 1: Imitation learning (control via supervised learning)\n",
    "2. Homework 2: Policy gradients (“REINFORCE”)\n",
    "3. Homework 3: Q learning and actor-critic algorithms\n",
    "4. Homework 4: Model-based reinforcement learning\n",
    "5. Homework 5: Advanced model-free RL algorithms\n",
    "6. Final project: Research-level project of your choice (form a group of up to 2-3 students, you’re welcome to start early!)\n",
    "\n",
    "##### Emacs IPython Notebook Commands/Keybinds\n",
    "* http://millejoh.github.io/emacs-ipython-notebook/#commands-keybinds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Homework 1 Imitation Learning\n",
    "\n",
    "Instead of trying to produce a program to simulate the adult mind, why not rather try to produce one which simulates the child's?\n",
    "\n",
    "If this were then subjected to an appropriate course of education one would obtain the adult brain.\n",
    "\n",
    "\\- Alan Turing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Behavioral Cloning\n",
    "\n",
    "1. The starter code provides an expert policy for each of the MuJoCo tasks in OpenAI Gym (See run expert.py). Generate roll-outs from the provided policies, and implement behavioral cloning. => expert_data/XXX.pkl\n",
    "\n",
    "2. Run behavioral cloning (BC) and report results on two tasks\n",
    " – one task where a behavioral cloning agent achieves comparable performance to the expert,\n",
    " and one task where it does not.\n",
    " When providing results, report the mean and standard deviation of the return over multiple rollouts in a table, and state which task was used.\n",
    " Be sure to set up a fair comparison, in terms of network size, amount of data, and number of training iterations, and provide these details (and any others you feel are appropriate) in the table caption.\n",
    "\n",
    "3. Experiment with one hyperparameter that affects the performance of the behavioral cloning agent, such as\n",
    "* the number of demonstrations,\n",
    "* the number of training epochs,\n",
    "* the variance of the expert policy, or\n",
    "* something that you come up with yourself.\n",
    " For one of the tasks used in the previous question, show a graph of how the BC agent’s performance varies with the value of this hyperparameter, and state the hyperparameter and a brief rationale for why you chose it in the caption for the graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "gym_envs = ('Ant-v2', 'Hopper-v2', 'Reacher-v2', 'HalfCheetah-v2', 'Humanoid-v2', 'Walker2d-v2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## running experts\n",
    "\n",
    "run experts of each gym environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# run expert\n",
    "\n",
    "import sys, os\n",
    "import datetime as dt\n",
    "import run_expert\n",
    "\n",
    "# https://ipython.readthedocs.io/en/stable/interactive/magics.html\n",
    "# path=os.environ['PATH']\n",
    "# %env PATH='/usr/local/bin:'+path\n",
    "\n",
    "RENDER = False\n",
    "NUM_ROLLOUTS = 20\n",
    "\n",
    "for gym_env in gym_envs :\n",
    "    sys.argv = ['run_expert.py', 'experts/' + gym_env + '.pkl', gym_env, '--num_rollouts', str(NUM_ROLLOUTS) ]\n",
    "    if RENDER :\n",
    "        sys.argv.append('--render')\n",
    "    run_expert.main()\n",
    "    print('finished run_expert ', gym_env, 'at', dt.datetime.now())\n",
    "\n",
    "print('finished run_expert on all gym_envs at', dt.datetime.now())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## train on each envs\n",
    "\n",
    "using the data gathered by expert policy\n",
    "\n",
    "* environment details : https://github.com/openai/gym/tree/master/gym/envs/mujoco/assets\n",
    "* source codes of each environments : https://github.com/openai/gym/blob/master/gym/envs/mujoco/\n",
    "* reference for an HW1 implementation :  https://hollygrimm.com/rl_bc\n",
    "\n",
    "for the regressor\n",
    "input : observation\n",
    "output : action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from load_policy import load_policy\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_expert_data(gymenv) -> (np.array, np.array) : # observations, actions\n",
    "    with open(os.path.join('expert_data', gymenv + '.pkl'), 'rb') as f :\n",
    "        expert_data = pk.load(f)\n",
    "        return expert_data['observations'], expert_data['actions']\n",
    "\n",
    "def load_expert_policy_fn(gymenv) :\n",
    "    return load_policy('experts/' + gymenv + '.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Behavior Cloning\n",
    "\n",
    "1. generate rollouts(= expert data) with expert policy (and record the returns)\n",
    "2. learn the rollouts changing some environments (network size, amount of data, and number of training iterations, ...)\n",
    "3. generate rollouts several times according to each policies learned above and show the returns in a table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from enum import Enum, IntEnum\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime as dt\n",
    "import time\n",
    "import math\n",
    "\n",
    "default_model_config = dict(neurons = [400, 200, 100],\n",
    "                            activation = tf.nn.elu, # Using ReLu, which is a discontinuous function, may raise issues. Try using other activation functions, such as tanh or sigmoid.\n",
    "                            last_activation = None, # final layer activation function. default is no activation\n",
    "                            optimizer = tf.train.AdadeltaOptimizer, # tf.train.AdamOptimizer, tf.train.ProximalAdagradOptimizer\n",
    "                            cost_function = tf.losses.mean_squared_error, # tf.losses.huber_loss (robust to outlier)\n",
    "                            measure_function = 'r_squared', # 'smape' means symmetric_mean_absolute_percentage_error\n",
    ")\n",
    "\n",
    "default_train_config = dict(start_learning_rate = 0.001,\n",
    "                            # minimum_learning_rate = 0.000001,\n",
    "                            num_epochs = 1000,\n",
    "                            batch_size = 100, # 500,\n",
    "                            keep_prob = 0.9, # for training only (dropout)\n",
    "                            validationset_percent = 0.2, # by default 20 percent is validation set\n",
    "                            break_accuracy = -1.0, # 0.999, # -1.0\n",
    "                            early_stopping_epoch_on_max_no_decrease = 20, # 100,\n",
    "                            shuffle_samples_epochs = 10, # shuffle samples per given epochs considering performance. -1 means no shuffling\n",
    "                            check_accuracy_epochs = 200, # 5000,\n",
    "                            use_tboard = True,\n",
    "                            print_cost_interval = 500,\n",
    "                            print_trained_model = False,\n",
    "                            )\n",
    "\n",
    "class BehavioralCloning(object) :\n",
    "    default_random_seed = 777\n",
    "\n",
    "    def __init__(self,\n",
    "                 X_shape = None, # X shape as list\n",
    "                 Y_shape = None, # Y shape as list\n",
    "                 model_config = default_model_config,\n",
    "                 scope_name = '',\n",
    "                 restore_mode=False,\n",
    "                 session=None) :\n",
    "        self.model_config = model_config\n",
    "        self.restore_mode = restore_mode\n",
    "        self.scope_name = scope_name\n",
    "        self.X_shape = list(X_shape)\n",
    "        self.X_shape[0] = None\n",
    "        self.Y_shape = list(Y_shape)\n",
    "        self.Y_shape[0] = None\n",
    "\n",
    "        tf.set_random_seed(BehavioralCloning.default_random_seed)  # reproducibility\n",
    "        np.random.seed(BehavioralCloning.default_random_seed)\n",
    "\n",
    "        # Launch new session before graph init\n",
    "        # interactive session will declare itself as a default session and won't be closed on context destroy (so, should explicity call sess.close()\n",
    "        if session is None :\n",
    "            tf.reset_default_graph()\n",
    "            self.session = tf.InteractiveSession()\n",
    "        else :\n",
    "            self.session = session\n",
    "        self._build_network()\n",
    "\n",
    "    def _build_network(self) :\n",
    "        g = tf.get_default_graph()\n",
    "\n",
    "        # build the network\n",
    "        with g.as_default(), self.session.as_default() :\n",
    "            self.X = tf.placeholder(tf.float32, shape=self.X_shape, name='X')\n",
    "            self.Y = tf.placeholder(tf.float32, shape=self.Y_shape, name='Y')\n",
    "            self.p_keep_prob = tf.placeholder(tf.float32, name='p_keep_prob')\n",
    "            self.p_training = tf.placeholder(tf.bool, name='p_training')\n",
    "            self.p_lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "            with tf.variable_scope(self.scope_name + '-dnn', reuse=tf.AUTO_REUSE) as scope:\n",
    "                neurons = self.model_config['neurons']\n",
    "                layer = self.X\n",
    "                for i in range(len(neurons)) :\n",
    "                    neuron = neurons[i]\n",
    "\n",
    "                    layer = tf.layers.dense(layer, neuron,\n",
    "                                            kernel_initializer = tf.contrib.layers.xavier_initializer(seed=BehavioralCloning.default_random_seed),\n",
    "                                            activation=self.model_config['activation'],\n",
    "                                            name = 'layer-' + str(i))\n",
    "                    layer = tf.layers.dropout(layer, rate=1-self.p_keep_prob, training=self.p_training)\n",
    "                n_output = self.Y_shape[1]\n",
    "                layer = tf.layers.dense(layer, n_output,\n",
    "                                        kernel_initializer = tf.contrib.layers.xavier_initializer(seed=BehavioralCloning.default_random_seed),\n",
    "                                        activation=self.model_config['last_activation'],\n",
    "                                        name = 'layer-last')\n",
    "                    \n",
    "\n",
    "                self.hypothesis = layer\n",
    "                cost_fn = self.model_config['cost_function']\n",
    "                self.cost = cost_fn(self.Y, self.hypothesis)\n",
    "                tf.summary.scalar(\"cost\", self.cost)\n",
    "                measure_alg = self.model_config['measure_function']\n",
    "                if measure_alg == 'r_squared' :\n",
    "                    self.measure = self.r_squared(self.Y, self.hypothesis)\n",
    "                elif measure_alg == 'smape' :\n",
    "                    self.measure = self.smape(self.Y, self.hypothesis)\n",
    "                else :\n",
    "                    self.measure = None\n",
    "                optimizer_fn = self.model_config['optimizer']\n",
    "                opt = optimizer_fn(learning_rate=self.p_lr)\n",
    "                self.objective_tensor = opt.minimize(self.cost)\n",
    "\n",
    "            if not self.restore_mode :\n",
    "                self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    def train(self, X, Y, train_config = default_train_config) :\n",
    "        learning_rate = train_config['start_learning_rate']\n",
    "        num_epochs = train_config['num_epochs']\n",
    "        keep_prob = train_config['keep_prob']\n",
    "        batch_size = train_config['batch_size']\n",
    "        vset_percent = train_config['validationset_percent']\n",
    "        break_accuracy = train_config['break_accuracy']\n",
    "        check_accuracy_epochs = train_config['check_accuracy_epochs']\n",
    "        early_stopping_epoch_on_max_no_decrease = train_config['early_stopping_epoch_on_max_no_decrease']\n",
    "        print_cost_interval = train_config['print_cost_interval']\n",
    "        shuffle_samples_epochs = train_config['shuffle_samples_epochs']\n",
    "        use_tboard = train_config['use_tboard']\n",
    "\n",
    "        training_costs = np.zeros(num_epochs, dtype=np.float32)\n",
    "        validation_costs = np.zeros(num_epochs, dtype=np.float32)\n",
    "        validation_measures = np.zeros(num_epochs, dtype=np.float32)\n",
    "        min_cost = np.inf\n",
    "        no_cost_decrease_epochs = 0\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        n_features = X.shape[1]\n",
    "        n_output = Y.shape[1]\n",
    "        n_train = int(n_samples * (1 - vset_percent))\n",
    "        n_validate = n_samples - n_train\n",
    "\n",
    "        batch_loop = (n_train - 1) // batch_size + 1\n",
    "\n",
    "        sess = self.session\n",
    "        if use_tboard :\n",
    "            merged_summary = tf.summary.merge_all()\n",
    "            writer = tf.summary.FileWriter(\"./tboard_logs\")\n",
    "            writer.add_graph(sess.graph)  # Show the graph\n",
    "        else :\n",
    "            merged_summary = None\n",
    "\n",
    "        current_X = train_X = X[:n_train]\n",
    "        current_Y = train_Y = Y[:n_train]\n",
    "        validate_X = X[n_train:]\n",
    "        validate_Y = Y[n_train:]\n",
    "\n",
    "        if shuffle_samples_epochs > 0 :\n",
    "            current_XY = np.hstack((train_X, train_Y))\n",
    "\n",
    "        start_time = dt.datetime.now()\n",
    "        print('Learning starts. It will take some time...', start_time)\n",
    "        for epoch in range(num_epochs):\n",
    "            shuffle_samples = shuffle_samples_epochs > 0 and epoch % shuffle_samples_epochs == 0 # shuffle on 0th epoch\n",
    "            \n",
    "            if shuffle_samples :\n",
    "                np.random.shuffle(current_XY) # this will shuffle current_XY in place.\n",
    "                _, current_X, current_Y = np.split(current_XY, (0, n_features), axis=-1)\n",
    "\n",
    "            epoch_hyps = np.zeros(Y.shape, dtype=np.float32)\n",
    "            epoch_costs = np.zeros(batch_loop, dtype=np.float32)\n",
    "\n",
    "            for m in range(batch_loop) :\n",
    "                if m == batch_loop - 1 :\n",
    "                    m_X = current_X[batch_size * m :]\n",
    "                    m_Y = current_Y[batch_size * m :]\n",
    "                else :\n",
    "                    m_X = current_X[batch_size * m : batch_size * (m + 1)]\n",
    "                    m_Y = current_Y[batch_size * m : batch_size * (m + 1)]\n",
    "\n",
    "                feed_dict = {self.X:m_X, self.Y:m_Y,\n",
    "                             self.p_keep_prob:keep_prob,\n",
    "                             self.p_lr:learning_rate,\n",
    "                             self.p_training:True}\n",
    "                targets = [ self.hypothesis, self.cost, self.objective_tensor ]\n",
    "                if use_tboard :\n",
    "                    targets.append(merged_summary)\n",
    "                # print('m:', m, ', m_X:', np.shape(m_X), ', m_Y:', np.shape(m_Y), ', feed_dict:', feed_dict)\n",
    "                results = sess.run(targets, feed_dict = feed_dict)\n",
    "                if use_tboard :\n",
    "                    writer.add_summary(results[-1], global_step = epoch * batch_loop + m)\n",
    "\n",
    "                h_value = results[0]\n",
    "                epoch_hyps[batch_size * m : batch_size * m + m_Y.shape[0]] = h_value\n",
    "                cost_value = results[1]\n",
    "                epoch_costs[m] = cost_value\n",
    "\n",
    "            training_costs[epoch] = avg_cost = np.mean(epoch_costs)\n",
    "\n",
    "            validate_feed_dict = {self.X: validate_X, self.Y: validate_Y,\n",
    "                                  self.p_keep_prob:1.0, self.p_training:False}\n",
    "            validate_targets = [ self.hypothesis, self.cost, self.measure ]\n",
    "            vs_hyps, vs_cost, vs_measure = sess.run(validate_targets, feed_dict=validate_feed_dict)\n",
    "            validation_costs[epoch] = vs_cost\n",
    "            validation_measures[epoch] = vs_measure\n",
    "\n",
    "            if epoch % print_cost_interval == 0 :\n",
    "                print('Epoch:', '%04d' % epoch, 'average training cost =', '{:.9f}'.format(avg_cost),\n",
    "                      'validation cost =', '{:.9f}'.format(vs_cost), 'validation measure =', '{:.9f}'.format(vs_measure), dt.datetime.now())\n",
    "\n",
    "            if epoch % check_accuracy_epochs == check_accuracy_epochs :\n",
    "                print('Epoch:', '%04d' % epoch, 'validation cost =', '{:.9f}'.format(vs_cost),\n",
    "                      'validation measure =', '{:.9f}'.format(vs_measure), dt.datetime.now())\n",
    "\n",
    "                if break_accuracy > 0 and break_accuracy < vs_cost :\n",
    "                    print('Stops the training due to validation loss', vs_cost, ' exceeded the criteria', break_accuracy)\n",
    "                    training_costs = training_costs[:epoch + 1] # strip un-run epochs\n",
    "                    validation_costs = validation_costs[:epoch + 1] # strip un-run epochs\n",
    "                    validation_measures = validation_measures[:epoch + 1] # strip un-run epochs\n",
    "                    break\n",
    "\n",
    "            if early_stopping_epoch_on_max_no_decrease > 0 :\n",
    "                if vs_cost < min_cost :\n",
    "                    min_cost = vs_cost\n",
    "                    no_cost_decrease_epochs = 0\n",
    "                else :\n",
    "                    no_cost_decrease_epochs = no_cost_decrease_epochs + 1\n",
    "                    if no_cost_decrease_epochs >= early_stopping_epoch_on_max_no_decrease :\n",
    "                        # FIXME : in reality, i need to restore variables saved when it was not decreasing but i do not. maybe in the future ..\n",
    "                        print('Stops the training since cost is not reduced during ', no_cost_decrease_epochs, ' epochs.')\n",
    "                        training_costs = training_costs[:epoch + 1] # strip un-run epochs\n",
    "                        validation_costs = validation_costs[:epoch + 1] # strip un-run epochs\n",
    "                        validation_measures = validation_measures[:epoch + 1] # strip un-run epochs\n",
    "                        break\n",
    "\n",
    "        end_time = dt.datetime.now()\n",
    "        print('Training(learning) Finished!', end_time)\n",
    "        print('Training took ', '%10d' % ((end_time - start_time).total_seconds()),\n",
    "              ' seconds.')\n",
    "   \n",
    "        return training_costs, validation_costs, validation_measures\n",
    "                \n",
    "\n",
    "    def test(self, X, Y) :\n",
    "        start_time = dt.datetime.now()\n",
    "        g = tf.get_default_graph()\n",
    "\n",
    "        with g.as_default() :\n",
    "            vals = self._test_model(X, Y)\n",
    "            end_time = dt.datetime.now()\n",
    "            print('Prediction took ', '%10d' % ((end_time - start_time).total_seconds()),\n",
    "                  ' seconds.')\n",
    "            print('Started at ', start_time, ' and finished at ', end_time)\n",
    "            return vals\n",
    "\n",
    "    def _test_model(self, X, Y) :\n",
    "        test_feed_dict = {self.X: X, self.Y: Y,\n",
    "                          self.p_keep_prob:1.0, self.p_training:False}\n",
    "        test_targets = [ self.hypothesis, self.cost, self.measure ]\n",
    "\n",
    "        sess = self.session\n",
    "        hyps, cost, measure = sess.run(test_targets, feed_dict=test_feed_dict)\n",
    "        return hyps, cost, measure\n",
    "\n",
    "    def infer(self, X) :\n",
    "        g = tf.get_default_graph()\n",
    "\n",
    "        with g.as_default() :\n",
    "            vals = self._infer_model(X)\n",
    "            return vals\n",
    "\n",
    "    def _infer_model(self, X) :\n",
    "        test_feed_dict = {self.X: X,\n",
    "                          self.p_keep_prob:1.0, self.p_training:False}\n",
    "        test_targets = [ self.hypothesis ]\n",
    "\n",
    "        sess = self.session\n",
    "        hyps = sess.run(test_targets, feed_dict=test_feed_dict)\n",
    "        return hyps\n",
    "    \n",
    "    def r_squared(self, y, h) :\n",
    "        # in tf.reduce_mean, if axis has no entries, all dimensions are reduced, and a tensor with a single element is returned\n",
    "        total_error = tf.reduce_sum(tf.square(tf.subtract(y, tf.reduce_mean(y, 0))))  # reduce_mean by 0-axis maintains vector dimension\n",
    "        unexplained_error = tf.reduce_sum(tf.square(tf.subtract(y, h)))\n",
    "        r_squared = tf.subtract(1.0, tf.div(unexplained_error, total_error))\n",
    "        return r_squared\n",
    "\n",
    "    def smape(self, y, h) :\n",
    "        return tf.reduce_mean(2.0 * tf.abs(tf.subtract(y, h)) / tf.maximum(1e-7, (tf.abs(y) + tf.abs(h)))) # tf.maximum is used to avoid nan\n",
    "        \n",
    "    def check_nan(self, value) :\n",
    "        return value is None or math.isnan(value)\n",
    "\n",
    "    def save_model(self, save_file_name) :\n",
    "        # self._dump_graph('save_model(' + save_file_name + ')')\n",
    "        \n",
    "        tf.train.Saver().save(self.session, save_file_name)\n",
    "\n",
    "    def _dump_graph(self, where) :\n",
    "        print('')\n",
    "\n",
    "        print('--- dumping tensorflow graph [', where, '] ---')\n",
    "        g = tf.get_default_graph()\n",
    "        print('default tf graph :', g)\n",
    "\n",
    "        # debug graphs\n",
    "        keys = g.get_all_collection_keys()\n",
    "        print('current name scope :', g.get_name_scope())\n",
    "        for key in keys :\n",
    "            print('all graph (', key, ')  :', g.get_collection(key))\n",
    "        print('') \n",
    "        print('')\n",
    "\n",
    "       \n",
    "    def restore_model(self, saved_dir) :\n",
    "        print('saved dir:', saved_dir)\n",
    "\n",
    "        with self.session.as_default() :\n",
    "            # self._dump_graph('restore_model(' + saved_dir + ')')\n",
    "            \n",
    "            reader = tf.train.NewCheckpointReader(saved_dir)\n",
    "            # for var_name in reader.get_variable_to_shape_map() :\n",
    "            #     print(var_name)\n",
    "        \n",
    "            tf.train.Saver().restore(self.session, saved_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util\n",
    "\n",
    "TEST_PERCENT = 0.2\n",
    "\n",
    "def shuffle_XY(X, Y) :\n",
    "    hstacked = np.hstack((X, Y))\n",
    "    np.random.shuffle(hstacked)\n",
    "    _, new_X, new_Y = np.split(hstacked, (0, X.shape[1]), axis=-1)\n",
    "    return new_X, new_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  Ant-v2 2019-07-07 22:42:44.316212\n",
      "Ant-v2  observation shape:  (19992, 111) , actions shape: (19992, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0707 22:42:47.032993 50204 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0707 22:42:47.034992 50204 deprecation.py:323] From <ipython-input-3-fc99626fbdc6>:79: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0707 22:42:47.263379 50204 deprecation.py:323] From <ipython-input-3-fc99626fbdc6>:80: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "W0707 22:42:47.366063 50204 deprecation.py:323] From C:\\Works\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0707 22:42:47.378071 50204 deprecation.py:323] From <ipython-input-3-fc99626fbdc6>:272: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning starts. It will take some time... 2019-07-07 22:42:47.639329\n",
      "Epoch: 0000 average training cost = 0.802247047 validation cost = 0.619904339 validation measure = -7.028655052 2019-07-07 22:42:48.765364\n",
      "Epoch: 0500 average training cost = 0.060373325 validation cost = 0.018596660 validation measure = 0.759146452 2019-07-07 22:46:46.832457\n",
      "Training(learning) Finished! 2019-07-07 22:50:53.710971\n",
      "Training took         486  seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Works\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n",
      "W0707 22:50:54.270516 50204 deprecation.py:323] From C:\\Works\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved dir: model_Ant-v2\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-0/kernel/Adadelta\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-0/bias/Adadelta\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-0/kernel/Adadelta_1\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-0/bias/Adadelta_1\n",
      "Ant-v2-dnn/layer-last/kernel\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-1/kernel/Adadelta_1\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-1/bias/Adadelta\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-1/kernel/Adadelta\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-1/bias/Adadelta_1\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-last/kernel/Adadelta_1\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-last/kernel/Adadelta\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-2/bias/Adadelta_1\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-2/bias/Adadelta\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-last/bias/Adadelta_1\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-2/kernel/Adadelta\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-last/bias/Adadelta\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-2/kernel/Adadelta_1\n",
      "Ant-v2-dnn/layer-0/bias\n",
      "Ant-v2-dnn/layer-0/kernel\n",
      "Ant-v2-dnn/layer-1/bias\n",
      "Ant-v2-dnn/layer-1/kernel\n",
      "Ant-v2-dnn/layer-2/bias\n",
      "Ant-v2-dnn/layer-2/kernel\n",
      "Ant-v2-dnn/layer-last/bias\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-07 22:50:54.303385  and finished at  2019-07-07 22:50:54.328319\n",
      "ending  Ant-v2 2019-07-07 22:50:54.328319\n",
      "starting  Hopper-v2 2019-07-07 22:50:54.329315\n",
      "Hopper-v2  observation shape:  (20000, 11) , actions shape: (20000, 3)\n",
      "Learning starts. It will take some time... 2019-07-07 22:50:54.754780\n",
      "Epoch: 0000 average training cost = 2.600547791 validation cost = 2.395168543 validation measure = -0.116906166 2019-07-07 22:50:55.264421\n",
      "Epoch: 0500 average training cost = 0.258199543 validation cost = 0.116882704 validation measure = 0.945495665 2019-07-07 22:55:03.817415\n",
      "Training(learning) Finished! 2019-07-07 22:59:11.267866\n",
      "Training took         496  seconds.\n",
      "saved dir: model_Hopper-v2\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-0/kernel/Adadelta\n",
      "Hopper-v2-dnn/layer-last/bias\n",
      "Hopper-v2-dnn/layer-0/bias\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-0/bias/Adadelta\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-0/kernel/Adadelta_1\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-0/bias/Adadelta_1\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-1/kernel/Adadelta_1\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-1/bias/Adadelta\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-1/kernel/Adadelta\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-1/bias/Adadelta_1\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-2/bias/Adadelta\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-2/bias/Adadelta_1\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-2/kernel/Adadelta\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-2/kernel/Adadelta_1\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-last/bias/Adadelta\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-last/bias/Adadelta_1\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-last/kernel/Adadelta\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-last/kernel/Adadelta_1\n",
      "Hopper-v2-dnn/layer-last/kernel\n",
      "Hopper-v2-dnn/layer-0/kernel\n",
      "Hopper-v2-dnn/layer-1/bias\n",
      "Hopper-v2-dnn/layer-1/kernel\n",
      "Hopper-v2-dnn/layer-2/bias\n",
      "Hopper-v2-dnn/layer-2/kernel\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-07 22:59:11.814400  and finished at  2019-07-07 22:59:11.835344\n",
      "ending  Hopper-v2 2019-07-07 22:59:11.835344\n",
      "starting  Reacher-v2 2019-07-07 22:59:11.835344\n",
      "Reacher-v2  observation shape:  (1000, 11) , actions shape: (1000, 2)\n",
      "Learning starts. It will take some time... 2019-07-07 22:59:12.310076\n",
      "Epoch: 0000 average training cost = 0.149326846 validation cost = 0.102704681 validation measure = -14.160396576 2019-07-07 22:59:12.446217\n",
      "Epoch: 0500 average training cost = 0.053795539 validation cost = 0.008411404 validation measure = -0.241620302 2019-07-07 22:59:27.681710\n",
      "Stops the training since cost is not reduced during  20  epochs.\n",
      "Training(learning) Finished! 2019-07-07 22:59:40.681279\n",
      "Training took          28  seconds.\n",
      "saved dir: model_Reacher-v2\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-0/bias/Adadelta_1\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-0/bias/Adadelta\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-0/kernel/Adadelta_1\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-0/kernel/Adadelta\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-1/bias/Adadelta_1\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-1/kernel/Adadelta_1\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-1/bias/Adadelta\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-1/kernel/Adadelta\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-last/kernel/Adadelta\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-2/bias/Adadelta\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-last/kernel/Adadelta_1\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-2/bias/Adadelta_1\n",
      "Reacher-v2-dnn/layer-last/bias\n",
      "Reacher-v2-dnn/layer-0/bias\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-last/bias/Adadelta\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-2/kernel/Adadelta\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-last/bias/Adadelta_1\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-2/kernel/Adadelta_1\n",
      "Reacher-v2-dnn/layer-last/kernel\n",
      "Reacher-v2-dnn/layer-0/kernel\n",
      "Reacher-v2-dnn/layer-1/bias\n",
      "Reacher-v2-dnn/layer-1/kernel\n",
      "Reacher-v2-dnn/layer-2/bias\n",
      "Reacher-v2-dnn/layer-2/kernel\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-07 22:59:41.213917  and finished at  2019-07-07 22:59:41.230912\n",
      "ending  Reacher-v2 2019-07-07 22:59:41.230912\n",
      "starting  HalfCheetah-v2 2019-07-07 22:59:41.230912\n",
      "HalfCheetah-v2  observation shape:  (20000, 17) , actions shape: (20000, 6)\n",
      "Learning starts. It will take some time... 2019-07-07 22:59:41.698662\n",
      "Epoch: 0000 average training cost = 2.378319740 validation cost = 1.693411469 validation measure = -2.122699738 2019-07-07 22:59:42.282063\n",
      "Epoch: 0500 average training cost = 0.191245407 validation cost = 0.068073958 validation measure = 0.874469638 2019-07-07 23:04:04.988075\n",
      "Training(learning) Finished! 2019-07-07 23:08:28.164671\n",
      "Training took         526  seconds.\n",
      "saved dir: model_HalfCheetah-v2\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-0/kernel/Adadelta\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-0/bias/Adadelta\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-0/kernel/Adadelta_1\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-0/bias/Adadelta_1\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-1/kernel/Adadelta_1\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-1/bias/Adadelta\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-1/kernel/Adadelta\n",
      "HalfCheetah-v2-dnn/layer-2/kernel\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-1/bias/Adadelta_1\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-last/kernel/Adadelta_1\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-last/kernel/Adadelta\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-2/bias/Adadelta_1\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-2/bias/Adadelta\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-last/bias/Adadelta_1\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-2/kernel/Adadelta\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-last/bias/Adadelta\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-2/kernel/Adadelta_1\n",
      "HalfCheetah-v2-dnn/layer-0/bias\n",
      "HalfCheetah-v2-dnn/layer-0/kernel\n",
      "HalfCheetah-v2-dnn/layer-1/bias\n",
      "HalfCheetah-v2-dnn/layer-1/kernel\n",
      "HalfCheetah-v2-dnn/layer-last/kernel\n",
      "HalfCheetah-v2-dnn/layer-2/bias\n",
      "HalfCheetah-v2-dnn/layer-last/bias\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-07 23:08:28.801573  and finished at  2019-07-07 23:08:28.821519\n",
      "ending  HalfCheetah-v2 2019-07-07 23:08:28.822515\n",
      "starting  Humanoid-v2 2019-07-07 23:08:28.822515\n",
      "Humanoid-v2  observation shape:  (20000, 376) , actions shape: (20000, 17)\n",
      "Learning starts. It will take some time... 2019-07-07 23:08:29.377031\n",
      "Epoch: 0000 average training cost = 1084.891601562 validation cost = 678.221008301 validation measure = -711.868530273 2019-07-07 23:08:29.996379\n",
      "Epoch: 0500 average training cost = 5.571677208 validation cost = 1.397883415 validation measure = -0.469295502 2019-07-07 23:13:27.206154\n",
      "Training(learning) Finished! 2019-07-07 23:18:24.337937\n",
      "Training took         594  seconds.\n",
      "saved dir: model_Humanoid-v2\n",
      "Humanoid-v2-dnn/layer-1/kernel\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-0/kernel/Adadelta\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-0/bias/Adadelta\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-0/kernel/Adadelta_1\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-0/bias/Adadelta_1\n",
      "Humanoid-v2-dnn/layer-2/kernel\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-1/kernel/Adadelta_1\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-1/bias/Adadelta\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-1/kernel/Adadelta\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-1/bias/Adadelta_1\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-2/bias/Adadelta\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-2/bias/Adadelta_1\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-2/kernel/Adadelta\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-2/kernel/Adadelta_1\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-last/bias/Adadelta\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-last/bias/Adadelta_1\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-last/kernel/Adadelta\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-last/kernel/Adadelta_1\n",
      "Humanoid-v2-dnn/layer-0/kernel\n",
      "Humanoid-v2-dnn/layer-0/bias\n",
      "Humanoid-v2-dnn/layer-1/bias\n",
      "Humanoid-v2-dnn/layer-2/bias\n",
      "Humanoid-v2-dnn/layer-last/bias\n",
      "Humanoid-v2-dnn/layer-last/kernel\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-07 23:18:24.839546  and finished at  2019-07-07 23:18:24.868512\n",
      "ending  Humanoid-v2 2019-07-07 23:18:24.868512\n",
      "starting  Walker2d-v2 2019-07-07 23:18:24.868512\n",
      "Walker2d-v2  observation shape:  (20000, 17) , actions shape: (20000, 6)\n",
      "Learning starts. It will take some time... 2019-07-07 23:18:25.457076\n",
      "Epoch: 0000 average training cost = 2.810679436 validation cost = 2.294559002 validation measure = -1.296395540 2019-07-07 23:18:26.029480\n",
      "Epoch: 0500 average training cost = 0.379968286 validation cost = 0.203127339 validation measure = 0.796710074 2019-07-07 23:22:49.523807\n",
      "Training(learning) Finished! 2019-07-07 23:27:12.302580\n",
      "Training took         526  seconds.\n",
      "saved dir: model_Walker2d-v2\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-0/bias/Adadelta\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-0/kernel/Adadelta\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-0/bias/Adadelta_1\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-0/kernel/Adadelta_1\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-1/kernel/Adadelta_1\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-1/bias/Adadelta\n",
      "Walker2d-v2-dnn/layer-1/bias\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-1/kernel/Adadelta\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-1/bias/Adadelta_1\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-2/bias/Adadelta\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-2/bias/Adadelta_1\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-2/kernel/Adadelta_1\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-2/kernel/Adadelta\n",
      "Walker2d-v2-dnn/layer-last/bias\n",
      "Walker2d-v2-dnn/layer-0/kernel\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-last/bias/Adadelta_1\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-last/bias/Adadelta\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-last/kernel/Adadelta\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-last/kernel/Adadelta_1\n",
      "Walker2d-v2-dnn/layer-last/kernel\n",
      "Walker2d-v2-dnn/layer-0/bias\n",
      "Walker2d-v2-dnn/layer-1/kernel\n",
      "Walker2d-v2-dnn/layer-2/bias\n",
      "Walker2d-v2-dnn/layer-2/kernel\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-07 23:27:12.788744  and finished at  2019-07-07 23:27:12.807693\n",
      "ending  Walker2d-v2 2019-07-07 23:27:12.807693\n"
     ]
    }
   ],
   "source": [
    "# train behavior cloning policies\n",
    "\n",
    "for gym_env in gym_envs :\n",
    "    print('starting ', gym_env, dt.datetime.now())\n",
    "    observations, actions = load_expert_data(gym_env)\n",
    "    obs_shape, action_shape = np.shape(observations), np.shape(actions)\n",
    "    if action_shape[1] == 1 :\n",
    "        actions = np.reshape(actions, (action_shape[0], action_shape[2]))\n",
    "        action_shape = np.shape(actions)\n",
    "    print(gym_env, ' observation shape: ', obs_shape, ', actions shape:', action_shape)\n",
    "    # for i in range(2) :\n",
    "    #     print('observation:', observations[i])\n",
    "    #     print('actions:', actions[i])\n",
    "    cloning = BehavioralCloning(X_shape=obs_shape, Y_shape=action_shape, scope_name=gym_env)\n",
    "    \n",
    "    n_samples = observations.shape[0]\n",
    "    n_train = int(n_samples * (1 - TEST_PERCENT))\n",
    "\n",
    "    observations, actions = shuffle_XY(observations, actions)\n",
    "    training_costs, validation_costs, validation_measures = cloning.train(observations[:n_train], actions[:n_train])\n",
    "    \n",
    "    gym_env_model = 'model_' + gym_env\n",
    "    cloning.save_model(gym_env_model)\n",
    "    \n",
    "    cloning = BehavioralCloning(X_shape=obs_shape, Y_shape=action_shape, scope_name=gym_env, restore_mode=True)\n",
    "    cloning.restore_model(gym_env_model)\n",
    "    \n",
    "    test_hyps, test_costs, test_measures = cloning.test(observations[n_train:], actions[n_train:])\n",
    "    print('ending ', gym_env, dt.datetime.now())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# run_expert source code for reference\n",
    "\n",
    "import tf_util\n",
    "import pickle as pk\n",
    "import traceback\n",
    "\n",
    "def load_learned_policy_fn(gym_env, session=None) :\n",
    "    observations, actions = load_expert_data(gym_env)\n",
    "    obs_shape, action_shape = list(np.shape(observations)), list(np.shape(actions))\n",
    "    if action_shape[1] == 1 :\n",
    "        actions = np.reshape(actions, (action_shape[0], action_shape[2]))\n",
    "        action_shape = list(np.shape(actions))\n",
    "\n",
    "    gym_env_model = 'model_' + gym_env\n",
    "    cloning = BehavioralCloning(X_shape=obs_shape, Y_shape=action_shape, scope_name=gym_env, restore_mode=True, session=session)\n",
    "    cloning.restore_model(gym_env_model)\n",
    "\n",
    "    return lambda x : cloning.infer(x)\n",
    "    \n",
    "def rollout_by_policy(gym_env, max_timesteps, num_rollouts, policy_fn=None, render=False) :\n",
    "    policy_type = 'learned'\n",
    "    \n",
    "    if policy_fn is None : # default policy_fn is expert policy\n",
    "        print('loading and building expert policy')\n",
    "        policy_fn = load_expert_policy_fn(gym_env)\n",
    "        print('loaded and built')\n",
    "        policy_type = 'expert'\n",
    "\n",
    "    with tf.Session():\n",
    "        tf_util.initialize()\n",
    "\n",
    "        import gym\n",
    "        env = gym.make(gym_env)\n",
    "        max_steps = max_timesteps or env.spec.timestep_limit\n",
    "\n",
    "        returns = []\n",
    "        observations = []\n",
    "        actions = []\n",
    "        for i in range(num_rollouts):\n",
    "            # print('iter', i)\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            totalr = 0.\n",
    "            steps = 0\n",
    "            while not done:\n",
    "                action = policy_fn(obs[None,:])\n",
    "                observations.append(obs)\n",
    "                actions.append(action)\n",
    "                try :\n",
    "                    if np.shape(action)[1] == 1 :\n",
    "                        action_shape = np.shape(action)\n",
    "                        action = np.reshape(action, (action_shape[0], action_shape[2]))\n",
    "                    obs, r, done, _ = env.step(action) # observation, reward, done\n",
    "                except ValueError as e :\n",
    "                    print('action:', action, ', shape:', np.shape(action), ', policy_type:', policy_type)\n",
    "                    print('actions:', actions, ', shape:', np.shape(actions))\n",
    "                    traceback.print_exc()     \n",
    "\n",
    "                totalr += r\n",
    "                steps += 1\n",
    "                if render:\n",
    "                    env.render()\n",
    "                # if steps % 100 == 0: print(\"%i/%i\"%(steps, max_steps))\n",
    "                if steps >= max_steps:\n",
    "                    break\n",
    "            returns.append(totalr)\n",
    "\n",
    "        print('returns', returns)\n",
    "        print('mean return', np.mean(returns))\n",
    "        print('std of return', np.std(returns))\n",
    "\n",
    "        rollout_data = {'observations': np.array(observations),\n",
    "                        'actions': np.array(actions),\n",
    "                        'returns': np.array(returns)}\n",
    "\n",
    "        if not os.path.exists('rollout_data') :\n",
    "            os.mkdir('rollout_data')\n",
    "        with open(os.path.join('rollout_data', policy_type + '-' + gym_env + '.pkl'), 'wb') as f:\n",
    "            pk.dump(rollout_data, f, pk.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        return rollout_data, policy_type, env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and building expert policy\n",
      "obs (1, 111) (1, 111)\n",
      "loaded and built\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "returns [4747.680326602361, 4741.33141203345, 4849.193871008658, 4833.752898733907, 4998.299597073863, 4865.028149508911, 4710.76965776187, 5002.620061411658, 4801.008480298248, 4884.905360052358, 4859.259532020084, 4851.707784397738, 4992.636609725978, 5082.002053765846, 5025.357253313969, 4740.592035374321, 4940.539756066792, 4740.010348598251, 4695.18690206555, 4909.017572131628, 4747.75311241958, 4817.893227044679, 4766.990389490045, 4644.289659625638, 4740.790944634136, 4722.294687296413, 4683.6788184581865, 4665.8446391249645, 4928.847069560956, 4820.923032515179, 4879.911593685575, 4624.070944791383, 4942.153330334369, 4755.480397902153, 4684.802168880333, 4826.7330093160335, 4794.9830279908965, 4914.451053918959, 4847.662810651242, 4879.6378134806855, 4915.498752590309, 4810.4576463746225, 4770.464240835335, 4960.407237976342, 4904.7413022013525, 4832.154244324852, 4809.179011714278, 5005.44848468089, 4639.01843224234, 4995.244259177134, 4966.111177667341, 4870.097650583146, 4510.16767187446, 4864.3742442620505, 4681.907120249824, 4879.996511650495, 4720.26430454487, 4802.610865802735, 4795.025365291403, 4832.736440581053, 4730.150029219938, 4927.652026114341, 4806.986746252634, 4903.727392987055, 4808.314363386823, 4745.372173320991, 4704.518715864044, 4932.372251984579, 4986.625775571332, 4742.763406262772, 4736.372187415853, 4885.619951582773, 4679.901865367739, 4686.038005099586, 4816.59196242658, 4119.173394107344, 4864.544899000398, 4744.612396149885, 4798.274202311449, 4878.158449940031, 4916.147230955419, 4955.026906271631, 4727.652906062857, 4700.052940193193, 4762.307683161204, 4917.312885098742, 4898.383809938554, 4758.626988233982, 4624.58075063837, 4765.576704339817, 4950.031231884871, 4892.3528415158735, 4682.507059481471, 5018.337270529066, 4799.919727538729, 4787.885994701584, 4742.35200778796, 4901.487140486172, 4649.215725775627, 4804.58908518226, 4982.672107749209, 4719.3571958558305, 4902.5136265682295, 4792.522586328817, 4621.654130962056, 4785.492125266867, 4782.096118067614, 4756.920942668956, 4880.8238337391585, 4705.8912529381105, 4848.200406275677, 4643.434950779685, 4772.114844861007, 4900.630713298431, 4896.884318568071, 4937.200482121556, 4923.241079935422, 4909.278578993304, 4791.804009025535, 526.8388188291318, 4890.088692731763, 4787.650913653404, 4812.559785597879, 4746.762212754016, 4853.025572112123, 4719.869337416716, 4978.743585779591, 5026.09138226904, 4917.462009208257, 4755.026634071443, 4772.392448157007, 4921.749471822553, 4914.45642222246, 4590.013893854192, 4878.9558669470125, 4993.071221827759, 4818.015289262339, 4895.545970333634, 4854.032460917293, 5094.897591815849, 4924.08079191577, 4784.1074497397085, 4669.605015135513, 4449.299416035195, 4896.382028642794, 4802.579472478925, 4787.202908664503, 4857.931067374887, 4658.394057908337, 4783.787615264459, 4878.1322767020065, 2102.9680973320587, 4804.531855355659, 4741.445043189496, 4804.904063982993, 5017.773281933036, 4917.848694562819, 4903.833372585947, 4681.043992281041, 5103.981284870674, 4893.716260263154, 4886.6631114600295, 4980.8389912831135, 4836.180890064281, 5013.134874285272, 4891.665034348726, 4825.634632356032, 4730.34153457309, 4922.425880805948, 4971.155447730926, 4847.315304098528, 4845.440834474506, 4808.797749959241, 4791.057283299836, 4887.61040851941, 4639.1095493159855, 4848.466485602332, 2113.1260130506626, 4964.813292757361, 4739.40650950672, 4893.427255505784, 4887.323324055318, 4809.086083410855, 4739.690195779615, 4831.235764714164, 4865.673483958446, 4621.867748283399, 4716.311450390143, 4908.078717797474, 4668.872049034443, 4580.672592877658, 4736.298809247336, 5001.47994920759, 4705.068266802779, 4864.297412739592, 4885.7396242882305, 4866.68907316561, 4835.307517265426, 4848.309727679465, 4658.548409946901]\n",
      "mean return 4772.544028286417\n",
      "std of return 422.1639753193734\n",
      "Rollout result. env: Ant-v2 , policy_type: expert , returns: 200 / 4772.544028286417 / 422.1639753193734\n",
      "It took         530  seconds.\n",
      "saved dir: model_Ant-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "returns [821.551397434454, 1186.6292913738662, 874.3925527337345, 827.3375899214252, 1035.0468736152716, 925.2535669357646, 851.1666963036791, 958.9101537031712, 886.4300699520376, 876.6571081685015, 881.8639271795582, 430.2746635704982, 848.8787570046618, 887.3214627831196, 960.0576960724877, 981.100442485422, 922.7133748378761, 1179.828004992494, 969.603313635375, 987.5091956192891, 962.4935875099975, 1160.2826646763228, 881.5203793906219, 912.88801883715, 916.7864529209164, 902.220390374113, 956.3583967126734, 926.947868157199, 966.0277607734262, 1076.6578002210797, 869.2383722665465, 896.8668113177006, 877.2264889279112, 928.1225257422176, 909.4333485759967, 868.9981493442638, 849.6116992900519, 890.2896425127092, 888.9774283488141, 928.9249969943861, 864.1728675946368, 934.8703380247998, 1047.1493594006922, 996.1607142726062, 925.7163484418649, 911.925395683844, 956.9401402514793, 869.5788763246119, 1025.3016670927823, 844.8737240239, 872.3762668322458, 840.4919343718708, 905.6897133196263, 1167.0098830155791, 914.7775597591699, 886.9374290540453, 914.3177509364156, 909.9275939223994, 909.5283221012132, 858.7841698977777, 897.4417988286776, 864.1601126425975, 1070.9282116795446, 951.9381761485312, 859.9267781350329, 876.3966763195261, 1047.2047433441778, 1022.1345831209862, 889.7672779476937, 868.7347473038207, 923.2736829077389, 832.1673752515712, 1005.6224600845522, 926.7540178121585, 917.6494009403457, 886.9912898856173, 886.1145087536736, 854.3312187667372, 933.7456812793233, 1013.0186918185001, 1073.956815939722, 926.7729544044836, 252.7944689989555, 844.7757210355518, 826.7716362740592, 948.2224079664686, 955.4081014252048, 1406.5994850199675, 234.66674267510427, 885.4583144100334, 889.0906662502598, 862.4961393147977, 876.9581159981532, 872.6173005939604, 905.5108623224911, 848.2684776931056, 520.0902200030571, 854.5922208108568, 985.6486727394919, 927.8297539192374, 1200.8972105023115, 889.4988204301068, 841.0972395809463, 927.3440256433031, 900.7446142178507, 923.0276616744595, 890.2530102868593, 1094.3456808433257, 929.816123916865, 884.3098491992955, 152.72306523052822, 896.4198080349596, 959.1376946492154, 859.1117851234764, 867.7253095018033, 918.8996993959533, 950.3335119780778, 966.2314825239043, 879.3111027455457, 899.7732324519698, 988.1661056877654, 902.6018454716913, 1110.0813498359685, 839.009265732525, 815.410441247657, 1225.5551685605108, 874.5049403143994, 958.710240985342, 1104.7593319291966, 934.2067993623128, 913.4511546649899, 972.9994385967883, 855.7782489157704, 933.5193264297261, 1078.2332048705634, 876.8159233388864, 886.5638772191655, 836.0116102228608, 1190.3523741377942, 859.2254027649196, 1230.3814970387423, 1084.8798597028217, 909.4428627136646, 839.070580721266, 865.1546751053049, 970.1311074405857, 945.973700689659, 1071.4475105723664, 888.3768728541338, 1033.441707499506, 917.6004416910654, 889.937678664704, 985.4589485745539, 857.0310875277602, 848.9887970845319, 1079.8404348773909, 876.9945404163728, 912.8165964114603, 964.3631594465554, 946.6074484405102, 836.6970771551389, 966.7107134780075, 973.7240636323789, 912.2466326670371, 1056.9654225379893, 1069.7227262042018, 903.2860909042539, 882.5606734075042, 1287.1789000563153, 959.5685533503605, 897.557415764153, 884.7643562760761, 911.4034316811723, 968.4954291599367, 973.4424631121609, 970.373408567654, 1023.5407038815212, 900.9629620508347, 955.7640217382507, 847.990935803211, 969.1169906520565, 895.7428491366298, 899.6894923635036, 884.5742059787854, 975.3463121920628, 904.3667382626686, 849.178394209991, 902.7188502052584, 1287.9574590223308, 923.4671939556224, 828.0846171036975, 852.5884001006149, 1331.6033527671623, 926.8159811565063, 992.1629251056545, 915.7054039798948, 987.4529047894528, 990.194326034461, 895.933635789587, 867.3176400337757]\n",
      "mean return 926.0681481313227\n",
      "std of return 140.41459404931896\n",
      "Rollout result. env: Ant-v2 , policy_type: learned , returns: 200 / 926.0681481313227 / 140.41459404931896\n",
      "It took         700  seconds.\n",
      "loading and building expert policy\n",
      "obs (1, 11) (1, 11)\n",
      "loaded and built\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "returns [3786.697936250963, 3784.999491155297, 3777.688870713761, 3784.520518965233, 3774.998869297225, 3780.449911838472, 3777.7195845342544, 3781.1439034111613, 3781.0819902828002, 3772.7811066907766, 3780.28843939155, 3779.116096394092, 3783.8966722329014, 3777.246474195311, 3775.4683013338795, 3781.6679181491986, 3775.5912966493715, 3779.168826856357, 3772.948716974639, 3770.6144912044274, 3780.9646800844075, 3774.146231656441, 3776.1341539440527, 3781.911213665903, 3778.1301272750475, 3777.169962379374, 3774.346413395573, 3782.2471537338984, 3774.622927580688, 3781.6559284817367, 3770.4939376802404, 3784.687060906243, 3774.3501592337693, 3779.189309164957, 3775.3289431555286, 3774.2320130351227, 3781.8939631776916, 3782.382218408964, 3783.2772013758986, 3777.4313049393595, 3775.1570105330184, 3777.8824729753883, 3775.21295512209, 3777.3656554525924, 3781.477968874741, 3781.9956599247266, 3783.93302339186, 3777.9134928795024, 3782.5444535468546, 3776.4638334261585, 3778.4224584118415, 3774.5045659570046, 3787.457741168268, 3782.02436228786, 3779.5323999536363, 3781.0681943816835, 3782.50915125388, 3776.521525791044, 3779.1880934740816, 3777.194644716088, 3777.5824798204203, 3771.9382261011992, 3775.4099842209616, 3780.0043070491474, 3780.3484491530908, 3776.158141168202, 3784.103374230713, 3784.4060762503786, 3782.942755480538, 3775.8649159154756, 3776.029593691415, 3774.401535660913, 3772.682782659043, 3778.4911410776517, 3778.212290495205, 3778.6909900779756, 3779.642565015304, 3782.9545481735017, 3781.1886814585237, 3782.4295084662676, 3775.4479734490487, 3776.522426956828, 3775.3644369004683, 3777.805064219164, 3776.690774616185, 3780.639811773333, 3778.227973781149, 3781.7221507033296, 3784.193339293327, 3775.318123210607, 3772.9784581166655, 3775.776364753811, 3776.849030844087, 3777.703949949297, 3781.672831878274, 3780.0343775775837, 3780.692690869044, 3782.072157453011, 3780.82826476061, 3772.7441349862775, 3774.377135147519, 3779.551806811734, 3775.166399289408, 3780.095624504761, 3777.9774653661348, 3775.2719042456674, 3777.469497810138, 3780.821412082549, 3785.585112292084, 3780.664374931328, 3780.5193514531084, 3775.4457437941523, 3777.9729798574704, 3781.3900764341734, 3776.933269545086, 3781.1539631482024, 3780.1014606335475, 3779.142128034452, 3776.770735381767, 3781.5537844376436, 3779.1146811207786, 3775.5656148439884, 3783.435419101458, 3778.2609606135843, 3779.970562973345, 3781.228172904477, 3780.805094890953, 3788.7705426207417, 3778.332669233392, 3775.569015209434, 3775.9028605311496, 3776.064474239019, 3775.786230233864, 3777.2625796476614, 3778.7345806415806, 3772.2203672135943, 3768.9762660597503, 3773.148851493207, 3781.789555207888, 3782.7253351508643, 3771.1667263251334, 3774.6132812369146, 3780.908040950541, 3782.437875058123, 3781.0398698823033, 3777.7375938055284, 3776.643345750312, 3778.8326769103346, 3782.533144401194, 3774.3235561476445, 3777.5805749586716, 3777.537046635549, 3777.2725507801806, 3777.54131378036, 3779.3394275947176, 3777.364954038196, 3776.465570454891, 3774.3073416095913, 3777.795220163671, 3773.0303995595946, 3787.0678316059066, 3777.294481067658, 3784.982388506807, 3779.3611973875713, 3774.4008215130066, 3784.9082147230083, 3777.0785971494547, 3777.074258923422, 3775.496162053144, 3777.1700967954994, 3779.598620419491, 3771.376825444902, 3784.7940377636273, 3776.115597547011, 3782.648681947562, 3777.432722032994, 3784.1965527976213, 3772.566430238211, 3776.4048158128, 3774.691330976828, 3771.230182951919, 3770.9956161598466, 3776.3019764132305, 3784.0644139326664, 3772.9967758715306, 3783.46569681245, 3774.2001856678535, 3778.395240645042, 3777.1754653964076, 3786.5820630198214, 3780.797355972268, 3783.8340943861112, 3782.73748266374, 3782.5868140475804, 3778.941430118305, 3778.940868660336, 3779.9817534470762, 3778.3975075231906, 3772.878588338594, 3776.1029799876997]\n",
      "mean return 3778.515926957371\n",
      "std of return 3.7757909785337014\n",
      "Rollout result. env: Hopper-v2 , policy_type: expert , returns: 200 / 3778.515926957371 / 3.7757909785337014\n",
      "It took         408  seconds.\n",
      "saved dir: model_Hopper-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "returns [149.24399723720856, 149.41966239988218, 149.75185769738798, 231.73546395714507, 148.47596694453588, 147.93433456952914, 147.73392238061328, 147.2067299082631, 149.43566987312522, 145.60793136785264, 145.1062787942111, 149.3984819053028, 146.28400868051455, 144.6838360801626, 154.85532796084067, 147.1389048395023, 144.3111949907588, 145.92559999532082, 149.46323270402715, 147.80076137283072, 152.77625632172865, 146.49646892096817, 144.91092826509947, 147.58358472937817, 147.98951541944749, 149.7055587696971, 148.34319835800247, 148.09696651863766, 148.8506615693299, 150.45978360460145, 148.27836738335668, 144.911829105461, 151.5437379384333, 151.15191062594832, 151.50056440400562, 150.97841811426747, 147.2315106216768, 153.9990286602748, 148.44181510345263, 144.98067093581415, 152.0405725889574, 147.705605233093, 151.23656076693356, 152.34463375540298, 146.7221705943198, 148.5107828082, 147.81893533067054, 144.7391252639645, 149.63123607301438, 150.53029917452957, 152.1804201016783, 146.80080856464514, 151.52200213870577, 147.65023305426172, 151.04786704919167, 145.77795968346163, 145.008118987328, 148.19435861165263, 148.62925899730368, 146.08050759355024, 149.41520959246947, 145.2440230498082, 147.03042314420406, 146.03492508604276, 147.2431440201279, 148.86437645496588, 148.097609856168, 150.7367954479433, 151.37823029835565, 147.50498389022613, 153.6571508581479, 148.10846191943548, 144.2044462804841, 145.73126737140115, 144.01305380025605, 144.29274849858348, 144.99372004812057, 144.0596905964329, 153.9136577776843, 158.2584762511644, 147.9331907863783, 145.6875315097934, 146.50328135528315, 145.23973485342748, 145.18200865509527, 146.0566014215436, 147.6600157492802, 147.85861892101454, 150.83701952353204, 144.3605661240208, 148.69070579500314, 148.8994576656002, 145.59705453201857, 147.86790734996285, 154.66919512260176, 148.68235046322732, 147.79765408995266, 145.8355566042738, 149.29756555529582, 147.50616114481812, 145.1772536001864, 152.6350471746872, 148.84364569681054, 144.8108536201576, 146.03387428292172, 146.76488837785035, 146.00381692467016, 147.05589027988984, 146.84547419439585, 146.9308368265449, 147.47166637665842, 144.28307272121864, 146.51580102886473, 145.72342838536062, 152.37629601874605, 148.16767566802318, 145.2249481334732, 143.35195137024, 147.13198698910725, 146.75019462237248, 146.16135719156142, 145.68165701983293, 144.75419457953677, 144.90484278705418, 151.10781014679543, 145.3305627308157, 146.01651487213906, 144.8308409093077, 151.41862988888573, 147.48828791553743, 145.97355596206157, 149.43688118141773, 151.14604984406716, 151.28738770880892, 149.62603578742065, 147.84558541188954, 146.20219784595807, 152.18961885156634, 147.51534567836677, 151.69587385009228, 148.70362167073205, 147.95037819301726, 155.49446453647658, 152.942569608862, 146.60380830390508, 151.5948304189082, 147.87504822768793, 145.95342657236387, 148.33296086058635, 147.66109120499894, 149.66840949323648, 151.39212201398865, 148.66619620151926, 145.9727735214566, 148.34501469931683, 152.47717373665904, 148.62502687901008, 146.33067493801929, 149.33020368819092, 147.12003852968263, 148.52882022842124, 147.48748497361575, 151.208804000871, 144.50106958578593, 147.67485649241718, 145.78004268964366, 145.05635029668264, 147.15615295478975, 151.34420882548474, 150.1874658050679, 145.07899511457418, 145.24779713106653, 149.61283208971045, 146.3846617363256, 149.49197615958894, 152.53601526347848, 148.90206088338897, 149.76410204364436, 150.6740381017275, 152.17607042222525, 145.7716961148493, 144.84075523150702, 149.13295695524988, 148.40944218493203, 151.4264805791649, 152.9831673833121, 143.51923806418665, 147.6665794924519, 147.71889005014748, 145.45712807977014, 151.46865323598325, 150.39572851852503, 148.4235435016926, 151.88481476522435, 145.9599339077606, 152.2706918261992, 145.2294770412571, 149.79894503402753, 150.9857673372404, 153.18317198325641]\n",
      "mean return 148.6696624935138\n",
      "std of return 6.481258041242787\n",
      "Rollout result. env: Hopper-v2 , policy_type: learned , returns: 200 / 148.6696624935138 / 6.481258041242787\n",
      "It took          31  seconds.\n",
      "loading and building expert policy\n",
      "obs (1, 11) (1, 11)\n",
      "loaded and built\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "returns [-3.605565575277394, -3.2286633069788713, -4.257122731304268, -4.364710683999166, -3.464448077143676, -3.437907436586668, -5.99755522862407, -4.119759219191178, -2.4062338635647724, -2.304955495722906, -1.6777718969611992, -3.7202716538153813, -3.5234650517664043, -1.5699938617205975, -5.057730126950071, -5.6939641640078555, -2.0386403737294736, -1.866215476258885, -1.8456534258416915, -7.3960153622562785, -3.3689330057206583, -1.9833065238409748, -3.2643249988316727, -4.966083545208151, -5.364258379206476, -0.6689779297758771, -6.876926875485094, -2.4763579259253974, -5.2808185152992255, -2.06354816053516, -3.59797849882472, -1.8301585415769301, -3.663653182772379, -2.289934196073799, -1.7906588992164518, -5.466837279011356, -1.0634643425294557, -3.039109614851873, -3.3132423789794814, -4.875697723092778, -6.4036528295379505, -7.724718434118034, -5.112951031284445, -3.4121224176092033, -3.924482921631388, -1.2655355051536643, -3.766339330928162, -6.595138861317096, -6.271584099279144, -1.2455980738211618, -3.3833608629673924, -3.0361006290727777, -3.8271634296667636, -2.286375545379157, -3.0249923468207105, -6.509049941330049, -1.1336134320789448, -4.1650094238945625, -4.5695783168514215, -2.8505385181029634, -5.167693342672611, -3.9939155481973407, -3.1703526506647273, -1.8124736786008946, -6.276838196476524, -3.170141556301005, -3.3759390909711584, -1.6699559846379761, -2.578381563561734, -5.919618755190541, -3.866259886167155, -3.1361930290809794, -4.565957084221296, -2.2482816074585976, -4.177354836543527, -6.768560787846901, -2.0259583662001015, -1.7379178245693443, -7.024110389190535, -7.050701904688057, -4.953626262281268, -6.0461310155412775, -1.9106285214345515, -4.549803206677612, -7.663592595686996, -1.958824448698707, -4.695539798508506, -2.4116245461099175, -1.4094386179136447, -3.4784758730868552, -5.875825290124784, -2.049943029382773, -6.725983911319591, -4.296916984111336, -3.0546855474126873, -6.991832541459289, -4.227882879050892, -7.280333013018689, -6.460827290506702, -6.235858608295461, -3.126832538030981, -1.5145786611455518, -4.522066748124085, -8.491493460214926, -3.115179559975622, -4.224640876736297, -7.232964205762568, -4.189257212849143, -3.9389669316354596, -3.2606372970039676, -4.859812356601925, -3.053863107805837, -2.8358205598328223, -1.8524421949749994, -5.769345400918985, -4.831028340222665, -6.017745069093909, -3.13631365255067, -3.8590364070302963, -3.3491168936084454, -6.422536216445934, -6.610177192728878, -6.240797690657545, -5.79299742409089, -3.3114047887910063, -2.956461494671612, -3.2239894470445423, -3.541042676048957, -4.156140648881274, -3.7551689823855363, -2.3352494255415435, -4.74768953846347, -4.240267472449536, -3.636305089288595, -5.07715590558977, -3.844919721941708, -5.107159420676341, -6.348657542227178, -2.3585109218635187, -2.0256294869222597, -5.400479706036299, -2.911998762182616, -1.74352400104521, -3.7927786960593632, -1.9874614408294446, -6.86666284078634, -5.184464606603781, -5.753109479215149, -2.3280341063147283, -3.7808251716924244, -3.2893196662367097, -4.662760318370023, -5.26331509342063, -3.1198252508414046, -3.2399299418411065, -3.428595448698077, -6.229773363626931, -3.976024031959043, -6.167354482057333, -3.568716375853816, -0.7564518479437448, -5.8871000394124895, -1.4203641427543998, -4.684139913652284, -3.7777521649815093, -6.860280917680679, -5.8289179514849945, -1.7995132699534433, -7.814070148887374, -4.3694488451061115, -5.987494257549582, -3.928311363665922, -0.36081984212588897, -7.838967037026014, -3.1302784063930886, -5.175987454151491, -3.358877731421289, -7.3611712577082695, -0.6436401125445379, -3.812098767379519, -6.172117244154164, -5.64712600448757, -7.268517873618485, -3.137896793384833, -5.899582324440071, -3.7112602452488845, -5.886190794000173, -7.4147226361995004, -2.6545070859478224, -7.069438831728686, -3.3764300361243746, -2.4494648427866528, -5.37502316882775, -3.9151876770643534, -3.3373459254542657, -4.504951327021089, -3.83577471882469, -5.422164237305709, -5.631443574896439, -4.588174964271681]\n",
      "mean return -4.125222803326378\n",
      "std of return 1.7802415864182275\n",
      "Rollout result. env: Reacher-v2 , policy_type: expert , returns: 200 / -4.125222803326378 / 1.7802415864182275\n",
      "It took          17  seconds.\n",
      "saved dir: model_Reacher-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "returns [-13.173626141272083, -12.86633745499232, -18.225319885132905, -17.033930378128623, -17.802351920047748, -16.965811045360763, -14.418780513391972, -4.0153119826382, -13.06559100118663, -14.578218782783239, -18.913082598010487, -14.346646597963408, -9.534869370351736, -12.901336078260899, -12.99491514370572, -10.992643777583842, -5.218438575517203, -8.669834560180135, -10.774649871523506, -11.896210933030162, -7.0532407425377, -4.869171486452828, -18.949288566366537, -8.26029055462114, -8.716347006110274, -9.226929003142052, -11.118897838864449, -16.167730859129673, -19.440963270027606, -9.121564996099059, -16.77580381143422, -14.673389462966139, -16.05185348191857, -16.42198140157962, -16.997989283044525, -17.413848321662456, -13.55274075683329, -10.352176697027032, -8.349361633489949, -13.193719147438213, -16.77137171622064, -16.498887453791117, -7.410692693828164, -7.638217077217143, -7.548166737169386, -17.831056215390223, -3.682196142848974, -5.968574669910952, -6.953321792450176, -8.400416230231038, -13.827946009099238, -11.714537788973152, -13.286968526937656, -16.50698772445233, -9.976240988261301, -11.397486894680283, -19.057285105205516, -17.403320992365554, -5.254659962894762, -10.695115405410535, -12.707243093879075, -15.854315340700857, -12.61876322896253, -13.605749879601081, -18.354896783827336, -13.348202578368326, -16.90842559744648, -12.903394103860723, -15.451852021675956, -14.759864575489402, -13.086334395333031, -11.931580281575833, -15.688451133738264, -14.30651375050936, -6.646261591328117, -13.74190298869973, -20.46062180306227, -10.45135335697509, -13.163821000283033, -16.497811983620778, -19.304578146795958, -16.61015905324096, -18.23337891701305, -10.202657801399036, -15.554550075089912, -18.95395556795855, -6.965723457967507, -10.15262028007371, -15.444622565551096, -18.21434219805049, -15.394864572865616, -15.357237698317357, -13.186763496772265, -4.508234242511809, -6.066295741532687, -18.57490850700556, -12.539917011963722, -13.142551115481918, -16.08045120815714, -15.947645008496805, -16.2996071248337, -6.950057725619481, -4.832826734108507, -11.101027586604456, -13.59430274809015, -15.532524494346548, -14.422729452701459, -18.142530837091225, -15.292977419618387, -4.151132703945779, -12.264626356432867, -16.223154188304353, -17.25546248646613, -11.05814236812352, -12.961705766822483, -14.734237208197221, -16.45384749850997, -18.98525711989601, -14.313377712990393, -10.975578078402528, -8.425869087214988, -3.584009299235682, -11.383504164291951, -15.12910555209479, -10.831847404761877, -19.07032525161553, -18.870069215862287, -15.23271108123636, -16.83657576826361, -2.4870339720789527, -16.285352487753745, -12.922246533389302, -8.327872839657536, -19.36567990233467, -15.013207621580433, -17.700759622636053, -16.537277702125568, -8.637092216808465, -11.40710941793214, -11.619623695910713, -10.73547695072244, -10.068144937479822, -15.64771210395949, -18.12931586513347, -12.931469651989719, -4.126411513561425, -11.548756251214739, -6.462628882403309, -14.646032794607828, -12.20899609426573, -7.931262662583788, -15.348110719436804, -11.934946014249663, -15.174646557562328, -9.612563925831466, -17.32522089304652, -13.652119821721135, -8.870024701724443, -13.344838363698452, -9.406424023344043, -15.23550254200519, -16.1991089156683, -3.861089723220398, -10.717466466642394, -10.989535086907345, -4.112901356689544, -7.529520416083324, -4.866748121253761, -10.57030135542535, -17.797769278526154, -10.556862412111341, -16.83884951564943, -16.707383364539137, -16.20153652838615, -16.703222304232444, -16.726212030433956, -7.240101558087713, -10.167235497640046, -10.434473407877945, -13.262016234658663, -7.148785526469849, -17.240718828385784, -12.519256459400046, -15.496685811703605, -14.618256069470537, -11.391630820188333, -9.462082007407115, -14.300641821192466, -14.602481675261263, -15.952562299063016, -14.844217297124299, -17.055567064299847, -19.518451455925167, -17.990309676862235, -15.696098287678465, -3.729555808539018, -16.203692887875853, -11.868582506621793, -19.512686395218456, -10.034265405456768]\n",
      "mean return -12.93031653305468\n",
      "std of return 4.211620413211782\n",
      "Rollout result. env: Reacher-v2 , policy_type: learned , returns: 200 / -12.93031653305468 / 4.211620413211782\n",
      "It took          19  seconds.\n",
      "loading and building expert policy\n",
      "obs (1, 17) (1, 17)\n",
      "loaded and built\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "returns [4113.959235730012, 4169.1524737528525, 4200.152231157243, 4046.873171832371, 4155.901383651581, 4168.593100582697, 4107.80127276168, 4259.302072688185, 4114.434136900747, 4139.665619064314, 4214.9135499279055, 4216.606666210238, 4098.214976828147, 4084.886418843709, 4193.356397057088, 4179.683333981084, 4258.601953529324, 4200.409701148162, 4227.933786421065, 3980.4689384108656, 4310.15468399496, 4206.605978815626, 4173.267301681879, 4183.428534930118, 4210.4837610491295, 4152.486454609928, 4064.097093685041, 4222.789221178362, 4101.085050767516, 4135.494506070956, 4137.318794564381, 4127.44930273277, 4074.696124997134, 4154.095451611743, 4104.378336514534, 3978.106656738602, 4150.543407940362, 4344.198685795859, 4065.8994697878466, 4129.988831627757, 4122.626444775927, 4321.430071489352, 4197.410999392608, 4284.548192993906, 4162.736553984016, 4232.10663127951, 4156.283981677652, 4101.26279755811, 4161.608897216708, 4093.0659546206534, 4195.803478272714, 4211.014504727755, 4167.706601366746, 4057.8416786030994, 4080.691303581414, 4080.4856121534613, 4130.975396751112, 4153.893309307783, 4246.165436718705, 4161.840986200311, 3982.3758731758962, 4121.328348763968, 4173.892860464995, 4220.346141121369, 4175.882590891378, 4211.730831328492, 4131.523811844627, 4176.547333705685, 4120.091708531218, 4282.41547126357, 4247.696456639574, 4304.821041854728, 4119.918135354829, 4089.1952411152347, 4067.5681300223123, 4037.929687872247, 4213.194034329682, 4157.181270523183, 4190.392905851886, 4272.399828279179, 4167.555615787321, 4179.210406076761, 4145.24640651543, 4127.33758373156, 4197.136712862772, 4075.133769854566, 4077.784083026949, 4222.093047649454, 4058.2981956014264, 4240.864436064003, 4164.301462969908, 4099.5287694339995, 3883.265547562842, 4105.949499559552, 4243.6440975573105, 3946.764696048753, 4122.684773897549, 4198.425605267371, 4138.184763724525, 4105.595158916516, 4140.927745319127, 4120.1562615250505, 4154.624177478938, 4048.300016249757, 4114.353606590327, 4163.220428165793, 4096.332089288217, 4194.286806827701, 4260.767165804745, 4223.655586558139, 4273.8950355050665, 4102.387984542338, 3972.288117112105, 4150.839701542303, 4145.719187179992, 4183.277552718723, 4150.583658966331, 4075.7425397627762, 4015.962488314125, 4199.0252645808105, 4105.988151320936, 4093.7923923418243, 4115.394046320602, 4266.383379674579, 4081.1365813599673, 4203.505062792367, 4085.2076744402366, 4152.628448647569, 4061.614254257829, 4223.602646071964, 4136.6659019064355, 4124.471676047775, 4040.4942845599785, 4224.789655237703, 4170.743039735067, 4150.206073197711, 4087.1843252335884, 4234.579897575031, 4102.851663083196, 4131.186060614595, 4229.991951476073, 4252.815637935142, 4073.155985343596, 4138.665143323317, 4099.294355048561, 3984.567211024118, 4160.290424695156, 4012.471372236892, 4082.3973644493713, 4164.963461978293, 4221.459852595032, 4016.247808741213, 4187.541639945882, 4172.299308402936, 4069.2771936100753, 4171.61169486744, 4134.767913267673, 4162.831352146251, 4090.549813091206, 4154.820245971118, 4163.661120191407, 3994.396017370257, 4047.339922942348, 4128.587783591724, 4151.486301234745, 4264.761211689146, 4219.819229464771, 4030.450094210498, 4162.73706315147, 4162.415873305035, 3997.871712278855, 4081.4091205316886, 4176.106719455518, 4103.634460567711, 4112.559757974717, 4211.00787491491, 4032.8140053592, 4085.554262811606, 4219.924252438806, 4097.635267434057, 4154.807609360938, 4202.416342163545, 4298.590725392854, 4006.8014042395635, 4086.2655469376164, 4158.498406500654, 4100.160233070471, 4191.835324532921, 4252.994757830552, 4231.1775458679895, 4098.743435720592, 4064.700881495058, 4141.294203632537, 4110.189197077215, 4164.518499017762, 4166.21741792659, 4108.891058873076, 4146.24141657472, 4122.663849651953, 4178.552940524746]\n",
      "mean return 4144.304682215992\n",
      "std of return 75.58101190586441\n",
      "Rollout result. env: HalfCheetah-v2 , policy_type: expert , returns: 200 / 4144.304682215992 / 75.58101190586441\n",
      "It took         391  seconds.\n",
      "saved dir: model_HalfCheetah-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "returns [2156.4422208231567, 2331.45879366219, 2180.928480070543, 2297.7675737858303, 2311.1475745257994, 2195.7018937992984, 2173.657546609962, 2351.0247134594424, 2389.1105232771597, 2311.0136646569686, 2416.706299084199, 2297.285465512336, 2457.2266322693495, 2259.519118883818, 1879.3339872980519, 2058.3685212215973, 2451.618985296258, 2265.306122832969, 2197.23113095047, 2173.7743934827404, 2274.818838897442, 2024.053396918786, 2460.885854379403, 2491.661145028721, 2255.7993526935975, 2090.677623195685, 2158.7553544332413, 2157.0877470582386, 2253.5774412049836, 1975.0127979740885, 2080.391337647773, 2527.055480554031, 2293.772643482141, 2552.609149592063, 2325.5697107331853, 2238.102063678915, 2086.0596316537835, 2282.6625871769197, 1959.0506346945942, 2320.242070835248, 2140.8108050956257, 2151.5343503399354, 2256.0782294211335, 2140.2738411604937, 2119.3598924100443, 2278.1764841534396, 2249.6450468501084, 2454.8539981348113, 2466.8698002569504, 2286.466176628092, 2342.8350655754407, 2086.65329901818, 2456.344637270103, 2699.612095510947, 2457.9512737684404, 2411.48629209905, 2132.190022575781, 2494.4861821032177, 2189.9611975402404, 1963.7633042023717, 2140.022740762492, 2319.365238586723, 2245.5886913686354, 2335.289366130957, 2022.214903905367, 2041.272492358816, 2435.808192199137, 2181.1288436334626, 2619.8710538547566, 2213.030432864215, 2693.3765990981046, 2072.1653833781734, 2030.4933124971908, 2393.694088151227, 2224.115059380323, 2285.0228460825047, 2377.539129159189, 2020.433353049205, 2054.009808840276, 2299.2819929346133, 2374.5839831777507, 2303.298325551467, 2227.311349424449, 1898.0770574988944, 2447.100636904405, 2052.556715026128, 2338.559918601783, 2284.3199643278085, 2027.6320449773061, 2294.777497690062, 2066.6168627712136, 2092.936009005048, 1966.4895480057078, 2065.901399796173, 2339.661177996203, 2294.174314868572, 1987.18952586692, 2074.3496854904697, 2260.711746394505, 1909.7224985267264, 2415.393973642906, 2059.8396768280027, 2428.4262721540895, 2260.4736490025284, 2113.3814296120768, 2401.0292739341576, 2065.685602182856, 2185.6602650603963, 2399.729120122122, 2295.4718284669484, 2550.9439913976366, 2179.433430669371, 2001.752463200883, 2134.924390192475, 2077.2340588021684, 2385.6651762111305, 2389.777823254548, 2091.139365336528, 2454.3794743982253, 2302.677908131752, 2130.4506232893877, 2209.579452604584, 2318.86597125101, 2280.9815108260573, 2269.7219395643006, 2217.913976322256, 1945.5069929968065, 2415.8254433491934, 2178.091309603349, 2111.5025628030576, 2181.4419040161542, 2454.6926487781648, 2165.5375814010686, 2388.590139258515, 2210.269663050061, 2434.6233585219106, 2317.023319061759, 2392.973243411176, 1815.4491868081068, 2334.539455527779, 2283.629408494987, 2180.489654937875, 2354.9849555670307, 2370.7896318463995, 1883.7477789937568, 2206.2790385879625, 2321.19044525568, 2273.1887376518075, 2145.9993227316263, 2809.898334708845, 1750.648231672938, 2440.7130597504342, 2262.1871197444925, 2229.473601386758, 2549.735510446654, 2060.6362081699226, 2526.486223498303, 2166.3499222125106, 2193.0484797051536, 2222.686939456401, 1907.9324717325023, 2062.8504525759945, 2083.139367261743, 2161.372130625486, 2462.7170424694577, 2131.6867734371954, 2409.855945257388, 2032.4356327699443, 2239.324438912674, 2420.0975734587955, 2309.8737998158613, 2017.5138568337575, 2360.144309462041, 2475.3663596750166, 2211.271842854906, 2193.4474533986504, 2368.5242822384494, 2540.3780733115136, 2091.275202660835, 1998.0603966496744, 2038.801998520878, 2401.042559024693, 2091.399923342186, 2157.209670654203, 2144.372799851756, 2104.7300520422086, 2055.097777252305, 2186.804253507166, 2821.0430367238614, 2343.28357177396, 2028.509935604601, 2647.8410815988414, 2230.423035394171, 2246.75656028612, 2212.495017287536, 2328.37572075653, 2507.0348068830817, 2233.9142168144617, 2052.974850775109, 2307.3582717888785]\n",
      "mean return 2244.3350644636794\n",
      "std of return 180.08113660709785\n",
      "Rollout result. env: HalfCheetah-v2 , policy_type: learned , returns: 200 / 2244.3350644636794 / 180.08113660709785\n",
      "It took         504  seconds.\n",
      "loading and building expert policy\n",
      "obs (1, 376) (1, 376)\n",
      "loaded and built\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "returns [10313.660925581837, 10492.009077850218, 10436.79413931007, 10420.496306244619, 10362.75881306289, 10428.558595867658, 10455.331546486566, 10440.483289297663, 10382.367582213248, 10414.888304048498, 10381.332158320485, 10449.03204128693, 10317.60712744805, 10367.476366574747, 10432.048328373807, 10461.567789085622, 10404.395365126451, 10379.735094035166, 10437.412744562413, 10480.578938104665, 10489.806724168044, 10466.660905419374, 10367.760611773749, 10405.265233281847, 10496.245779467703, 10416.973998077743, 10487.900363581282, 10379.070059041334, 10371.978633450964, 10464.57872963014, 10419.100744870744, 10395.734911363981, 10431.03373290189, 10448.833584433398, 10222.718702090991, 10424.707139016871, 10431.24749760683, 10336.459087930543, 10474.150568024279, 10373.268720590797, 10467.263578877297, 10414.99150635158, 10321.105347138067, 10436.01752807939, 10434.142608819979, 10373.623898355572, 10455.28282195528, 10398.744403948132, 10480.503610071142, 10386.497937209822, 10296.011574569626, 10419.482452131242, 10455.677278310983, 10481.654378654132, 10334.625672865079, 10421.332457836059, 10389.239048245005, 10466.71723409939, 10412.75078044275, 10484.649007490192, 10390.097974499304, 10241.795308048431, 10422.033954776145, 10423.625324966975, 10306.675523444872, 10420.558353767163, 10457.736278224418, 10363.803409865759, 10358.32702187291, 10498.695952666225, 10409.201677902138, 10331.391968697308, 10384.536084365702, 10342.341988285698, 10423.153069976357, 10416.545810707199, 10468.306029250645, 10392.641149776715, 10365.280252471432, 10302.24246958894, 10434.13252521309, 10460.097214103485, 10433.90057708173, 10470.89033513393, 10486.062666099591, 10488.889440743656, 10298.794403881857, 10451.096962528796, 10449.364331431094, 10460.302511481981, 10382.028481584497, 10397.755699438041, 10358.635204789083, 10411.759656691647, 10393.095345463433, 10395.057452124778, 10421.467034664754, 10457.409494661339, 10352.782328266063, 10344.797346652784, 10446.4763218414, 10403.142523286368, 10382.549893125873, 10386.151228727711, 10502.407984193562, 10476.497023507947, 10398.903308980478, 10460.867179603856, 10468.886501933091, 10463.606416022154, 10403.449881644281, 10455.617230090429, 10304.999373166842, 10413.3015853015, 10452.025983563752, 10347.46846680665, 10418.859569453094, 10282.25993530101, 10357.104000027757, 10383.026767977291, 10341.997251620909, 10393.193697548833, 10368.589279456608, 10444.156181379576, 10409.180381883247, 10365.408376379957, 10336.710658656042, 10386.963652539998, 10457.973968620545, 10396.808448435926, 10312.159203011248, 10442.238997524313, 10419.798846151147, 10364.490403654443, 10449.454742021482, 10443.217451197894, 10399.093195599356, 10415.100826539388, 10422.403173588198, 10500.737991804295, 10461.052046912608, 10464.84196078763, 10408.21529456714, 10406.862210319772, 10434.263103513515, 10264.615277536002, 10339.373453278697, 10372.807212373931, 10471.7187447952, 10438.086776464794, 10402.743342321975, 10300.160326856054, 10361.251108850143, 10437.446132764275, 10494.042602965279, 10168.152130218903, 10399.212666678866, 10505.112042293338, 10448.360661440407, 10369.76154657323, 10414.549507404372, 10392.883455447705, 10466.978943874241, 10392.428565841652, 10390.755052383429, 10411.056029800267, 10410.6517006767, 10375.465373435516, 10342.308295860188, 10447.920145649994, 10467.109521045031, 10398.632896965844, 10411.75141466298, 10458.202547215376, 10406.940635868992, 10423.74237865308, 10345.938642845213, 10394.575970514485, 10297.973197188143, 10399.778900242072, 10410.027413564869, 10413.829494039344, 10426.016966141518, 10398.304597603297, 531.135895744783, 10356.03761391615, 10401.402839962313, 10349.518463001923, 10426.28978396606, 10343.912646046694, 10397.527059423825, 10402.385403711913, 10426.320833051268, 10431.502649342077, 10463.85897862085, 10382.658708597277, 10439.79785490118, 10357.141308558777, 10388.531867606653, 10453.492353068516]\n",
      "mean return 10357.17172225185\n",
      "std of return 698.7286180857941\n",
      "Rollout result. env: Humanoid-v2 , policy_type: expert , returns: 200 / 10357.17172225185 / 698.7286180857941\n",
      "It took         659  seconds.\n",
      "saved dir: model_Humanoid-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "returns [519.2117900542669, 415.08779678868564, 373.18572882698413, 528.307889146855, 399.07800085489475, 512.8330148352435, 354.40985841013526, 434.92768949435214, 444.06102740988314, 404.29139842959466, 396.5357240124504, 443.31624252822206, 465.2409293627934, 367.31118991974165, 404.1800958510094, 482.54771198400886, 489.9760521728007, 421.7855041036711, 597.4542271364751, 388.2346926439093, 379.38816272031585, 378.42525183736836, 422.8907064114434, 408.4104075553197, 434.2931116613672, 400.3159644034595, 452.2322494345212, 455.5728251525506, 370.85770943470635, 400.5842805326789, 405.4708419783868, 375.94790171647105, 462.5366217703378, 398.1602606311686, 357.81922215505426, 485.73815970466825, 452.1766388547212, 368.53295416662263, 441.10641413770844, 569.0282642935414, 550.2401486610213, 530.7037789003865, 487.5789379006608, 488.29861264344015, 453.02674631097574, 395.02956192254226, 445.52071101826516, 399.51196255086126, 416.4930175679354, 375.59188984767917, 380.1817149747187, 451.77945999959553, 618.4363036890662, 472.2873654606809, 387.59580927612143, 416.7962660847831, 411.9401020580949, 467.3651485028099, 365.2767075554824, 449.1657199617383, 532.9583759111858, 537.037808656905, 506.9041381905344, 452.1545009703676, 408.58212037106387, 395.2935202763141, 513.0228570140591, 401.980874333876, 370.2216978425141, 502.46927007188987, 496.57529336708836, 367.9735835187607, 449.2836947785777, 490.84002577834065, 490.7860527923855, 391.28230121156645, 463.12259089242644, 438.7367170140029, 569.847499399107, 466.4026195553551, 487.8061383245975, 453.44695239596115, 381.18729435862474, 409.4871028455779, 522.2543207994993, 540.7348744280706, 476.37165821716883, 498.36280891074153, 421.6964364558272, 381.7332610757659, 386.1235139916332, 480.28427950840234, 373.7772509235065, 424.4249498898364, 368.09088503755635, 442.03117013523627, 468.4461342955706, 406.88755720303067, 474.6429288610212, 422.20576457012584, 464.114317818864, 452.29682484155177, 419.7143669764379, 447.4396097855905, 339.93832992124436, 466.4184382124565, 390.69805164950077, 488.1087228750909, 536.0457636722289, 507.6040163802253, 411.16468443661114, 443.07116840013737, 443.8287757280582, 499.5182198081478, 373.11761122139967, 550.4881173581067, 375.67411356298055, 501.9469227439603, 498.21496899794977, 398.7278146890339, 496.58470107143273, 432.7181822822645, 396.06550266962836, 471.1335617352814, 387.3714141414228, 368.780100029509, 444.78357983717086, 450.0885851382829, 466.4768982202643, 451.9862976222061, 370.96710952775095, 397.35976671538276, 567.132898469919, 451.3707549942045, 486.4850340619084, 529.5923372788271, 466.435565470142, 384.05955929808675, 479.3548921978465, 407.0064359038385, 458.90076187064267, 457.5319858772501, 577.7206311011158, 430.44671799353955, 484.1751957103654, 478.49797776149103, 416.9763796020099, 368.12910232528293, 398.9497269497981, 426.1318785364306, 382.84543938672033, 350.8732844866173, 438.78927629431513, 502.2041678563087, 545.2117006842093, 411.71298177230284, 405.32138813200453, 379.98335644859833, 387.62741623715016, 467.15729681226054, 534.4575809981686, 390.182104858204, 507.31539982861176, 535.038676328653, 440.6618871671533, 539.5724729597649, 512.7450301211971, 494.0322273976839, 374.6786901244684, 418.7243376447956, 403.67295982370064, 536.6322660385156, 412.8826581719932, 434.55857316462135, 460.5052770512042, 391.44927832926845, 498.50918547740565, 470.6182310876676, 381.1723781011166, 474.93889080980875, 492.74649155784124, 491.3379448500742, 544.9842624632183, 442.0993231891419, 387.50669560825304, 505.92655726229265, 460.28624280129014, 400.96228202016783, 486.1988696908186, 424.64555289626543, 513.8957841794223, 455.53727759213706, 419.31561664910726, 439.54774810978745, 403.51040913126496, 383.35362603890036, 428.31748924455167, 574.9730861511491, 432.5038457198759, 445.9910161018884]\n",
      "mean return 446.5940710495207\n",
      "std of return 56.643693609146005\n",
      "Rollout result. env: Humanoid-v2 , policy_type: learned , returns: 200 / 446.5940710495207 / 56.643693609146005\n",
      "It took          74  seconds.\n",
      "loading and building expert policy\n",
      "obs (1, 17) (1, 17)\n",
      "loaded and built\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "returns [5531.222906361502, 5486.059343277904, 5524.728385233633, 5508.520180175027, 5522.614641408246, 5484.618461657045, 5540.3125953943, 5575.759455256447, 5448.207708794152, 5569.330445943332, 5339.55072983839, 5544.603066638932, 5457.694110577478, 5340.545813117958, 5463.818350987079, 5479.088229818731, 5428.919063244933, 5586.4414322954335, 5458.435835008507, 5604.104416128711, 5538.527715348434, 5488.009938537147, 5520.9465347120495, 5565.038927473312, 5605.835625023702, 5441.031442361532, 5506.49824265893, 5546.447955028823, 5505.086369052059, 5494.447074035428, 5511.793766227672, 5545.173576795007, 5504.350863877126, 5486.619342016107, 5531.837628573736, 5475.248104993521, 5535.036804963751, 5527.275846108503, 5562.027831586195, 5589.87950020147, 5555.852525530864, 5537.007275768173, 5556.015650071614, 5539.661948459617, 5552.44453987202, 5504.01505856424, 5533.463485371293, 5512.801935175464, 5561.383933734486, 5505.962364758397, 5524.252783330921, 5488.135994512408, 5564.642759166572, 5536.441828877297, 5509.470872671211, 5493.09486465241, 5182.530821778893, 5532.647157812314, 5581.67644474847, 5579.188009796445, 5527.955190852463, 5527.96853323934, 5493.609569341911, 5567.922251514795, 5502.651944807515, 5525.901600244163, 5535.494033970712, 5558.424257690983, 5580.027600169492, 5568.872008423842, 5534.776598912367, 5524.951953555899, 5575.980318623164, 5549.039621722154, 5475.349843836728, 5485.376493711118, 5608.062610603676, 5564.232853446737, 5456.271962972465, 5594.9863791774105, 5554.036003192261, 5577.962430843127, 4819.642260098997, 5570.555873159331, 5491.147517484959, 5558.503762208568, 5601.658979219664, 5574.588430974223, 5557.49351530709, 5509.568835889437, 5458.509608979318, 5462.980907751109, 5526.582093138385, 5577.010912346068, 5532.3385355939545, 5500.901437301228, 5437.523380502214, 5467.854348538091, 5591.205978327442, 5538.356415665161, 5440.502157067772, 5526.915617188927, 5412.943501285499, 5520.82176267287, 5528.955062347599, 5538.974577792776, 5421.868879241327, 5521.232301731887, 5580.335815069302, 5553.885993913151, 5489.382414418103, 5414.353253640177, 5513.830247505554, 5561.075342104973, 5520.529816017738, 5543.912949918063, 5549.365014985064, 5602.96007450273, 5511.47728485686, 5522.126732603781, 5516.7483508526875, 5465.0934334358635, 5569.5677178837095, 5528.873986249816, 5568.241658207295, 5519.667046341379, 5564.327754031057, 5572.6075031357495, 5595.950014677589, 5557.966508307675, 5525.898513936194, 5439.634058433974, 5488.120241501738, 5443.532939729008, 5542.62379361447, 5578.736164031492, 5575.0877424317505, 5453.36786775278, 5534.856797020579, 5498.4821160456395, 5551.19213784157, 5501.442116622691, 5569.375248983643, 5580.309755165796, 5555.653572395602, 5532.4692115197195, 5540.395294062655, 5576.358232383194, 5454.31698025603, 5577.049378882067, 5501.910349408386, 5580.588129127699, 5575.290508510241, 5436.970714599381, 5560.1595840504715, 5610.276053321475, 5564.506594644555, 5525.753818170476, 5549.384510772929, 5541.215522848264, 5465.157097695573, 5584.091287450077, 5383.421068620719, 5608.898007071398, 5542.476270697374, 5572.025338858602, 5486.681218746316, 5574.315604946441, 5371.4176746288, 5552.539963700162, 5590.698701870022, 5581.789776793498, 5485.898657934248, 5533.037565128869, 5592.285900595948, 5550.356876237122, 5460.337577078767, 5553.0513487012895, 5459.44588665773, 5572.377543672851, 5505.578628658799, 5505.760408830906, 5548.275666306841, 5513.683695551825, 5493.81676564262, 5538.995904874093, 5588.016307683485, 5501.845005369043, 5493.214459298746, 5545.657355479434, 5548.277493050578, 5478.751458400305, 5565.476082139746, 5566.461010714544, 5503.269923414645, 5452.407837700245, 5569.889833299705, 5607.148560267797, 5524.5907174828135, 5459.311394856936]\n",
      "mean return 5520.614189824417\n",
      "std of return 74.8640488464037\n",
      "Rollout result. env: Walker2d-v2 , policy_type: expert , returns: 200 / 5520.614189824417 / 74.8640488464037\n",
      "It took         448  seconds.\n",
      "saved dir: model_Walker2d-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "returns [1077.5329880416634, 1061.6866193711148, 1070.7566871447275, 1025.5515912437334, 1065.9237366166199, 13.69988813526635, 1085.508546580086, 1024.4589096888096, 1080.867390076598, 1024.030125767916, 1026.4108975136612, 1030.524257642515, 14.411071485458422, 1072.4373833745403, 1023.9746447726269, 13.184998630413926, 1064.781238780489, 1067.1776532290987, 1032.1645634742304, 1032.0023641171867, 1070.7573042840154, 1024.1951062273286, 13.512961249988464, 1073.6872373039232, 1076.9916850943341, 1064.227149087487, 13.298760933043658, 1083.2446196406006, 1028.7329175382913, 1064.1878260005003, 1066.2667672353607, 1062.0876666496551, 13.338518498623175, 12.6947892479326, 1073.020203636702, 1074.794870809656, 1070.7343391310326, 1062.3005942924383, 1073.9746346333357, 13.23880059907151, 1023.6601059979288, 1024.3891231107436, 1073.360792761852, 14.535662286127474, 1024.19501914804, 1068.9509115583285, 1076.4248867323947, 1085.9566323765903, 1069.5603044515296, 1023.4109559230978, 1024.0208393240846, 1074.0289922629693, 1072.4282511917877, 1065.9131602626662, 1018.2920233480722, 13.95313153263427, 1059.04306259499, 1072.3556158581837, 1024.7495888191577, 13.118544711411708, 1025.7258748405643, 1065.3097089626754, 13.948662926568275, 1025.7675899785722, 1068.6447495085074, 1075.8059036711313, 1072.5106394089928, 1068.1199855553882, 1023.7300339379601, 1026.525742371512, 1024.4132835513776, 1025.5237219735343, 1068.288720208395, 13.23478889883555, 1027.8624822173085, 1064.3353246675956, 14.347550481143564, 1027.7566328058374, 13.479520000397066, 1067.9204457100848, 1023.8257206981706, 13.595789708085787, 13.907302007554794, 1023.7436149440058, 13.436598582272163, 1030.6941237660637, 1023.8430562427465, 13.343960818511693, 1023.8801537864165, 1023.622768378622, 1070.2514469141774, 13.430134496482776, 1080.5342566816641, 1068.288581074352, 1074.278090122722, 1023.7117059376974, 13.268921491890456, 13.238775629977376, 1077.6275529710933, 1025.1523992499897, 1031.927427214828, 13.645825365129856, 1026.1059296051073, 14.2592275892312, 1088.391052100804, 13.472448732360442, 1063.4266583752124, 1023.1436515362504, 1066.6496570357715, 1024.3590457154087, 1061.206473761942, 13.7426309093678, 1068.5430286239334, 1070.2264327648088, 13.232771451248597, 1023.9560568844967, 14.173856743539915, 1071.211089877948, 1068.6593460297458, 1074.8657079478073, 1023.5287699840393, 1069.511480958833, 1024.304510007612, 1032.4858378883484, 1065.579820367178, 13.37864260998869, 13.92853534193492, 1071.5276505286895, 1066.5987926472783, 1075.3567199201173, 1067.579789484214, 1080.5133067385311, 14.31356448531051, 1073.4798403646953, 1072.1333108495799, 1024.0048885514714, 1064.7245394922359, 14.207412590046713, 1024.7318369609338, 1074.968978595388, 1070.8222302543195, 1068.9041004766655, 1059.1762126397086, 1083.8250466276131, 1062.6090351152168, 1071.5430375929568, 1023.6965310545861, 1067.4204093508802, 13.986942737188748, 1025.2524748836743, 1060.1866362111134, 1078.40568914606, 1072.400205380619, 1085.717123121487, 1079.7038696549957, 13.876776426899015, 1024.3072210641885, 1024.677283007228, 1024.092909906256, 1069.3282285368582, 1024.6258786330459, 1026.1582924310937, 14.013194953212588, 1026.034701640491, 1025.07479723819, 14.020305544655752, 1078.287540002579, 1068.6587297959302, 1067.0421137626215, 14.007525944357845, 12.936318897629837, 1023.1582562430682, 1073.9617177601922, 13.94349749399928, 1064.4342142328005, 1023.9209978685986, 1024.581156092234, 1065.0421229575459, 1061.6064417602174, 1024.8597863025313, 1023.5643755124207, 1025.4437914160164, 1025.084512770784, 1074.3021608581282, 1062.1981871979433, 1076.2048120971644, 1023.8341043481172, 1025.261449757786, 13.802987057785689, 1025.487281139183, 1071.6259499610721, 1025.0424340522125, 1069.5938334926707, 1021.3011877577065, 1070.8760868855647, 1023.9945076318801, 1071.3054595235437, 1072.3888378463705, 1064.9580960197618, 1067.1096586058034]\n",
      "mean return 844.5669102025917\n",
      "std of return 415.95183499422893\n",
      "Rollout result. env: Walker2d-v2 , policy_type: learned , returns: 200 / 844.5669102025917 / 415.95183499422893\n",
      "It took         448  seconds.\n"
     ]
    }
   ],
   "source": [
    "# rollout and check\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "max_timesteps = None\n",
    "num_rollouts = 200\n",
    "\n",
    "df = pd.DataFrame(columns=['gymenv', 'policy_type', 'rollouts', 'mean', 'std'])\n",
    "\n",
    "for gym_env in gym_envs :\n",
    "    for expert_policy in [ True, False] :\n",
    "        start_time = dt.datetime.now()\n",
    "        rollout_data, policy_type, _ = rollout_by_policy(gym_env, max_timesteps, num_rollouts,\n",
    "                                                      policy_fn=None if expert_policy else load_learned_policy_fn(gym_env),\n",
    "                                                      render=False)\n",
    "        returns = rollout_data['returns']\n",
    "        end_time = dt.datetime.now()\n",
    "        print('Rollout result. env:', gym_env, ', policy_type:', policy_type, ', returns:', len(returns), '/', np.mean(returns), '/', np.std(returns))\n",
    "        df = df.append({'gymenv':gym_env, 'policy_type':policy_type, 'rollouts': len(returns), 'mean': np.mean(returns), 'std': np.std(returns)}, ignore_index=True)\n",
    "        print('It took ', '%10d' % ((end_time - start_time).total_seconds()), ' seconds.')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gymenv</th>\n",
       "      <th>policy_type</th>\n",
       "      <th>rollouts</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ant-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>200</td>\n",
       "      <td>4772.544028</td>\n",
       "      <td>422.163975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ant-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>200</td>\n",
       "      <td>926.068148</td>\n",
       "      <td>140.414594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hopper-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>200</td>\n",
       "      <td>3778.515927</td>\n",
       "      <td>3.775791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hopper-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>200</td>\n",
       "      <td>148.669662</td>\n",
       "      <td>6.481258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reacher-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>200</td>\n",
       "      <td>-4.125223</td>\n",
       "      <td>1.780242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reacher-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>200</td>\n",
       "      <td>-12.930317</td>\n",
       "      <td>4.211620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HalfCheetah-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>200</td>\n",
       "      <td>4144.304682</td>\n",
       "      <td>75.581012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HalfCheetah-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>200</td>\n",
       "      <td>2244.335064</td>\n",
       "      <td>180.081137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Humanoid-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>200</td>\n",
       "      <td>10357.171722</td>\n",
       "      <td>698.728618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Humanoid-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>200</td>\n",
       "      <td>446.594071</td>\n",
       "      <td>56.643694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Walker2d-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>200</td>\n",
       "      <td>5520.614190</td>\n",
       "      <td>74.864049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Walker2d-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>200</td>\n",
       "      <td>844.566910</td>\n",
       "      <td>415.951835</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            gymenv policy_type rollouts          mean         std\n",
       "0           Ant-v2      expert      200   4772.544028  422.163975\n",
       "1           Ant-v2     learned      200    926.068148  140.414594\n",
       "2        Hopper-v2      expert      200   3778.515927    3.775791\n",
       "3        Hopper-v2     learned      200    148.669662    6.481258\n",
       "4       Reacher-v2      expert      200     -4.125223    1.780242\n",
       "5       Reacher-v2     learned      200    -12.930317    4.211620\n",
       "6   HalfCheetah-v2      expert      200   4144.304682   75.581012\n",
       "7   HalfCheetah-v2     learned      200   2244.335064  180.081137\n",
       "8      Humanoid-v2      expert      200  10357.171722  698.728618\n",
       "9      Humanoid-v2     learned      200    446.594071   56.643694\n",
       "10     Walker2d-v2      expert      200   5520.614190   74.864049\n",
       "11     Walker2d-v2     learned      200    844.566910  415.951835"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## DAgger\n",
    "\n",
    "1. Implement DAgger. See the code provided in run expert.py to see how to query the expert policy and perform roll-outs in the environment.\n",
    "\n",
    "2. Run DAgger and report results on one task in which DAgger can learn a better policy than behavioral cloning.\n",
    "Report your results in the form of a learning curve, plotting the number of DAgger iterations vs. the policy’s mean return,\n",
    "with error bars to show the standard deviation.\n",
    "\n",
    "Include the performance of the expert policy and the behavioral cloning agent on the same plot.\n",
    "In the caption, state which task you used, and any details regarding network architecture, amount of data, etc. (as in the previous section).\n",
    "\n",
    "### note\n",
    "1. DAgger needs labeling by human experts.\n",
    "1. The main idea is that the trajectories are collected by the learned policy. but the action is relabeled by the expert policy.\n",
    "1. DAgger addresses the problem of distributional “drift”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "def rollout_by_dagger(gym_env, max_timesteps, num_rollouts, num_epochs=50, render=False) :\n",
    "    policy_type = 'dagger'\n",
    "    \n",
    "    print('loading and building learned policy')\n",
    "    policy_fn = load_learned_policy_fn(gym_env)\n",
    "    print('loaded and built')\n",
    "\n",
    "    print('starting dagger ', gym_env, dt.datetime.now())\n",
    "    train_observations, train_actions = load_expert_data(gym_env)\n",
    "    obs_shape, action_shape = np.shape(train_observations), np.shape(train_actions)\n",
    "    if action_shape[1] == 1 :\n",
    "        train_actions = np.reshape(train_actions, (action_shape[0], action_shape[2]))\n",
    "        action_shape = np.shape(train_actions)\n",
    "    print(gym_env, ' observation shape: ', obs_shape, ', actions shape:', action_shape)\n",
    "    \n",
    "    with tf.Session(graph=tf.Graph()) as session, session.graph.as_default() : # for session nesting, the graphs should be isolated for each tf sessions\n",
    "        print('loading and building expert policy for DAgger')\n",
    "        expert_policy_fn = load_expert_policy_fn(gym_env)\n",
    "        print('loaded and built for DAgger')\n",
    "\n",
    "        tf_util.initialize()\n",
    "\n",
    "        gym_env_model = 'model_' + gym_env\n",
    "        gym_env_dagger_model = 'model_dagger_' + gym_env # new model file to save after lite training\n",
    "        light_train_config = default_train_config.copy()\n",
    "        light_train_config['num_epochs'] = num_epochs\n",
    "        cloning_model = None\n",
    "\n",
    "        env = gym.make(gym_env)\n",
    "        max_steps = max_timesteps or env.spec.timestep_limit\n",
    "\n",
    "        returns = []\n",
    "        observations = []\n",
    "        actions = []\n",
    "\n",
    "        for i in range(num_rollouts):\n",
    "            # print('iter', i)\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            totalr = 0.\n",
    "            steps = 0\n",
    "\n",
    "            while not done:\n",
    "                action = policy_fn(obs[None,:])\n",
    "                # print('before append>>>> observations shape:', np.shape(observations), ', train_observations shape:', np.shape(train_observations), ', obs shape:', np.shape(obs))\n",
    "                observations.append(obs)\n",
    "                train_observations = np.append(train_observations, obs[None, :], axis=0)\n",
    "                # print('after append>>>> observations shape:', np.shape(observations), ', train_observations shape:', np.shape(train_observations), ', obs shape:', np.shape(obs))\n",
    "\n",
    "                expert_action = expert_policy_fn(obs[None,:]) # None makes additional dimension. to reduce, use np.hstack\n",
    "                actions.append(expert_action)\n",
    "                # print('before append>>>> actions shape:', np.shape(actions), ', train_actions shape:', np.shape(train_actions), ', expert_action shape:', np.shape(expert_action))\n",
    "                train_actions = np.append(train_actions, expert_action, axis=0)\n",
    "                # print('after append>>>> actions shape:', np.shape(actions), ', train_actions shape:', np.shape(train_actions), ', expert_action shape:', np.shape(expert_action))\n",
    "\n",
    "                try :\n",
    "                    if np.shape(action)[1] == 1 :\n",
    "                        action_shape = np.shape(action)\n",
    "                        action = np.reshape(action, (action_shape[0], action_shape[2]))\n",
    "                        action_shape = np.shape(action)\n",
    "                    obs, r, done, _ = env.step(action) # observation, reward, done\n",
    "                except ValueError as e :\n",
    "                    print('action:', action, ', shape:', np.shape(action), ', policy_type:', policy_type)\n",
    "                    print('actions:', actions, ', shape:', np.shape(actions))\n",
    "                    print('expert_action:', expert_action, ', shape:', np.shape(expert_action))\n",
    "                    traceback.print_exc()     \n",
    "\n",
    "                totalr += r\n",
    "                steps += 1\n",
    "                if render:\n",
    "                    env.render()\n",
    "                # if steps % 100 == 0: print(\"%i/%i\"%(steps, max_steps))\n",
    "                if steps >= max_steps:\n",
    "                    break\n",
    "            returns.append(totalr)\n",
    "\n",
    "            # retrain on every new rollouts\n",
    "            n_samples = train_observations.shape[0]\n",
    "            n_train = int(n_samples * (1 - TEST_PERCENT))\n",
    "\n",
    "            print('train_observations shape:', train_observations.shape, ', train_actions shape:', train_actions.shape)\n",
    "            print('observations shape:', np.shape(observations), ', actions shape:', np.shape(actions))\n",
    "            try :\n",
    "                train_observations, train_actions = shuffle_XY(train_observations, train_actions)\n",
    "            except IndexError as e :\n",
    "                print('train_observations:', train_observations, ', train_actions:', train_actions)\n",
    "                traceback.print_exc() \n",
    "\n",
    "            print('train input : train_observations shape:', train_observations[:n_train].shape, ', train_actions shape:', train_actions[:n_train].shape)\n",
    "\n",
    "            with tf.Session(graph=tf.Graph()) as nested_session, nested_session.graph.as_default():\n",
    "                saved_model = gym_env_model if cloning_model is None else gym_env_dagger_model\n",
    "                cloning_model = BehavioralCloning(X_shape=obs_shape, Y_shape=action_shape,\n",
    "                                                  scope_name=gym_env, restore_mode=True, session=nested_session)\n",
    "                cloning_model.restore_model(saved_model)        \n",
    "                training_costs, validation_costs, validation_measures = cloning_model.train(train_observations[:n_train], train_actions[:n_train],\n",
    "                                                                                           train_config = light_train_config)\n",
    "                cloning_model.save_model(gym_env_dagger_model)\n",
    "\n",
    "        with tf.Session(graph=tf.Graph()) as nested_session, nested_session.graph.as_default():\n",
    "            cloning = BehavioralCloning(X_shape=obs_shape, Y_shape=action_shape,\n",
    "                                        scope_name=gym_env, restore_mode=True, session=nested_session)\n",
    "            cloning.restore_model(gym_env_dagger_model)\n",
    "            test_hyps, test_costs, test_measures = cloning.test(train_observations[n_train:], train_actions[n_train:])\n",
    "\n",
    "        print('returns', returns)\n",
    "        print('mean return', np.mean(returns))\n",
    "        print('std of return', np.std(returns))\n",
    "\n",
    "        rollout_data = {'observations': np.array(observations),\n",
    "                        'actions': np.array(actions),\n",
    "                        'returns': np.array(returns)}\n",
    "\n",
    "        if not os.path.exists('rollout_data') :\n",
    "            os.mkdir('rollout_data')\n",
    "        with open(os.path.join('rollout_data', policy_type + '-' + gym_env + '.pkl'), 'wb') as f:\n",
    "            pk.dump(rollout_data, f, pk.HIGHEST_PROTOCOL)\n",
    "\n",
    "        return rollout_data, policy_type, env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and building learned policy\n",
      "saved dir: model_Ant-v2\n",
      "loaded and built\n",
      "starting dagger  Ant-v2 2019-07-09 18:54:32.707609\n",
      "Ant-v2  observation shape:  (19992, 111) , actions shape: (19992, 8)\n",
      "loading and building expert policy for DAgger\n",
      "obs (1, 111) (1, 111)\n",
      "loaded and built for DAgger\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "train_observations shape: (20992, 111) , train_actions shape: (20992, 8)\n",
      "observations shape: (1000, 111) , actions shape: (1000, 1, 8)\n",
      "train input : train_observations shape: (16793, 111) , train_actions shape: (16793, 8)\n",
      "saved dir: model_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-09 18:54:40.918082\n",
      "Epoch: 0000 average training cost = 0.040187810 validation cost = 0.018603407 validation measure = 0.763998151 2019-07-09 18:54:41.572289\n",
      "Training(learning) Finished! 2019-07-09 18:55:07.957608\n",
      "Training took          27  seconds.\n",
      "train_observations shape: (21992, 111) , train_actions shape: (21992, 8)\n",
      "observations shape: (2000, 111) , actions shape: (2000, 1, 8)\n",
      "train input : train_observations shape: (17593, 111) , train_actions shape: (17593, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-09 18:55:17.268081\n",
      "Epoch: 0000 average training cost = 0.038242396 validation cost = 0.017839974 validation measure = 0.779292762 2019-07-09 18:55:18.002171\n",
      "Training(learning) Finished! 2019-07-09 18:55:45.395587\n",
      "Training took          28  seconds.\n",
      "train_observations shape: (22992, 111) , train_actions shape: (22992, 8)\n",
      "observations shape: (3000, 111) , actions shape: (3000, 1, 8)\n",
      "train input : train_observations shape: (18393, 111) , train_actions shape: (18393, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-09 18:55:55.103750\n",
      "Epoch: 0000 average training cost = 0.036599424 validation cost = 0.016534958 validation measure = 0.799280226 2019-07-09 18:55:55.802844\n",
      "Training(learning) Finished! 2019-07-09 18:56:23.416734\n",
      "Training took          28  seconds.\n",
      "train_observations shape: (23992, 111) , train_actions shape: (23992, 8)\n",
      "observations shape: (4000, 111) , actions shape: (4000, 1, 8)\n",
      "train input : train_observations shape: (19193, 111) , train_actions shape: (19193, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-09 18:56:33.216999\n",
      "Epoch: 0000 average training cost = 0.034831665 validation cost = 0.016633796 validation measure = 0.799754381 2019-07-09 18:56:33.982952\n",
      "Training(learning) Finished! 2019-07-09 18:57:06.306674\n",
      "Training took          33  seconds.\n",
      "train_observations shape: (24992, 111) , train_actions shape: (24992, 8)\n",
      "observations shape: (5000, 111) , actions shape: (5000, 1, 8)\n",
      "train input : train_observations shape: (19993, 111) , train_actions shape: (19993, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-09 18:57:16.250418\n",
      "Epoch: 0000 average training cost = 0.033458121 validation cost = 0.016585369 validation measure = 0.805092394 2019-07-09 18:57:17.021741\n",
      "Training(learning) Finished! 2019-07-09 18:57:49.085338\n",
      "Training took          32  seconds.\n",
      "train_observations shape: (25992, 111) , train_actions shape: (25992, 8)\n",
      "observations shape: (6000, 111) , actions shape: (6000, 1, 8)\n",
      "train input : train_observations shape: (20793, 111) , train_actions shape: (20793, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-09 18:57:59.376012\n",
      "Epoch: 0000 average training cost = 0.032318641 validation cost = 0.015903061 validation measure = 0.813658357 2019-07-09 18:58:00.221754\n",
      "Training(learning) Finished! 2019-07-09 18:58:35.842110\n",
      "Training took          36  seconds.\n",
      "train_observations shape: (26992, 111) , train_actions shape: (26992, 8)\n",
      "observations shape: (7000, 111) , actions shape: (7000, 1, 8)\n",
      "train input : train_observations shape: (21593, 111) , train_actions shape: (21593, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-09 18:58:46.705059\n",
      "Epoch: 0000 average training cost = 0.030989291 validation cost = 0.015135627 validation measure = 0.820849895 2019-07-09 18:58:47.498966\n",
      "Training(learning) Finished! 2019-07-09 18:59:22.700217\n",
      "Training took          35  seconds.\n",
      "train_observations shape: (27992, 111) , train_actions shape: (27992, 8)\n",
      "observations shape: (8000, 111) , actions shape: (8000, 1, 8)\n",
      "train input : train_observations shape: (22393, 111) , train_actions shape: (22393, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-09 18:59:33.592681\n",
      "Epoch: 0000 average training cost = 0.029933056 validation cost = 0.015785076 validation measure = 0.816448569 2019-07-09 18:59:34.447957\n",
      "Training(learning) Finished! 2019-07-09 19:00:11.361286\n",
      "Training took          37  seconds.\n",
      "train_observations shape: (28992, 111) , train_actions shape: (28992, 8)\n",
      "observations shape: (9000, 111) , actions shape: (9000, 1, 8)\n",
      "train input : train_observations shape: (23193, 111) , train_actions shape: (23193, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:00:22.687493\n",
      "Epoch: 0000 average training cost = 0.029095193 validation cost = 0.015707618 validation measure = 0.818474114 2019-07-09 19:00:23.584057\n",
      "Training(learning) Finished! 2019-07-09 19:01:03.176905\n",
      "Training took          40  seconds.\n",
      "train_observations shape: (29992, 111) , train_actions shape: (29992, 8)\n",
      "observations shape: (10000, 111) , actions shape: (10000, 1, 8)\n",
      "train input : train_observations shape: (23993, 111) , train_actions shape: (23993, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:01:14.735750\n",
      "Epoch: 0000 average training cost = 0.028245062 validation cost = 0.014831727 validation measure = 0.827398241 2019-07-09 19:01:15.601395\n",
      "Training(learning) Finished! 2019-07-09 19:01:56.034871\n",
      "Training took          41  seconds.\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-09 19:01:56.734652  and finished at  2019-07-09 19:01:56.767567\n",
      "returns [843.55558768834, 844.7957524278728, 859.622397472056, 842.4164445449272, 876.4106208741372, 925.1307642790097, 853.5781545546199, 915.4040390671727, 908.5096003676722, 887.2895346119328]\n",
      "mean return 875.671289588774\n",
      "std of return 30.12489216073617\n",
      "Rollout result. env: Ant-v2 , policy_type: dagger , returns: 10 / 875.671289588774 / 30.12489216073617\n",
      "It took         444  seconds.\n",
      "loading and building learned policy\n",
      "saved dir: model_Hopper-v2\n",
      "loaded and built\n",
      "starting dagger  Hopper-v2 2019-07-09 19:01:57.152536\n",
      "Hopper-v2  observation shape:  (20000, 11) , actions shape: (20000, 3)\n",
      "loading and building expert policy for DAgger\n",
      "obs (1, 11) (1, 11)\n",
      "loaded and built for DAgger\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "train_observations shape: (20082, 11) , train_actions shape: (20082, 3)\n",
      "observations shape: (82, 11) , actions shape: (82, 1, 3)\n",
      "train input : train_observations shape: (16065, 11) , train_actions shape: (16065, 3)\n",
      "saved dir: model_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:01:58.279771\n",
      "Epoch: 0000 average training cost = 0.197413266 validation cost = 0.097068615 validation measure = 0.954202354 2019-07-09 19:01:58.910088\n",
      "Training(learning) Finished! 2019-07-09 19:02:24.574143\n",
      "Training took          26  seconds.\n",
      "train_observations shape: (20165, 11) , train_actions shape: (20165, 3)\n",
      "observations shape: (165, 11) , actions shape: (165, 1, 3)\n",
      "train input : train_observations shape: (16132, 11) , train_actions shape: (16132, 3)\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:02:25.737348\n",
      "Epoch: 0000 average training cost = 0.209344134 validation cost = 0.105512641 validation measure = 0.952591896 2019-07-09 19:02:26.358639\n",
      "Training(learning) Finished! 2019-07-09 19:02:51.436292\n",
      "Training took          25  seconds.\n",
      "train_observations shape: (20248, 11) , train_actions shape: (20248, 3)\n",
      "observations shape: (248, 11) , actions shape: (248, 1, 3)\n",
      "train input : train_observations shape: (16198, 11) , train_actions shape: (16198, 3)\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:02:52.753813\n",
      "Epoch: 0000 average training cost = 0.216662094 validation cost = 0.097218178 validation measure = 0.956894696 2019-07-09 19:02:53.380097\n",
      "Training(learning) Finished! 2019-07-09 19:03:18.868326\n",
      "Training took          26  seconds.\n",
      "train_observations shape: (20330, 11) , train_actions shape: (20330, 3)\n",
      "observations shape: (330, 11) , actions shape: (330, 1, 3)\n",
      "train input : train_observations shape: (16264, 11) , train_actions shape: (16264, 3)\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:03:19.997861\n",
      "Epoch: 0000 average training cost = 0.224120006 validation cost = 0.126828909 validation measure = 0.940975547 2019-07-09 19:03:20.626142\n",
      "Training(learning) Finished! 2019-07-09 19:03:46.559906\n",
      "Training took          26  seconds.\n",
      "train_observations shape: (20412, 11) , train_actions shape: (20412, 3)\n",
      "observations shape: (412, 11) , actions shape: (412, 1, 3)\n",
      "train input : train_observations shape: (16329, 11) , train_actions shape: (16329, 3)\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:03:47.792277\n",
      "Epoch: 0000 average training cost = 0.226845086 validation cost = 0.121873192 validation measure = 0.942879677 2019-07-09 19:03:48.481447\n",
      "Training(learning) Finished! 2019-07-09 19:04:14.309415\n",
      "Training took          26  seconds.\n",
      "train_observations shape: (20494, 11) , train_actions shape: (20494, 3)\n",
      "observations shape: (494, 11) , actions shape: (494, 1, 3)\n",
      "train input : train_observations shape: (16395, 11) , train_actions shape: (16395, 3)\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:04:15.413324\n",
      "Epoch: 0000 average training cost = 0.224380463 validation cost = 0.143079430 validation measure = 0.934268713 2019-07-09 19:04:16.063544\n",
      "Training(learning) Finished! 2019-07-09 19:04:42.324122\n",
      "Training took          26  seconds.\n",
      "train_observations shape: (20576, 11) , train_actions shape: (20576, 3)\n",
      "observations shape: (576, 11) , actions shape: (576, 1, 3)\n",
      "train input : train_observations shape: (16460, 11) , train_actions shape: (16460, 3)\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:04:43.656071\n",
      "Epoch: 0000 average training cost = 0.228446841 validation cost = 0.126756504 validation measure = 0.943355560 2019-07-09 19:04:44.263450\n",
      "Training(learning) Finished! 2019-07-09 19:05:10.241927\n",
      "Training took          26  seconds.\n",
      "train_observations shape: (20658, 11) , train_actions shape: (20658, 3)\n",
      "observations shape: (658, 11) , actions shape: (658, 1, 3)\n",
      "train input : train_observations shape: (16526, 11) , train_actions shape: (16526, 3)\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:05:11.381879\n",
      "Epoch: 0000 average training cost = 0.227729216 validation cost = 0.145937115 validation measure = 0.933999598 2019-07-09 19:05:12.051095\n",
      "Training(learning) Finished! 2019-07-09 19:05:38.226627\n",
      "Training took          26  seconds.\n",
      "train_observations shape: (20741, 11) , train_actions shape: (20741, 3)\n",
      "observations shape: (741, 11) , actions shape: (741, 1, 3)\n",
      "train input : train_observations shape: (16592, 11) , train_actions shape: (16592, 3)\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:05:39.382575\n",
      "Epoch: 0000 average training cost = 0.233780652 validation cost = 0.154873520 validation measure = 0.928525209 2019-07-09 19:05:39.997910\n",
      "Training(learning) Finished! 2019-07-09 19:06:06.024323\n",
      "Training took          26  seconds.\n",
      "train_observations shape: (20824, 11) , train_actions shape: (20824, 3)\n",
      "observations shape: (824, 11) , actions shape: (824, 1, 3)\n",
      "train input : train_observations shape: (16659, 11) , train_actions shape: (16659, 3)\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:06:07.122380\n",
      "Epoch: 0000 average training cost = 0.240724400 validation cost = 0.139896855 validation measure = 0.935665727 2019-07-09 19:06:07.810544\n",
      "Training(learning) Finished! 2019-07-09 19:06:33.429048\n",
      "Training took          26  seconds.\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-09 19:06:34.119799  and finished at  2019-07-09 19:06:34.150717\n",
      "returns [145.84337131284195, 151.0454827250433, 150.590237823237, 147.4608503212219, 148.88516359805757, 147.70563651381968, 146.4581494578527, 145.9916458832463, 150.39269516748558, 152.50831763161966]\n",
      "mean return 148.68815504344258\n",
      "std of return 2.2253387356471266\n",
      "Rollout result. env: Hopper-v2 , policy_type: dagger , returns: 10 / 148.68815504344258 / 2.2253387356471266\n",
      "It took         277  seconds.\n",
      "loading and building learned policy\n",
      "saved dir: model_Reacher-v2\n",
      "loaded and built\n",
      "starting dagger  Reacher-v2 2019-07-09 19:06:34.505765\n",
      "Reacher-v2  observation shape:  (1000, 11) , actions shape: (1000, 2)\n",
      "loading and building expert policy for DAgger\n",
      "obs (1, 11) (1, 11)\n",
      "loaded and built for DAgger\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "train_observations shape: (1050, 11) , train_actions shape: (1050, 2)\n",
      "observations shape: (50, 11) , actions shape: (50, 1, 2)\n",
      "train input : train_observations shape: (840, 11) , train_actions shape: (840, 2)\n",
      "saved dir: model_Reacher-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:06:35.189982\n",
      "Epoch: 0000 average training cost = 0.046482760 validation cost = 0.007769155 validation measure = -0.034836173 2019-07-09 19:06:35.370452\n",
      "Training(learning) Finished! 2019-07-09 19:06:36.868452\n",
      "Training took           1  seconds.\n",
      "train_observations shape: (1100, 11) , train_actions shape: (1100, 2)\n",
      "observations shape: (100, 11) , actions shape: (100, 1, 2)\n",
      "train input : train_observations shape: (880, 11) , train_actions shape: (880, 2)\n",
      "saved dir: model_dagger_Reacher-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:06:37.627466\n",
      "Epoch: 0000 average training cost = 0.048374012 validation cost = 0.008901619 validation measure = -0.081825972 2019-07-09 19:06:37.826891\n",
      "Training(learning) Finished! 2019-07-09 19:06:39.553276\n",
      "Training took           1  seconds.\n",
      "train_observations shape: (1150, 11) , train_actions shape: (1150, 2)\n",
      "observations shape: (150, 11) , actions shape: (150, 1, 2)\n",
      "train input : train_observations shape: (920, 11) , train_actions shape: (920, 2)\n",
      "saved dir: model_dagger_Reacher-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:06:40.420997\n",
      "Epoch: 0000 average training cost = 0.054277822 validation cost = 0.018501926 validation measure = -0.104505539 2019-07-09 19:06:40.607458\n",
      "Training(learning) Finished! 2019-07-09 19:06:42.283974\n",
      "Training took           1  seconds.\n",
      "train_observations shape: (1200, 11) , train_actions shape: (1200, 2)\n",
      "observations shape: (200, 11) , actions shape: (200, 1, 2)\n",
      "train input : train_observations shape: (960, 11) , train_actions shape: (960, 2)\n",
      "saved dir: model_dagger_Reacher-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:06:43.025988\n",
      "Epoch: 0000 average training cost = 0.051644914 validation cost = 0.014267214 validation measure = -0.035312176 2019-07-09 19:06:43.196536\n",
      "Training(learning) Finished! 2019-07-09 19:06:44.835126\n",
      "Training took           1  seconds.\n",
      "train_observations shape: (1250, 11) , train_actions shape: (1250, 2)\n",
      "observations shape: (250, 11) , actions shape: (250, 1, 2)\n",
      "train input : train_observations shape: (1000, 11) , train_actions shape: (1000, 2)\n",
      "saved dir: model_dagger_Reacher-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:06:45.553207\n",
      "Epoch: 0000 average training cost = 0.053709894 validation cost = 0.020222563 validation measure = 0.067443848 2019-07-09 19:06:45.735717\n",
      "Training(learning) Finished! 2019-07-09 19:06:47.134976\n",
      "Training took           1  seconds.\n",
      "train_observations shape: (1300, 11) , train_actions shape: (1300, 2)\n",
      "observations shape: (300, 11) , actions shape: (300, 1, 2)\n",
      "train input : train_observations shape: (1040, 11) , train_actions shape: (1040, 2)\n",
      "saved dir: model_dagger_Reacher-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:06:47.823183\n",
      "Epoch: 0000 average training cost = 0.051738925 validation cost = 0.017767403 validation measure = 0.095814526 2019-07-09 19:06:48.006647\n",
      "Training(learning) Finished! 2019-07-09 19:06:49.836760\n",
      "Training took           2  seconds.\n",
      "train_observations shape: (1350, 11) , train_actions shape: (1350, 2)\n",
      "observations shape: (350, 11) , actions shape: (350, 1, 2)\n",
      "train input : train_observations shape: (1080, 11) , train_actions shape: (1080, 2)\n",
      "saved dir: model_dagger_Reacher-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:06:50.548898\n",
      "Epoch: 0000 average training cost = 0.053299174 validation cost = 0.020309197 validation measure = 0.112903953 2019-07-09 19:06:50.767269\n",
      "Training(learning) Finished! 2019-07-09 19:06:52.683150\n",
      "Training took           2  seconds.\n",
      "train_observations shape: (1400, 11) , train_actions shape: (1400, 2)\n",
      "observations shape: (400, 11) , actions shape: (400, 1, 2)\n",
      "train input : train_observations shape: (1120, 11) , train_actions shape: (1120, 2)\n",
      "saved dir: model_dagger_Reacher-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:06:53.627659\n",
      "Epoch: 0000 average training cost = 0.052420694 validation cost = 0.025058761 validation measure = 0.075369120 2019-07-09 19:06:53.782211\n",
      "Training(learning) Finished! 2019-07-09 19:06:55.766909\n",
      "Training took           2  seconds.\n",
      "train_observations shape: (1450, 11) , train_actions shape: (1450, 2)\n",
      "observations shape: (450, 11) , actions shape: (450, 1, 2)\n",
      "train input : train_observations shape: (1160, 11) , train_actions shape: (1160, 2)\n",
      "saved dir: model_dagger_Reacher-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:06:56.436162\n",
      "Epoch: 0000 average training cost = 0.053209297 validation cost = 0.019213228 validation measure = 0.120623291 2019-07-09 19:06:56.629647\n",
      "Training(learning) Finished! 2019-07-09 19:06:58.715031\n",
      "Training took           2  seconds.\n",
      "train_observations shape: (1500, 11) , train_actions shape: (1500, 2)\n",
      "observations shape: (500, 11) , actions shape: (500, 1, 2)\n",
      "train input : train_observations shape: (1200, 11) , train_actions shape: (1200, 2)\n",
      "saved dir: model_dagger_Reacher-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:06:59.437139\n",
      "Epoch: 0000 average training cost = 0.055943698 validation cost = 0.021149982 validation measure = 0.136527121 2019-07-09 19:06:59.616614\n",
      "Training(learning) Finished! 2019-07-09 19:07:00.997923\n",
      "Training took           1  seconds.\n",
      "saved dir: model_dagger_Reacher-v2\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-09 19:07:01.479635  and finished at  2019-07-09 19:07:01.508558\n",
      "returns [-14.030375373307946, -13.257091944801353, -17.44336375729596, -14.2667723808781, -7.227902986591902, -12.217896715006697, -17.554254686548564, -12.195787959181605, -13.695498067282468, -14.700072806146792]\n",
      "mean return -13.65890166770414\n",
      "std of return 2.7717358584559864\n",
      "Rollout result. env: Reacher-v2 , policy_type: dagger , returns: 10 / -13.65890166770414 / 2.7717358584559864\n",
      "It took          27  seconds.\n",
      "loading and building learned policy\n",
      "saved dir: model_HalfCheetah-v2\n",
      "loaded and built\n",
      "starting dagger  HalfCheetah-v2 2019-07-09 19:07:02.074045\n",
      "HalfCheetah-v2  observation shape:  (20000, 17) , actions shape: (20000, 6)\n",
      "loading and building expert policy for DAgger\n",
      "obs (1, 17) (1, 17)\n",
      "loaded and built for DAgger\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "train_observations shape: (21000, 17) , train_actions shape: (21000, 6)\n",
      "observations shape: (1000, 17) , actions shape: (1000, 1, 6)\n",
      "train input : train_observations shape: (16800, 17) , train_actions shape: (16800, 6)\n",
      "saved dir: model_HalfCheetah-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:07:05.210502\n",
      "Epoch: 0000 average training cost = 0.133350939 validation cost = 0.058040045 validation measure = 0.893642545 2019-07-09 19:07:05.907641\n",
      "Training(learning) Finished! 2019-07-09 19:07:33.324924\n",
      "Training took          28  seconds.\n",
      "train_observations shape: (22000, 17) , train_actions shape: (22000, 6)\n",
      "observations shape: (2000, 17) , actions shape: (2000, 1, 6)\n",
      "train input : train_observations shape: (17600, 17) , train_actions shape: (17600, 6)\n",
      "saved dir: model_dagger_HalfCheetah-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:07:36.835882\n",
      "Epoch: 0000 average training cost = 0.133081257 validation cost = 0.061236862 validation measure = 0.889688849 2019-07-09 19:07:37.532974\n",
      "Training(learning) Finished! 2019-07-09 19:08:05.213918\n",
      "Training took          28  seconds.\n",
      "train_observations shape: (23000, 17) , train_actions shape: (23000, 6)\n",
      "observations shape: (3000, 17) , actions shape: (3000, 1, 6)\n",
      "train input : train_observations shape: (18400, 17) , train_actions shape: (18400, 6)\n",
      "saved dir: model_dagger_HalfCheetah-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:08:09.084573\n",
      "Epoch: 0000 average training cost = 0.132714897 validation cost = 0.062757693 validation measure = 0.882857800 2019-07-09 19:08:09.831581\n",
      "Training(learning) Finished! 2019-07-09 19:08:39.589325\n",
      "Training took          30  seconds.\n",
      "train_observations shape: (24000, 17) , train_actions shape: (24000, 6)\n",
      "observations shape: (4000, 17) , actions shape: (4000, 1, 6)\n",
      "train input : train_observations shape: (19200, 17) , train_actions shape: (19200, 6)\n",
      "saved dir: model_dagger_HalfCheetah-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:08:43.773052\n",
      "Epoch: 0000 average training cost = 0.133759171 validation cost = 0.067843288 validation measure = 0.877291203 2019-07-09 19:08:44.506626\n",
      "Training(learning) Finished! 2019-07-09 19:09:14.743746\n",
      "Training took          30  seconds.\n",
      "train_observations shape: (25000, 17) , train_actions shape: (25000, 6)\n",
      "observations shape: (5000, 17) , actions shape: (5000, 1, 6)\n",
      "train input : train_observations shape: (20000, 17) , train_actions shape: (20000, 6)\n",
      "saved dir: model_dagger_HalfCheetah-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:09:18.781660\n",
      "Epoch: 0000 average training cost = 0.133324370 validation cost = 0.067469642 validation measure = 0.875139773 2019-07-09 19:09:19.565499\n",
      "Training(learning) Finished! 2019-07-09 19:09:51.693737\n",
      "Training took          32  seconds.\n",
      "train_observations shape: (26000, 17) , train_actions shape: (26000, 6)\n",
      "observations shape: (6000, 17) , actions shape: (6000, 1, 6)\n",
      "train input : train_observations shape: (20800, 17) , train_actions shape: (20800, 6)\n",
      "saved dir: model_dagger_HalfCheetah-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:09:55.871299\n",
      "Epoch: 0000 average training cost = 0.131912336 validation cost = 0.067790776 validation measure = 0.873115778 2019-07-09 19:09:56.685689\n",
      "Training(learning) Finished! 2019-07-09 19:10:29.042962\n",
      "Training took          33  seconds.\n",
      "train_observations shape: (27000, 17) , train_actions shape: (27000, 6)\n",
      "observations shape: (7000, 17) , actions shape: (7000, 1, 6)\n",
      "train input : train_observations shape: (21600, 17) , train_actions shape: (21600, 6)\n",
      "saved dir: model_dagger_HalfCheetah-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:10:33.186714\n",
      "Epoch: 0000 average training cost = 0.128884792 validation cost = 0.068992212 validation measure = 0.873609781 2019-07-09 19:10:34.007350\n",
      "Training(learning) Finished! 2019-07-09 19:11:07.292055\n",
      "Training took          34  seconds.\n",
      "train_observations shape: (28000, 17) , train_actions shape: (28000, 6)\n",
      "observations shape: (8000, 17) , actions shape: (8000, 1, 6)\n",
      "train input : train_observations shape: (22400, 17) , train_actions shape: (22400, 6)\n",
      "saved dir: model_dagger_HalfCheetah-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:11:11.478464\n",
      "Epoch: 0000 average training cost = 0.128405154 validation cost = 0.069867179 validation measure = 0.871301293 2019-07-09 19:11:12.290334\n",
      "Training(learning) Finished! 2019-07-09 19:11:47.797072\n",
      "Training took          36  seconds.\n",
      "train_observations shape: (29000, 17) , train_actions shape: (29000, 6)\n",
      "observations shape: (9000, 17) , actions shape: (9000, 1, 6)\n",
      "train input : train_observations shape: (23200, 17) , train_actions shape: (23200, 6)\n",
      "saved dir: model_dagger_HalfCheetah-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:11:52.378878\n",
      "Epoch: 0000 average training cost = 0.125775129 validation cost = 0.068986788 validation measure = 0.872317791 2019-07-09 19:11:53.285494\n",
      "Training(learning) Finished! 2019-07-09 19:12:29.577627\n",
      "Training took          37  seconds.\n",
      "train_observations shape: (30000, 17) , train_actions shape: (30000, 6)\n",
      "observations shape: (10000, 17) , actions shape: (10000, 1, 6)\n",
      "train input : train_observations shape: (24000, 17) , train_actions shape: (24000, 6)\n",
      "saved dir: model_dagger_HalfCheetah-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:12:33.954458\n",
      "Epoch: 0000 average training cost = 0.123000659 validation cost = 0.068895198 validation measure = 0.873124182 2019-07-09 19:12:34.877612\n",
      "Training(learning) Finished! 2019-07-09 19:13:13.760318\n",
      "Training took          39  seconds.\n",
      "saved dir: model_dagger_HalfCheetah-v2\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-09 19:13:14.288901  and finished at  2019-07-09 19:13:14.320818\n",
      "returns [2384.332941176247, 2214.6884371629358, 2371.342273816347, 2195.603364028972, 2278.8473721805617, 1979.6318370784447, 2396.7058647836693, 2262.354483042265, 2399.9592534569, 2434.3334174542783]\n",
      "mean return 2291.7799244180624\n",
      "std of return 130.99332465257282\n",
      "Rollout result. env: HalfCheetah-v2 , policy_type: dagger , returns: 10 / 2291.7799244180624 / 130.99332465257282\n",
      "It took         372  seconds.\n",
      "loading and building learned policy\n",
      "saved dir: model_Humanoid-v2\n",
      "loaded and built\n",
      "starting dagger  Humanoid-v2 2019-07-09 19:13:14.765279\n",
      "Humanoid-v2  observation shape:  (20000, 376) , actions shape: (20000, 17)\n",
      "loading and building expert policy for DAgger\n",
      "obs (1, 376) (1, 376)\n",
      "loaded and built for DAgger\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "train_observations shape: (20093, 376) , train_actions shape: (20093, 17)\n",
      "observations shape: (93, 376) , actions shape: (93, 1, 17)\n",
      "train input : train_observations shape: (16074, 376) , train_actions shape: (16074, 17)\n",
      "saved dir: model_Humanoid-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:13:17.399748\n",
      "Epoch: 0000 average training cost = 1.427860498 validation cost = 0.864512503 validation measure = 0.103707671 2019-07-09 19:13:18.133739\n",
      "Training(learning) Finished! 2019-07-09 19:13:45.711128\n",
      "Training took          28  seconds.\n",
      "train_observations shape: (20180, 376) , train_actions shape: (20180, 17)\n",
      "observations shape: (180, 376) , actions shape: (180, 1, 17)\n",
      "train input : train_observations shape: (16144, 376) , train_actions shape: (16144, 17)\n",
      "saved dir: model_dagger_Humanoid-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:13:48.411950\n",
      "Epoch: 0000 average training cost = 1.365552902 validation cost = 0.859526098 validation measure = 0.112513244 2019-07-09 19:13:49.082119\n",
      "Training(learning) Finished! 2019-07-09 19:14:14.947055\n",
      "Training took          26  seconds.\n",
      "train_observations shape: (20254, 376) , train_actions shape: (20254, 17)\n",
      "observations shape: (254, 376) , actions shape: (254, 1, 17)\n",
      "train input : train_observations shape: (16203, 376) , train_actions shape: (16203, 17)\n",
      "saved dir: model_dagger_Humanoid-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:14:17.222987\n",
      "Epoch: 0000 average training cost = 1.314703584 validation cost = 0.859222412 validation measure = 0.114920676 2019-07-09 19:14:17.910108\n",
      "Training(learning) Finished! 2019-07-09 19:14:46.400358\n",
      "Training took          29  seconds.\n",
      "train_observations shape: (20321, 376) , train_actions shape: (20321, 17)\n",
      "observations shape: (321, 376) , actions shape: (321, 1, 17)\n",
      "train input : train_observations shape: (16256, 376) , train_actions shape: (16256, 17)\n",
      "saved dir: model_dagger_Humanoid-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:14:48.487235\n",
      "Epoch: 0000 average training cost = 1.247454762 validation cost = 0.841768205 validation measure = 0.120919585 2019-07-09 19:14:49.184390\n",
      "Training(learning) Finished! 2019-07-09 19:15:18.803815\n",
      "Training took          30  seconds.\n",
      "train_observations shape: (20418, 376) , train_actions shape: (20418, 17)\n",
      "observations shape: (418, 376) , actions shape: (418, 1, 17)\n",
      "train input : train_observations shape: (16334, 376) , train_actions shape: (16334, 17)\n",
      "saved dir: model_dagger_Humanoid-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:15:21.570568\n",
      "Epoch: 0000 average training cost = 1.285352111 validation cost = 0.856303036 validation measure = 0.127333939 2019-07-09 19:15:22.283626\n",
      "Training(learning) Finished! 2019-07-09 19:15:52.353422\n",
      "Training took          30  seconds.\n",
      "train_observations shape: (20503, 376) , train_actions shape: (20503, 17)\n",
      "observations shape: (503, 376) , actions shape: (503, 1, 17)\n",
      "train input : train_observations shape: (16402, 376) , train_actions shape: (16402, 17)\n",
      "saved dir: model_dagger_Humanoid-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:15:55.077149\n",
      "Epoch: 0000 average training cost = 1.190667391 validation cost = 0.836839259 validation measure = 0.141004145 2019-07-09 19:15:55.817163\n",
      "Training(learning) Finished! 2019-07-09 19:16:25.431891\n",
      "Training took          30  seconds.\n",
      "train_observations shape: (20585, 376) , train_actions shape: (20585, 17)\n",
      "observations shape: (585, 376) , actions shape: (585, 1, 17)\n",
      "train input : train_observations shape: (16468, 376) , train_actions shape: (16468, 17)\n",
      "saved dir: model_dagger_Humanoid-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:16:27.853223\n",
      "Epoch: 0000 average training cost = 1.183667779 validation cost = 0.838951409 validation measure = 0.145672143 2019-07-09 19:16:28.595244\n",
      "Training(learning) Finished! 2019-07-09 19:16:57.832063\n",
      "Training took          29  seconds.\n",
      "train_observations shape: (20663, 376) , train_actions shape: (20663, 17)\n",
      "observations shape: (663, 376) , actions shape: (663, 1, 17)\n",
      "train input : train_observations shape: (16530, 376) , train_actions shape: (16530, 17)\n",
      "saved dir: model_dagger_Humanoid-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:17:00.134881\n",
      "Epoch: 0000 average training cost = 1.174017429 validation cost = 0.834902227 validation measure = 0.154678881 2019-07-09 19:17:00.881885\n",
      "Training(learning) Finished! 2019-07-09 19:17:30.106807\n",
      "Training took          29  seconds.\n",
      "train_observations shape: (20771, 376) , train_actions shape: (20771, 17)\n",
      "observations shape: (771, 376) , actions shape: (771, 1, 17)\n",
      "train input : train_observations shape: (16616, 376) , train_actions shape: (16616, 17)\n",
      "saved dir: model_dagger_Humanoid-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:17:33.118148\n",
      "Epoch: 0000 average training cost = 1.129717350 validation cost = 0.842005491 validation measure = 0.161912978 2019-07-09 19:17:33.818236\n",
      "Training(learning) Finished! 2019-07-09 19:18:03.348072\n",
      "Training took          30  seconds.\n",
      "train_observations shape: (20841, 376) , train_actions shape: (20841, 17)\n",
      "observations shape: (841, 376) , actions shape: (841, 1, 17)\n",
      "train input : train_observations shape: (16672, 376) , train_actions shape: (16672, 17)\n",
      "saved dir: model_dagger_Humanoid-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:18:05.522330\n",
      "Epoch: 0000 average training cost = 1.142240882 validation cost = 0.834366500 validation measure = 0.168731987 2019-07-09 19:18:06.242357\n",
      "Training(learning) Finished! 2019-07-09 19:18:33.894962\n",
      "Training took          28  seconds.\n",
      "saved dir: model_dagger_Humanoid-v2\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-09 19:18:34.663880  and finished at  2019-07-09 19:18:34.701830\n",
      "returns [486.3214433744689, 457.0664138540921, 388.60257075232073, 353.1450075992209, 507.6018355655634, 442.1349656312522, 428.3575409314927, 408.4509063434434, 558.4173668783338, 367.4686145967866]\n",
      "mean return 439.7566665526974\n",
      "std of return 61.327616116539744\n",
      "Rollout result. env: Humanoid-v2 , policy_type: dagger , returns: 10 / 439.7566665526974 / 61.327616116539744\n",
      "It took         320  seconds.\n",
      "loading and building learned policy\n",
      "saved dir: model_Walker2d-v2\n",
      "loaded and built\n",
      "starting dagger  Walker2d-v2 2019-07-09 19:18:35.126641\n",
      "Walker2d-v2  observation shape:  (20000, 17) , actions shape: (20000, 6)\n",
      "loading and building expert policy for DAgger\n",
      "obs (1, 17) (1, 17)\n",
      "loaded and built for DAgger\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "train_observations shape: (21000, 17) , train_actions shape: (21000, 6)\n",
      "observations shape: (1000, 17) , actions shape: (1000, 1, 6)\n",
      "train input : train_observations shape: (16800, 17) , train_actions shape: (16800, 6)\n",
      "saved dir: model_Walker2d-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:18:38.494529\n",
      "Epoch: 0000 average training cost = 0.317096740 validation cost = 0.193993583 validation measure = 0.809342980 2019-07-09 19:18:39.200643\n",
      "Training(learning) Finished! 2019-07-09 19:19:06.522683\n",
      "Training took          28  seconds.\n",
      "train_observations shape: (22000, 17) , train_actions shape: (22000, 6)\n",
      "observations shape: (2000, 17) , actions shape: (2000, 1, 6)\n",
      "train input : train_observations shape: (17600, 17) , train_actions shape: (17600, 6)\n",
      "saved dir: model_dagger_Walker2d-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:19:10.103113\n",
      "Epoch: 0000 average training cost = 0.329725534 validation cost = 0.200651601 validation measure = 0.804886460 2019-07-09 19:19:10.818202\n",
      "Training(learning) Finished! 2019-07-09 19:19:39.162706\n",
      "Training took          29  seconds.\n",
      "train_observations shape: (22031, 17) , train_actions shape: (22031, 6)\n",
      "observations shape: (2031, 17) , actions shape: (2031, 1, 6)\n",
      "train input : train_observations shape: (17624, 17) , train_actions shape: (17624, 6)\n",
      "saved dir: model_dagger_Walker2d-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:19:39.940682\n",
      "Epoch: 0000 average training cost = 0.302650809 validation cost = 0.180723414 validation measure = 0.825315416 2019-07-09 19:19:40.637779\n",
      "Training(learning) Finished! 2019-07-09 19:20:09.557044\n",
      "Training took          29  seconds.\n",
      "train_observations shape: (23031, 17) , train_actions shape: (23031, 6)\n",
      "observations shape: (3031, 17) , actions shape: (3031, 1, 6)\n",
      "train input : train_observations shape: (18424, 17) , train_actions shape: (18424, 6)\n",
      "saved dir: model_dagger_Walker2d-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:20:13.821257\n",
      "Epoch: 0000 average training cost = 0.345211685 validation cost = 0.225448355 validation measure = 0.792054057 2019-07-09 19:20:14.541334\n",
      "Training(learning) Finished! 2019-07-09 19:20:43.548460\n",
      "Training took          29  seconds.\n",
      "train_observations shape: (24031, 17) , train_actions shape: (24031, 6)\n",
      "observations shape: (4031, 17) , actions shape: (4031, 1, 6)\n",
      "train input : train_observations shape: (19224, 17) , train_actions shape: (19224, 6)\n",
      "saved dir: model_dagger_Walker2d-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:20:47.671344\n",
      "Epoch: 0000 average training cost = 0.322151244 validation cost = 0.210662141 validation measure = 0.805241823 2019-07-09 19:20:48.385415\n",
      "Training(learning) Finished! 2019-07-09 19:21:17.704988\n",
      "Training took          30  seconds.\n",
      "train_observations shape: (25031, 17) , train_actions shape: (25031, 6)\n",
      "observations shape: (5031, 17) , actions shape: (5031, 1, 6)\n",
      "train input : train_observations shape: (20024, 17) , train_actions shape: (20024, 6)\n",
      "saved dir: model_dagger_Walker2d-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:21:21.841353\n",
      "Epoch: 0000 average training cost = 0.298839539 validation cost = 0.195205376 validation measure = 0.821684241 2019-07-09 19:21:22.580452\n",
      "Training(learning) Finished! 2019-07-09 19:21:54.805239\n",
      "Training took          32  seconds.\n",
      "train_observations shape: (26031, 17) , train_actions shape: (26031, 6)\n",
      "observations shape: (6031, 17) , actions shape: (6031, 1, 6)\n",
      "train input : train_observations shape: (20824, 17) , train_actions shape: (20824, 6)\n",
      "saved dir: model_dagger_Walker2d-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:21:59.005558\n",
      "Epoch: 0000 average training cost = 0.281112999 validation cost = 0.175648153 validation measure = 0.835476696 2019-07-09 19:21:59.802392\n",
      "Training(learning) Finished! 2019-07-09 19:22:32.332318\n",
      "Training took          33  seconds.\n",
      "train_observations shape: (27031, 17) , train_actions shape: (27031, 6)\n",
      "observations shape: (7031, 17) , actions shape: (7031, 1, 6)\n",
      "train input : train_observations shape: (21624, 17) , train_actions shape: (21624, 6)\n",
      "saved dir: model_dagger_Walker2d-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:22:36.553522\n",
      "Epoch: 0000 average training cost = 0.274505496 validation cost = 0.165831044 validation measure = 0.847043216 2019-07-09 19:22:37.345367\n",
      "Training(learning) Finished! 2019-07-09 19:23:09.642745\n",
      "Training took          33  seconds.\n",
      "train_observations shape: (28031, 17) , train_actions shape: (28031, 6)\n",
      "observations shape: (8031, 17) , actions shape: (8031, 1, 6)\n",
      "train input : train_observations shape: (22424, 17) , train_actions shape: (22424, 6)\n",
      "saved dir: model_dagger_Walker2d-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:23:14.231426\n",
      "Epoch: 0000 average training cost = 0.258569419 validation cost = 0.160370484 validation measure = 0.854147375 2019-07-09 19:23:15.060173\n",
      "Training(learning) Finished! 2019-07-09 19:23:49.648201\n",
      "Training took          35  seconds.\n",
      "train_observations shape: (28061, 17) , train_actions shape: (28061, 6)\n",
      "observations shape: (8061, 17) , actions shape: (8061, 1, 6)\n",
      "train input : train_observations shape: (22448, 17) , train_actions shape: (22448, 6)\n",
      "saved dir: model_dagger_Walker2d-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:23:50.455209\n",
      "Epoch: 0000 average training cost = 0.249320686 validation cost = 0.145990923 validation measure = 0.865153313 2019-07-09 19:23:51.309407\n",
      "Training(learning) Finished! 2019-07-09 19:24:26.590726\n",
      "Training took          36  seconds.\n",
      "saved dir: model_dagger_Walker2d-v2\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-09 19:24:27.107393  and finished at  2019-07-09 19:24:27.139360\n",
      "returns [1026.6189909341629, 1022.7575021408782, 14.251806577301313, 1064.229468090026, 1072.4671784816303, 1073.817144976306, 1080.5939761070363, 1067.5714876927343, 1067.4015967695536, 13.23685062265212]\n",
      "mean return 850.2946002392282\n",
      "std of return 418.6813321385898\n",
      "Rollout result. env: Walker2d-v2 , policy_type: dagger , returns: 10 / 850.2946002392282 / 418.6813321385898\n",
      "It took         352  seconds.\n"
     ]
    }
   ],
   "source": [
    "max_timesteps = None\n",
    "num_rollouts = 10 # incremental learning is too slow \n",
    "\n",
    "for gym_env in gym_envs :\n",
    "    start_time = dt.datetime.now()\n",
    "    rollout_data, policy_type, _ = rollout_by_dagger(gym_env, max_timesteps, num_rollouts,\n",
    "                                                    render=False)\n",
    "    returns = rollout_data['returns']\n",
    "    end_time = dt.datetime.now()\n",
    "    print('Rollout result. env:', gym_env, ', policy_type:', policy_type, ', returns:', len(returns), '/', np.mean(returns), '/', np.std(returns))\n",
    "    df = df.append({'gymenv':gym_env, 'policy_type':policy_type, 'rollouts': len(returns), 'mean': np.mean(returns), 'std': np.std(returns)}, ignore_index=True)\n",
    "    print('It took ', '%10d' % ((end_time - start_time).total_seconds()), ' seconds.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gymenv</th>\n",
       "      <th>policy_type</th>\n",
       "      <th>rollouts</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ant-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>200</td>\n",
       "      <td>4772.544028</td>\n",
       "      <td>422.163975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ant-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>200</td>\n",
       "      <td>926.068148</td>\n",
       "      <td>140.414594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hopper-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>200</td>\n",
       "      <td>3778.515927</td>\n",
       "      <td>3.775791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hopper-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>200</td>\n",
       "      <td>148.669662</td>\n",
       "      <td>6.481258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reacher-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>200</td>\n",
       "      <td>-4.125223</td>\n",
       "      <td>1.780242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reacher-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>200</td>\n",
       "      <td>-12.930317</td>\n",
       "      <td>4.211620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HalfCheetah-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>200</td>\n",
       "      <td>4144.304682</td>\n",
       "      <td>75.581012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HalfCheetah-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>200</td>\n",
       "      <td>2244.335064</td>\n",
       "      <td>180.081137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Humanoid-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>200</td>\n",
       "      <td>10357.171722</td>\n",
       "      <td>698.728618</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Humanoid-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>200</td>\n",
       "      <td>446.594071</td>\n",
       "      <td>56.643694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Walker2d-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>200</td>\n",
       "      <td>5520.614190</td>\n",
       "      <td>74.864049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Walker2d-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>200</td>\n",
       "      <td>844.566910</td>\n",
       "      <td>415.951835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ant-v2</td>\n",
       "      <td>dagger</td>\n",
       "      <td>10</td>\n",
       "      <td>875.671290</td>\n",
       "      <td>30.124892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Hopper-v2</td>\n",
       "      <td>dagger</td>\n",
       "      <td>10</td>\n",
       "      <td>148.688155</td>\n",
       "      <td>2.225339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Reacher-v2</td>\n",
       "      <td>dagger</td>\n",
       "      <td>10</td>\n",
       "      <td>-13.658902</td>\n",
       "      <td>2.771736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>HalfCheetah-v2</td>\n",
       "      <td>dagger</td>\n",
       "      <td>10</td>\n",
       "      <td>2291.779924</td>\n",
       "      <td>130.993325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Humanoid-v2</td>\n",
       "      <td>dagger</td>\n",
       "      <td>10</td>\n",
       "      <td>439.756667</td>\n",
       "      <td>61.327616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Walker2d-v2</td>\n",
       "      <td>dagger</td>\n",
       "      <td>10</td>\n",
       "      <td>850.294600</td>\n",
       "      <td>418.681332</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            gymenv policy_type rollouts          mean         std\n",
       "0           Ant-v2      expert      200   4772.544028  422.163975\n",
       "1           Ant-v2     learned      200    926.068148  140.414594\n",
       "2        Hopper-v2      expert      200   3778.515927    3.775791\n",
       "3        Hopper-v2     learned      200    148.669662    6.481258\n",
       "4       Reacher-v2      expert      200     -4.125223    1.780242\n",
       "5       Reacher-v2     learned      200    -12.930317    4.211620\n",
       "6   HalfCheetah-v2      expert      200   4144.304682   75.581012\n",
       "7   HalfCheetah-v2     learned      200   2244.335064  180.081137\n",
       "8      Humanoid-v2      expert      200  10357.171722  698.728618\n",
       "9      Humanoid-v2     learned      200    446.594071   56.643694\n",
       "10     Walker2d-v2      expert      200   5520.614190   74.864049\n",
       "11     Walker2d-v2     learned      200    844.566910  415.951835\n",
       "12          Ant-v2      dagger       10    875.671290   30.124892\n",
       "13       Hopper-v2      dagger       10    148.688155    2.225339\n",
       "14      Reacher-v2      dagger       10    -13.658902    2.771736\n",
       "15  HalfCheetah-v2      dagger       10   2291.779924  130.993325\n",
       "16     Humanoid-v2      dagger       10    439.756667   61.327616\n",
       "17     Walker2d-v2      dagger       10    850.294600  418.681332"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## just rollout and render using the policies. enjoy the visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0709 19:46:10.948683 53768 deprecation_wrapper.py:119] From C:\\Exception\\Works\\GitHub\\cs294\\yoonforh\\hw1\\load_policy.py:55: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and building expert policy\n",
      "obs (1, 111) (1, 111)\n",
      "loaded and built\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0709 19:46:12.753803 53768 deprecation.py:323] From C:\\Exception\\Works\\GitHub\\cs294\\yoonforh\\hw1\\tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "W0709 19:46:12.755799 53768 deprecation_wrapper.py:119] From C:\\Exception\\Works\\GitHub\\cs294\\yoonforh\\hw1\\tf_util.py:74: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0709 19:46:12.756796 53768 deprecation.py:323] From C:\\Works\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:193: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n",
      "C:\\Works\\tensorflow\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Creating window glfw\n",
      "returns [4784.474920335864, 4713.279314667821, 4809.302924908928, 4914.444299500961, 4809.060126767605]\n",
      "mean return 4806.112317236235\n",
      "std of return 64.5613371147942\n",
      "Rollout result. env: Ant-v2 , policy_type: expert , returns: 5 / 4806.112317236235 / 64.5613371147942\n",
      "Ant-v2 - expert . It took          66  seconds.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0709 19:47:25.381299 53768 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0709 19:47:25.382292 53768 deprecation.py:323] From <ipython-input-3-a1d04047b886>:82: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0709 19:47:25.725375 53768 deprecation.py:323] From <ipython-input-3-a1d04047b886>:83: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "W0709 19:47:25.871944 53768 deprecation.py:323] From C:\\Works\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0709 19:47:25.887902 53768 deprecation.py:323] From <ipython-input-3-a1d04047b886>:277: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "W0709 19:47:26.086368 53768 deprecation.py:323] From C:\\Works\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved dir: model_Ant-v2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Works\\tensorflow\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Creating window glfw\n",
      "returns [896.5271234364158, 886.5595075282029, 1044.8450915467165, 934.9606952685215, 1033.3468939267186]\n",
      "mean return 959.247862341315\n",
      "std of return 67.26798142975883\n",
      "Rollout result. env: Ant-v2 , policy_type: learned , returns: 5 / 959.247862341315 / 67.26798142975883\n",
      "Ant-v2 - learned . It took          63  seconds.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and building learned policy\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Works\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved dir: model_Ant-v2\n",
      "loaded and built\n",
      "starting dagger  Ant-v2 2019-07-09 19:48:43.320826\n",
      "Ant-v2  observation shape:  (19992, 111) , actions shape: (19992, 8)\n",
      "loading and building expert policy for DAgger\n",
      "obs (1, 111) (1, 111)\n",
      "loaded and built for DAgger\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Creating window glfw\n",
      "train_observations shape: (20992, 111) , train_actions shape: (20992, 8)\n",
      "observations shape: (1000, 111) , actions shape: (1000, 1, 8)\n",
      "train input : train_observations shape: (16793, 111) , train_actions shape: (16793, 8)\n",
      "saved dir: model_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:49:01.376691\n",
      "Training(learning) Finished! 2019-07-09 19:49:01.376691\n",
      "Training took           0  seconds.\n",
      "train_observations shape: (21992, 111) , train_actions shape: (21992, 8)\n",
      "observations shape: (2000, 111) , actions shape: (2000, 1, 8)\n",
      "train input : train_observations shape: (17593, 111) , train_actions shape: (17593, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:49:19.999510\n",
      "Training(learning) Finished! 2019-07-09 19:49:20.000460\n",
      "Training took           0  seconds.\n",
      "train_observations shape: (22992, 111) , train_actions shape: (22992, 8)\n",
      "observations shape: (3000, 111) , actions shape: (3000, 1, 8)\n",
      "train input : train_observations shape: (18393, 111) , train_actions shape: (18393, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-09 19:49:39.257663\n",
      "Training(learning) Finished! 2019-07-09 19:49:39.258616\n",
      "Training took           0  seconds.\n"
     ]
    }
   ],
   "source": [
    "# rollout and check\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import glfw\n",
    "\n",
    "max_timesteps = None\n",
    "num_rollouts = 5\n",
    "\n",
    "df2 = pd.DataFrame(columns=['gymenv', 'policy_type', 'rollouts', 'mean', 'std'])\n",
    "\n",
    "def close_mujoco_window(win) :\n",
    "    if win.unwrapped.viewer is not None :\n",
    "        glfw.destroy_window(win.unwrapped.viewer.window)\n",
    "        win.unwrapped.viewer = None\n",
    "    \n",
    "for gym_env in gym_envs :\n",
    "    for expert_policy in [ True, False] :\n",
    "        start_time = dt.datetime.now()\n",
    "        rollout_data, policy_type, opengym_win = rollout_by_policy(gym_env, max_timesteps, num_rollouts,\n",
    "                                                                  policy_fn=None if expert_policy else load_learned_policy_fn(gym_env),\n",
    "                                                                  render=True)\n",
    "        returns = rollout_data['returns']\n",
    "        end_time = dt.datetime.now()\n",
    "        print('Rollout result. env:', gym_env, ', policy_type:', policy_type, ', returns:', len(returns), '/', np.mean(returns), '/', np.std(returns))\n",
    "        df2 = df2.append({'gymenv':gym_env, 'policy_type':policy_type, 'rollouts': len(returns), 'mean': np.mean(returns), 'std': np.std(returns)}, ignore_index=True)\n",
    "        print(gym_env, '-', policy_type, '. It took ', '%10d' % ((end_time - start_time).total_seconds()), ' seconds.')\n",
    "        input(\"Press Enter to continue...\")\n",
    "        close_mujoco_window(opengym_win)\n",
    "\n",
    "    start_time = dt.datetime.now()\n",
    "    rollout_data, policy_type, opengym_win = rollout_by_dagger(gym_env, max_timesteps, num_rollouts, num_epochs=0,\n",
    "                                                              render=True)\n",
    "    returns = rollout_data['returns']\n",
    "    end_time = dt.datetime.now()\n",
    "    print('Rollout result. env:', gym_env, ', policy_type:', policy_type, ', returns:', len(returns), '/', np.mean(returns), '/', np.std(returns))\n",
    "    df2 = df2.append({'gymenv':gym_env, 'policy_type':policy_type, 'rollouts': len(returns), 'mean': np.mean(returns), 'std': np.std(returns)}, ignore_index=True)\n",
    "    print(gym_env, '-', policy_type, '. It took ', '%10d' % ((end_time - start_time).total_seconds()), ' seconds.')\n",
    "    input(\"Press Enter to continue...\")\n",
    "    close_mujoco_window(opengym_win)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "##  Bonus: Alternative Policy Architectures\n",
    "\n",
    "1. (Optional) Experiment with a different policy architecture, e.g. using recurrence or changing the size or nonlinearities used.\n",
    "\n",
    "Compare performance between your new and original policy architectures using behavioral cloning and/or DAgger,\n",
    "and report your results in the same form as above, with a caption describing what you did.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "hw1.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
