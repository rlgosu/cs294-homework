{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Assignments\n",
    "1. Homework 1: Imitation learning (control via supervised learning)\n",
    "2. Homework 2: Policy gradients (“REINFORCE”)\n",
    "3. Homework 3: Q learning and actor-critic algorithms\n",
    "4. Homework 4: Model-based reinforcement learning\n",
    "5. Homework 5: Advanced model-free RL algorithms\n",
    "6. Final project: Research-level project of your choice (form a group of up to 2-3 students, you’re welcome to start early!)\n",
    "\n",
    "##### Emacs IPython Notebook Commands/Keybinds\n",
    "* http://millejoh.github.io/emacs-ipython-notebook/#commands-keybinds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Homework 1 Imitation Learning\n",
    "\n",
    "Instead of trying to produce a program to simulate the adult mind, why not rather try to produce one which simulates the child's?\n",
    "\n",
    "If this were then subjected to an appropriate course of education one would obtain the adult brain.\n",
    "\n",
    "\\- Alan Turing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Behavioral Cloning\n",
    "\n",
    "1. The starter code provides an expert policy for each of the MuJoCo tasks in OpenAI Gym (See run expert.py). Generate roll-outs from the provided policies, and implement behavioral cloning. => expert_data/XXX.pkl\n",
    "\n",
    "2. Run behavioral cloning (BC) and report results on two tasks\n",
    " – one task where a behavioral cloning agent achieves comparable performance to the expert,\n",
    " and one task where it does not.\n",
    " When providing results, report the mean and standard deviation of the return over multiple rollouts in a table, and state which task was used.\n",
    " Be sure to set up a fair comparison, in terms of network size, amount of data, and number of training iterations, and provide these details (and any others you feel are appropriate) in the table caption.\n",
    "\n",
    "3. Experiment with one hyperparameter that affects the performance of the behavioral cloning agent, such as\n",
    "* the number of demonstrations,\n",
    "* the number of training epochs,\n",
    "* the variance of the expert policy, or\n",
    "* something that you come up with yourself.\n",
    " For one of the tasks used in the previous question, show a graph of how the BC agent’s performance varies with the value of this hyperparameter, and state the hyperparameter and a brief rationale for why you chose it in the caption for the graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "gym_envs = ('Ant-v2', 'Hopper-v2', 'Reacher-v2', 'HalfCheetah-v2', 'Humanoid-v2', 'Walker2d-v2')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## running experts\n",
    "\n",
    "run experts of each gym environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# run expert\n",
    "\n",
    "import sys, os\n",
    "import datetime as dt\n",
    "import run_expert\n",
    "\n",
    "# https://ipython.readthedocs.io/en/stable/interactive/magics.html\n",
    "# path=os.environ['PATH']\n",
    "# %env PATH='/usr/local/bin:'+path\n",
    "\n",
    "RENDER = False\n",
    "NUM_ROLLOUTS = 20\n",
    "\n",
    "for gym_env in gym_envs :\n",
    "    sys.argv = ['run_expert.py', 'experts/' + gym_env + '.pkl', gym_env, '--num_rollouts', str(NUM_ROLLOUTS) ]\n",
    "    if RENDER :\n",
    "        sys.argv.append('--render')\n",
    "    run_expert.main()\n",
    "    print('finished run_expert ', gym_env, 'at', dt.datetime.now())\n",
    "\n",
    "print('finished run_expert on all gym_envs at', dt.datetime.now())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## train on each envs\n",
    "\n",
    "using the data gathered by expert policy\n",
    "\n",
    "* environment details : https://github.com/openai/gym/tree/master/gym/envs/mujoco/assets\n",
    "* source codes of each environments : https://github.com/openai/gym/blob/master/gym/envs/mujoco/\n",
    "* reference for an HW1 implementation :  https://hollygrimm.com/rl_bc\n",
    "\n",
    "for the regressor\n",
    "input : observation\n",
    "output : action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from load_policy import load_policy\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_expert_data(gymenv) -> (np.array, np.array) : # observations, actions\n",
    "    with open(os.path.join('expert_data', gymenv + '.pkl'), 'rb') as f :\n",
    "        expert_data = pk.load(f)\n",
    "        return expert_data['observations'], expert_data['actions']\n",
    "\n",
    "def load_expert_policy_fn(gymenv) :\n",
    "    return load_policy('experts/' + gymenv + '.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Behavior Cloning\n",
    "\n",
    "1. generate rollouts(= expert data) with expert policy (and record the returns)\n",
    "2. learn the rollouts changing some environments (network size, amount of data, and number of training iterations, ...)\n",
    "3. generate rollouts several times according to each policies learned above and show the returns in a table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from enum import Enum, IntEnum\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime as dt\n",
    "import time\n",
    "import math\n",
    "\n",
    "default_model_config = dict(neurons = [400, 200, 100],\n",
    "                            activation = tf.nn.elu, # Using ReLu, which is a discontinuous function, may raise issues. Try using other activation functions, such as tanh or sigmoid.\n",
    "                            last_activation = None, # final layer activation function. default is no activation\n",
    "                            optimizer = tf.train.AdadeltaOptimizer, # tf.train.AdamOptimizer, tf.train.ProximalAdagradOptimizer\n",
    "                            cost_function = tf.losses.mean_squared_error, # tf.losses.huber_loss (robust to outlier)\n",
    "                            measure_function = 'r_squared', # 'smape' means symmetric_mean_absolute_percentage_error\n",
    ")\n",
    "\n",
    "default_train_config = dict(start_learning_rate = 0.001,\n",
    "                            # minimum_learning_rate = 0.000001,\n",
    "                            num_epochs = 1000,\n",
    "                            batch_size = 100, # 500,\n",
    "                            keep_prob = 0.9, # for training only (dropout)\n",
    "                            validationset_percent = 0.2, # by default 20 percent is validation set\n",
    "                            break_accuracy = -1.0, # 0.999, # -1.0\n",
    "                            early_stopping_epoch_on_max_no_decrease = 20, # 100,\n",
    "                            shuffle_samples_epochs = 10, # shuffle samples per given epochs considering performance. -1 means no shuffling\n",
    "                            check_accuracy_epochs = 200, # 5000,\n",
    "                            use_tboard = True,\n",
    "                            print_cost_interval = 500,\n",
    "                            print_trained_model = False,\n",
    "                            )\n",
    "\n",
    "class BehavioralCloning(object) :\n",
    "    default_random_seed = 777\n",
    "\n",
    "    def __init__(self,\n",
    "                 X_shape = None, # X shape as list\n",
    "                 Y_shape = None, # Y shape as list\n",
    "                 model_config = default_model_config,\n",
    "                 scope_name = '',\n",
    "                 restore_mode=False,\n",
    "                 session=None) :\n",
    "        self.model_config = model_config\n",
    "        self.restore_mode = restore_mode\n",
    "        self.scope_name = scope_name\n",
    "        self.X_shape = list(X_shape)\n",
    "        self.X_shape[0] = None\n",
    "        self.Y_shape = list(Y_shape)\n",
    "        self.Y_shape[0] = None\n",
    "\n",
    "        tf.set_random_seed(BehavioralCloning.default_random_seed)  # reproducibility\n",
    "        np.random.seed(BehavioralCloning.default_random_seed)\n",
    "\n",
    "        # Launch new session before graph init\n",
    "        # interactive session will declare itself as a default session and won't be closed on context destroy (so, should explicity call sess.close()\n",
    "        if session is None :\n",
    "            tf.reset_default_graph()\n",
    "            self.session = tf.InteractiveSession()\n",
    "        else :\n",
    "            self.session = session\n",
    "        self._build_network()\n",
    "\n",
    "    def _build_network(self) :\n",
    "        g = tf.get_default_graph()\n",
    "\n",
    "        # build the network\n",
    "        with g.as_default(), self.session.as_default() :\n",
    "            self.X = tf.placeholder(tf.float32, shape=self.X_shape, name='X')\n",
    "            self.Y = tf.placeholder(tf.float32, shape=self.Y_shape, name='Y')\n",
    "            self.p_keep_prob = tf.placeholder(tf.float32, name='p_keep_prob')\n",
    "            self.p_training = tf.placeholder(tf.bool, name='p_training')\n",
    "            self.p_lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "            with tf.variable_scope(self.scope_name + '-dnn', reuse=tf.AUTO_REUSE) as scope:\n",
    "                neurons = self.model_config['neurons']\n",
    "                layer = self.X\n",
    "                for i in range(len(neurons)) :\n",
    "                    neuron = neurons[i]\n",
    "\n",
    "                    layer = tf.layers.dense(layer, neuron,\n",
    "                                            kernel_initializer = tf.contrib.layers.xavier_initializer(seed=BehavioralCloning.default_random_seed),\n",
    "                                            activation=self.model_config['activation'],\n",
    "                                            name = 'layer-' + str(i))\n",
    "                    layer = tf.layers.dropout(layer, rate=1-self.p_keep_prob, training=self.p_training)\n",
    "                n_output = self.Y_shape[1]\n",
    "                layer = tf.layers.dense(layer, n_output,\n",
    "                                        kernel_initializer = tf.contrib.layers.xavier_initializer(seed=BehavioralCloning.default_random_seed),\n",
    "                                        activation=self.model_config['last_activation'],\n",
    "                                        name = 'layer-last')\n",
    "                    \n",
    "\n",
    "                self.hypothesis = layer\n",
    "                cost_fn = self.model_config['cost_function']\n",
    "                self.cost = cost_fn(self.Y, self.hypothesis)\n",
    "                tf.summary.scalar(\"cost\", self.cost)\n",
    "                measure_alg = self.model_config['measure_function']\n",
    "                if measure_alg == 'r_squared' :\n",
    "                    self.measure = self.r_squared(self.Y, self.hypothesis)\n",
    "                elif measure_alg == 'smape' :\n",
    "                    self.measure = self.smape(self.Y, self.hypothesis)\n",
    "                else :\n",
    "                    self.measure = None\n",
    "                optimizer_fn = self.model_config['optimizer']\n",
    "                opt = optimizer_fn(learning_rate=self.p_lr)\n",
    "                self.objective_tensor = opt.minimize(self.cost)\n",
    "\n",
    "            if not self.restore_mode :\n",
    "                self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    def train(self, X, Y, train_config = default_train_config) :\n",
    "        learning_rate = train_config['start_learning_rate']\n",
    "        num_epochs = train_config['num_epochs']\n",
    "        keep_prob = train_config['keep_prob']\n",
    "        batch_size = train_config['batch_size']\n",
    "        vset_percent = train_config['validationset_percent']\n",
    "        break_accuracy = train_config['break_accuracy']\n",
    "        check_accuracy_epochs = train_config['check_accuracy_epochs']\n",
    "        early_stopping_epoch_on_max_no_decrease = train_config['early_stopping_epoch_on_max_no_decrease']\n",
    "        print_cost_interval = train_config['print_cost_interval']\n",
    "        shuffle_samples_epochs = train_config['shuffle_samples_epochs']\n",
    "        use_tboard = train_config['use_tboard']\n",
    "\n",
    "        training_costs = np.zeros(num_epochs, dtype=np.float32)\n",
    "        validation_costs = np.zeros(num_epochs, dtype=np.float32)\n",
    "        validation_measures = np.zeros(num_epochs, dtype=np.float32)\n",
    "        min_cost = np.inf\n",
    "        no_cost_decrease_epochs = 0\n",
    "\n",
    "        n_samples = X.shape[0]\n",
    "        n_features = X.shape[1]\n",
    "        n_output = Y.shape[1]\n",
    "        n_train = int(n_samples * (1 - vset_percent))\n",
    "        n_validate = n_samples - n_train\n",
    "\n",
    "        batch_loop = (n_train - 1) // batch_size + 1\n",
    "\n",
    "        sess = self.session\n",
    "        if use_tboard :\n",
    "            merged_summary = tf.summary.merge_all()\n",
    "            writer = tf.summary.FileWriter(\"./tboard_logs\")\n",
    "            writer.add_graph(sess.graph)  # Show the graph\n",
    "        else :\n",
    "            merged_summary = None\n",
    "\n",
    "        current_X = train_X = X[:n_train]\n",
    "        current_Y = train_Y = Y[:n_train]\n",
    "        validate_X = X[n_train:]\n",
    "        validate_Y = Y[n_train:]\n",
    "\n",
    "        if shuffle_samples_epochs > 0 :\n",
    "            current_XY = np.hstack((train_X, train_Y))\n",
    "\n",
    "        start_time = dt.datetime.now()\n",
    "        print('Learning starts. It will take some time...', start_time)\n",
    "        for epoch in range(num_epochs):\n",
    "            shuffle_samples = shuffle_samples_epochs > 0 and epoch % shuffle_samples_epochs == 0 # shuffle on 0th epoch\n",
    "            \n",
    "            if shuffle_samples :\n",
    "                np.random.shuffle(current_XY) # this will shuffle current_XY in place.\n",
    "                _, current_X, current_Y = np.split(current_XY, (0, n_features), axis=-1)\n",
    "\n",
    "            epoch_hyps = np.zeros(Y.shape, dtype=np.float32)\n",
    "            epoch_costs = np.zeros(batch_loop, dtype=np.float32)\n",
    "\n",
    "            for m in range(batch_loop) :\n",
    "                if m == batch_loop - 1 :\n",
    "                    m_X = current_X[batch_size * m :]\n",
    "                    m_Y = current_Y[batch_size * m :]\n",
    "                else :\n",
    "                    m_X = current_X[batch_size * m : batch_size * (m + 1)]\n",
    "                    m_Y = current_Y[batch_size * m : batch_size * (m + 1)]\n",
    "\n",
    "                feed_dict = {self.X:m_X, self.Y:m_Y,\n",
    "                             self.p_keep_prob:keep_prob,\n",
    "                             self.p_lr:learning_rate,\n",
    "                             self.p_training:True}\n",
    "                targets = [ self.hypothesis, self.cost, self.objective_tensor ]\n",
    "                if use_tboard :\n",
    "                    targets.append(merged_summary)\n",
    "                # print('m:', m, ', m_X:', np.shape(m_X), ', m_Y:', np.shape(m_Y), ', feed_dict:', feed_dict)\n",
    "                results = sess.run(targets, feed_dict = feed_dict)\n",
    "                if use_tboard :\n",
    "                    writer.add_summary(results[-1], global_step = epoch * batch_loop + m)\n",
    "\n",
    "                h_value = results[0]\n",
    "                epoch_hyps[batch_size * m : batch_size * m + m_Y.shape[0]] = h_value\n",
    "                cost_value = results[1]\n",
    "                epoch_costs[m] = cost_value\n",
    "\n",
    "            training_costs[epoch] = avg_cost = np.mean(epoch_costs)\n",
    "\n",
    "            validate_feed_dict = {self.X: validate_X, self.Y: validate_Y,\n",
    "                                  self.p_keep_prob:1.0, self.p_training:False}\n",
    "            validate_targets = [ self.hypothesis, self.cost, self.measure ]\n",
    "            vs_hyps, vs_cost, vs_measure = sess.run(validate_targets, feed_dict=validate_feed_dict)\n",
    "            validation_costs[epoch] = vs_cost\n",
    "            validation_measures[epoch] = vs_measure\n",
    "\n",
    "            if epoch % print_cost_interval == 0 :\n",
    "                print('Epoch:', '%04d' % epoch, 'average training cost =', '{:.9f}'.format(avg_cost),\n",
    "                      'validation cost =', '{:.9f}'.format(vs_cost), 'validation measure =', '{:.9f}'.format(vs_measure), dt.datetime.now())\n",
    "\n",
    "            if epoch % check_accuracy_epochs == check_accuracy_epochs :\n",
    "                print('Epoch:', '%04d' % epoch, 'validation cost =', '{:.9f}'.format(vs_cost),\n",
    "                      'validation measure =', '{:.9f}'.format(vs_measure), dt.datetime.now())\n",
    "\n",
    "                if break_accuracy > 0 and break_accuracy < vs_cost :\n",
    "                    print('Stops the training due to validation loss', vs_cost, ' exceeded the criteria', break_accuracy)\n",
    "                    training_costs = training_costs[:epoch + 1] # strip un-run epochs\n",
    "                    validation_costs = validation_costs[:epoch + 1] # strip un-run epochs\n",
    "                    validation_measures = validation_measures[:epoch + 1] # strip un-run epochs\n",
    "                    break\n",
    "\n",
    "            if early_stopping_epoch_on_max_no_decrease > 0 :\n",
    "                if vs_cost < min_cost :\n",
    "                    min_cost = vs_cost\n",
    "                    no_cost_decrease_epochs = 0\n",
    "                else :\n",
    "                    no_cost_decrease_epochs = no_cost_decrease_epochs + 1\n",
    "                    if no_cost_decrease_epochs >= early_stopping_epoch_on_max_no_decrease :\n",
    "                        print('Stops the training since cost is not reduced during ', no_cost_decrease_epochs, ' epochs.')\n",
    "                        training_costs = training_costs[:epoch + 1] # strip un-run epochs\n",
    "                        validation_costs = validation_costs[:epoch + 1] # strip un-run epochs\n",
    "                        validation_measures = validation_measures[:epoch + 1] # strip un-run epochs\n",
    "                        break\n",
    "\n",
    "        end_time = dt.datetime.now()\n",
    "        print('Training(learning) Finished!', end_time)\n",
    "        print('Training took ', '%10d' % ((end_time - start_time).total_seconds()),\n",
    "              ' seconds.')\n",
    "   \n",
    "        return training_costs, validation_costs, validation_measures\n",
    "                \n",
    "\n",
    "    def test(self, X, Y) :\n",
    "        start_time = dt.datetime.now()\n",
    "        g = tf.get_default_graph()\n",
    "\n",
    "        with g.as_default() :\n",
    "            vals = self._test_model(X, Y)\n",
    "            end_time = dt.datetime.now()\n",
    "            print('Prediction took ', '%10d' % ((end_time - start_time).total_seconds()),\n",
    "                  ' seconds.')\n",
    "            print('Started at ', start_time, ' and finished at ', end_time)\n",
    "            return vals\n",
    "\n",
    "    def _test_model(self, X, Y) :\n",
    "        test_feed_dict = {self.X: X, self.Y: Y,\n",
    "                          self.p_keep_prob:1.0, self.p_training:False}\n",
    "        test_targets = [ self.hypothesis, self.cost, self.measure ]\n",
    "\n",
    "        sess = self.session\n",
    "        hyps, cost, measure = sess.run(test_targets, feed_dict=test_feed_dict)\n",
    "        return hyps, cost, measure\n",
    "\n",
    "    def infer(self, X) :\n",
    "        g = tf.get_default_graph()\n",
    "\n",
    "        with g.as_default() :\n",
    "            vals = self._infer_model(X)\n",
    "            return vals\n",
    "\n",
    "    def _infer_model(self, X) :\n",
    "        test_feed_dict = {self.X: X,\n",
    "                          self.p_keep_prob:1.0, self.p_training:False}\n",
    "        test_targets = [ self.hypothesis ]\n",
    "\n",
    "        sess = self.session\n",
    "        hyps = sess.run(test_targets, feed_dict=test_feed_dict)\n",
    "        return hyps\n",
    "    \n",
    "    def r_squared(self, y, h) :\n",
    "        # in tf.reduce_mean, if axis has no entries, all dimensions are reduced, and a tensor with a single element is returned\n",
    "        total_error = tf.reduce_sum(tf.square(tf.subtract(y, tf.reduce_mean(y, 0))))  # reduce_mean by 0-axis maintains vector dimension\n",
    "        unexplained_error = tf.reduce_sum(tf.square(tf.subtract(y, h)))\n",
    "        r_squared = tf.subtract(1.0, tf.div(unexplained_error, total_error))\n",
    "        return r_squared\n",
    "\n",
    "    def smape(self, y, h) :\n",
    "        return tf.reduce_mean(2.0 * tf.abs(tf.subtract(y, h)) / tf.maximum(1e-7, (tf.abs(y) + tf.abs(h)))) # tf.maximum is used to avoid nan\n",
    "        \n",
    "    def check_nan(self, value) :\n",
    "        return value is None or math.isnan(value)\n",
    "\n",
    "    def save_model(self, save_file_name) :\n",
    "        # self._dump_graph('save_model(' + save_file_name + ')')\n",
    "        \n",
    "        tf.train.Saver().save(self.session, save_file_name)\n",
    "\n",
    "    def _dump_graph(self, where) :\n",
    "        print('')\n",
    "\n",
    "        print('--- dumping tensorflow graph [', where, '] ---')\n",
    "        g = tf.get_default_graph()\n",
    "        print('default tf graph :', g)\n",
    "\n",
    "        # debug graphs\n",
    "        keys = g.get_all_collection_keys()\n",
    "        print('current name scope :', g.get_name_scope())\n",
    "        for key in keys :\n",
    "            print('all graph (', key, ')  :', g.get_collection(key))\n",
    "        print('') \n",
    "        print('')\n",
    "\n",
    "       \n",
    "    def restore_model(self, saved_dir) :\n",
    "        print('saved dir:', saved_dir)\n",
    "\n",
    "        with self.session.as_default() :\n",
    "            # self._dump_graph('restore_model(' + saved_dir + ')')\n",
    "            \n",
    "            reader = tf.train.NewCheckpointReader(saved_dir)\n",
    "            # for var_name in reader.get_variable_to_shape_map() :\n",
    "            #     print(var_name)\n",
    "        \n",
    "            tf.train.Saver().restore(self.session, saved_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util\n",
    "\n",
    "TEST_PERCENT = 0.2\n",
    "\n",
    "def shuffle_XY(X, Y) :\n",
    "    hstacked = np.hstack((X, Y))\n",
    "    np.random.shuffle(hstacked)\n",
    "    _, new_X, new_Y = np.split(hstacked, (0, X.shape[1]), axis=-1)\n",
    "    return new_X, new_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  Ant-v2 2019-07-07 22:42:44.316212\n",
      "Ant-v2  observation shape:  (19992, 111) , actions shape: (19992, 8)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0707 22:42:47.032993 50204 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0707 22:42:47.034992 50204 deprecation.py:323] From <ipython-input-3-fc99626fbdc6>:79: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0707 22:42:47.263379 50204 deprecation.py:323] From <ipython-input-3-fc99626fbdc6>:80: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "W0707 22:42:47.366063 50204 deprecation.py:323] From C:\\Works\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0707 22:42:47.378071 50204 deprecation.py:323] From <ipython-input-3-fc99626fbdc6>:272: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning starts. It will take some time... 2019-07-07 22:42:47.639329\n",
      "Epoch: 0000 average training cost = 0.802247047 validation cost = 0.619904339 validation measure = -7.028655052 2019-07-07 22:42:48.765364\n",
      "Epoch: 0500 average training cost = 0.060373325 validation cost = 0.018596660 validation measure = 0.759146452 2019-07-07 22:46:46.832457\n",
      "Training(learning) Finished! 2019-07-07 22:50:53.710971\n",
      "Training took         486  seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Works\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n",
      "W0707 22:50:54.270516 50204 deprecation.py:323] From C:\\Works\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved dir: model_Ant-v2\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-0/kernel/Adadelta\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-0/bias/Adadelta\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-0/kernel/Adadelta_1\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-0/bias/Adadelta_1\n",
      "Ant-v2-dnn/layer-last/kernel\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-1/kernel/Adadelta_1\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-1/bias/Adadelta\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-1/kernel/Adadelta\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-1/bias/Adadelta_1\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-last/kernel/Adadelta_1\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-last/kernel/Adadelta\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-2/bias/Adadelta_1\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-2/bias/Adadelta\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-last/bias/Adadelta_1\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-2/kernel/Adadelta\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-last/bias/Adadelta\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-2/kernel/Adadelta_1\n",
      "Ant-v2-dnn/layer-0/bias\n",
      "Ant-v2-dnn/layer-0/kernel\n",
      "Ant-v2-dnn/layer-1/bias\n",
      "Ant-v2-dnn/layer-1/kernel\n",
      "Ant-v2-dnn/layer-2/bias\n",
      "Ant-v2-dnn/layer-2/kernel\n",
      "Ant-v2-dnn/layer-last/bias\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-07 22:50:54.303385  and finished at  2019-07-07 22:50:54.328319\n",
      "ending  Ant-v2 2019-07-07 22:50:54.328319\n",
      "starting  Hopper-v2 2019-07-07 22:50:54.329315\n",
      "Hopper-v2  observation shape:  (20000, 11) , actions shape: (20000, 3)\n",
      "Learning starts. It will take some time... 2019-07-07 22:50:54.754780\n",
      "Epoch: 0000 average training cost = 2.600547791 validation cost = 2.395168543 validation measure = -0.116906166 2019-07-07 22:50:55.264421\n",
      "Epoch: 0500 average training cost = 0.258199543 validation cost = 0.116882704 validation measure = 0.945495665 2019-07-07 22:55:03.817415\n",
      "Training(learning) Finished! 2019-07-07 22:59:11.267866\n",
      "Training took         496  seconds.\n",
      "saved dir: model_Hopper-v2\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-0/kernel/Adadelta\n",
      "Hopper-v2-dnn/layer-last/bias\n",
      "Hopper-v2-dnn/layer-0/bias\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-0/bias/Adadelta\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-0/kernel/Adadelta_1\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-0/bias/Adadelta_1\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-1/kernel/Adadelta_1\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-1/bias/Adadelta\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-1/kernel/Adadelta\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-1/bias/Adadelta_1\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-2/bias/Adadelta\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-2/bias/Adadelta_1\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-2/kernel/Adadelta\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-2/kernel/Adadelta_1\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-last/bias/Adadelta\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-last/bias/Adadelta_1\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-last/kernel/Adadelta\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-last/kernel/Adadelta_1\n",
      "Hopper-v2-dnn/layer-last/kernel\n",
      "Hopper-v2-dnn/layer-0/kernel\n",
      "Hopper-v2-dnn/layer-1/bias\n",
      "Hopper-v2-dnn/layer-1/kernel\n",
      "Hopper-v2-dnn/layer-2/bias\n",
      "Hopper-v2-dnn/layer-2/kernel\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-07 22:59:11.814400  and finished at  2019-07-07 22:59:11.835344\n",
      "ending  Hopper-v2 2019-07-07 22:59:11.835344\n",
      "starting  Reacher-v2 2019-07-07 22:59:11.835344\n",
      "Reacher-v2  observation shape:  (1000, 11) , actions shape: (1000, 2)\n",
      "Learning starts. It will take some time... 2019-07-07 22:59:12.310076\n",
      "Epoch: 0000 average training cost = 0.149326846 validation cost = 0.102704681 validation measure = -14.160396576 2019-07-07 22:59:12.446217\n",
      "Epoch: 0500 average training cost = 0.053795539 validation cost = 0.008411404 validation measure = -0.241620302 2019-07-07 22:59:27.681710\n",
      "Stops the training since cost is not reduced during  20  epochs.\n",
      "Training(learning) Finished! 2019-07-07 22:59:40.681279\n",
      "Training took          28  seconds.\n",
      "saved dir: model_Reacher-v2\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-0/bias/Adadelta_1\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-0/bias/Adadelta\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-0/kernel/Adadelta_1\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-0/kernel/Adadelta\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-1/bias/Adadelta_1\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-1/kernel/Adadelta_1\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-1/bias/Adadelta\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-1/kernel/Adadelta\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-last/kernel/Adadelta\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-2/bias/Adadelta\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-last/kernel/Adadelta_1\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-2/bias/Adadelta_1\n",
      "Reacher-v2-dnn/layer-last/bias\n",
      "Reacher-v2-dnn/layer-0/bias\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-last/bias/Adadelta\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-2/kernel/Adadelta\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-last/bias/Adadelta_1\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-2/kernel/Adadelta_1\n",
      "Reacher-v2-dnn/layer-last/kernel\n",
      "Reacher-v2-dnn/layer-0/kernel\n",
      "Reacher-v2-dnn/layer-1/bias\n",
      "Reacher-v2-dnn/layer-1/kernel\n",
      "Reacher-v2-dnn/layer-2/bias\n",
      "Reacher-v2-dnn/layer-2/kernel\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-07 22:59:41.213917  and finished at  2019-07-07 22:59:41.230912\n",
      "ending  Reacher-v2 2019-07-07 22:59:41.230912\n",
      "starting  HalfCheetah-v2 2019-07-07 22:59:41.230912\n",
      "HalfCheetah-v2  observation shape:  (20000, 17) , actions shape: (20000, 6)\n",
      "Learning starts. It will take some time... 2019-07-07 22:59:41.698662\n",
      "Epoch: 0000 average training cost = 2.378319740 validation cost = 1.693411469 validation measure = -2.122699738 2019-07-07 22:59:42.282063\n",
      "Epoch: 0500 average training cost = 0.191245407 validation cost = 0.068073958 validation measure = 0.874469638 2019-07-07 23:04:04.988075\n",
      "Training(learning) Finished! 2019-07-07 23:08:28.164671\n",
      "Training took         526  seconds.\n",
      "saved dir: model_HalfCheetah-v2\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-0/kernel/Adadelta\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-0/bias/Adadelta\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-0/kernel/Adadelta_1\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-0/bias/Adadelta_1\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-1/kernel/Adadelta_1\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-1/bias/Adadelta\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-1/kernel/Adadelta\n",
      "HalfCheetah-v2-dnn/layer-2/kernel\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-1/bias/Adadelta_1\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-last/kernel/Adadelta_1\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-last/kernel/Adadelta\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-2/bias/Adadelta_1\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-2/bias/Adadelta\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-last/bias/Adadelta_1\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-2/kernel/Adadelta\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-last/bias/Adadelta\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-2/kernel/Adadelta_1\n",
      "HalfCheetah-v2-dnn/layer-0/bias\n",
      "HalfCheetah-v2-dnn/layer-0/kernel\n",
      "HalfCheetah-v2-dnn/layer-1/bias\n",
      "HalfCheetah-v2-dnn/layer-1/kernel\n",
      "HalfCheetah-v2-dnn/layer-last/kernel\n",
      "HalfCheetah-v2-dnn/layer-2/bias\n",
      "HalfCheetah-v2-dnn/layer-last/bias\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-07 23:08:28.801573  and finished at  2019-07-07 23:08:28.821519\n",
      "ending  HalfCheetah-v2 2019-07-07 23:08:28.822515\n",
      "starting  Humanoid-v2 2019-07-07 23:08:28.822515\n",
      "Humanoid-v2  observation shape:  (20000, 376) , actions shape: (20000, 17)\n",
      "Learning starts. It will take some time... 2019-07-07 23:08:29.377031\n",
      "Epoch: 0000 average training cost = 1084.891601562 validation cost = 678.221008301 validation measure = -711.868530273 2019-07-07 23:08:29.996379\n",
      "Epoch: 0500 average training cost = 5.571677208 validation cost = 1.397883415 validation measure = -0.469295502 2019-07-07 23:13:27.206154\n",
      "Training(learning) Finished! 2019-07-07 23:18:24.337937\n",
      "Training took         594  seconds.\n",
      "saved dir: model_Humanoid-v2\n",
      "Humanoid-v2-dnn/layer-1/kernel\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-0/kernel/Adadelta\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-0/bias/Adadelta\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-0/kernel/Adadelta_1\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-0/bias/Adadelta_1\n",
      "Humanoid-v2-dnn/layer-2/kernel\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-1/kernel/Adadelta_1\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-1/bias/Adadelta\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-1/kernel/Adadelta\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-1/bias/Adadelta_1\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-2/bias/Adadelta\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-2/bias/Adadelta_1\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-2/kernel/Adadelta\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-2/kernel/Adadelta_1\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-last/bias/Adadelta\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-last/bias/Adadelta_1\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-last/kernel/Adadelta\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-last/kernel/Adadelta_1\n",
      "Humanoid-v2-dnn/layer-0/kernel\n",
      "Humanoid-v2-dnn/layer-0/bias\n",
      "Humanoid-v2-dnn/layer-1/bias\n",
      "Humanoid-v2-dnn/layer-2/bias\n",
      "Humanoid-v2-dnn/layer-last/bias\n",
      "Humanoid-v2-dnn/layer-last/kernel\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-07 23:18:24.839546  and finished at  2019-07-07 23:18:24.868512\n",
      "ending  Humanoid-v2 2019-07-07 23:18:24.868512\n",
      "starting  Walker2d-v2 2019-07-07 23:18:24.868512\n",
      "Walker2d-v2  observation shape:  (20000, 17) , actions shape: (20000, 6)\n",
      "Learning starts. It will take some time... 2019-07-07 23:18:25.457076\n",
      "Epoch: 0000 average training cost = 2.810679436 validation cost = 2.294559002 validation measure = -1.296395540 2019-07-07 23:18:26.029480\n",
      "Epoch: 0500 average training cost = 0.379968286 validation cost = 0.203127339 validation measure = 0.796710074 2019-07-07 23:22:49.523807\n",
      "Training(learning) Finished! 2019-07-07 23:27:12.302580\n",
      "Training took         526  seconds.\n",
      "saved dir: model_Walker2d-v2\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-0/bias/Adadelta\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-0/kernel/Adadelta\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-0/bias/Adadelta_1\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-0/kernel/Adadelta_1\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-1/kernel/Adadelta_1\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-1/bias/Adadelta\n",
      "Walker2d-v2-dnn/layer-1/bias\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-1/kernel/Adadelta\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-1/bias/Adadelta_1\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-2/bias/Adadelta\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-2/bias/Adadelta_1\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-2/kernel/Adadelta_1\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-2/kernel/Adadelta\n",
      "Walker2d-v2-dnn/layer-last/bias\n",
      "Walker2d-v2-dnn/layer-0/kernel\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-last/bias/Adadelta_1\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-last/bias/Adadelta\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-last/kernel/Adadelta\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-last/kernel/Adadelta_1\n",
      "Walker2d-v2-dnn/layer-last/kernel\n",
      "Walker2d-v2-dnn/layer-0/bias\n",
      "Walker2d-v2-dnn/layer-1/kernel\n",
      "Walker2d-v2-dnn/layer-2/bias\n",
      "Walker2d-v2-dnn/layer-2/kernel\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-07 23:27:12.788744  and finished at  2019-07-07 23:27:12.807693\n",
      "ending  Walker2d-v2 2019-07-07 23:27:12.807693\n"
     ]
    }
   ],
   "source": [
    "# train behavior cloning policies\n",
    "\n",
    "for gym_env in gym_envs :\n",
    "    print('starting ', gym_env, dt.datetime.now())\n",
    "    observations, actions = load_expert_data(gym_env)\n",
    "    obs_shape, action_shape = np.shape(observations), np.shape(actions)\n",
    "    if action_shape[1] == 1 :\n",
    "        actions = np.reshape(actions, (action_shape[0], action_shape[2]))\n",
    "        action_shape = np.shape(actions)\n",
    "    print(gym_env, ' observation shape: ', obs_shape, ', actions shape:', action_shape)\n",
    "    # for i in range(2) :\n",
    "    #     print('observation:', observations[i])\n",
    "    #     print('actions:', actions[i])\n",
    "    cloning = BehavioralCloning(X_shape=obs_shape, Y_shape=action_shape, scope_name=gym_env)\n",
    "    \n",
    "    n_samples = observations.shape[0]\n",
    "    n_train = int(n_samples * (1 - TEST_PERCENT))\n",
    "\n",
    "    observations, actions = shuffle_XY(observations, actions)\n",
    "    training_costs, validation_costs, validation_measures = cloning.train(observations[:n_train], actions[:n_train])\n",
    "    \n",
    "    gym_env_model = 'model_' + gym_env\n",
    "    cloning.save_model(gym_env_model)\n",
    "    \n",
    "    cloning = BehavioralCloning(X_shape=obs_shape, Y_shape=action_shape, scope_name=gym_env, restore_mode=True)\n",
    "    cloning.restore_model(gym_env_model)\n",
    "    \n",
    "    test_hyps, test_costs, test_measures = cloning.test(observations[n_train:], actions[n_train:])\n",
    "    print('ending ', gym_env, dt.datetime.now())\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# run_expert source code for reference\n",
    "\n",
    "import tf_util\n",
    "import pickle as pk\n",
    "import traceback\n",
    "\n",
    "def load_learned_policy_fn(gym_env, session=None) :\n",
    "    observations, actions = load_expert_data(gym_env)\n",
    "    obs_shape, action_shape = list(np.shape(observations)), list(np.shape(actions))\n",
    "    if action_shape[1] == 1 :\n",
    "        actions = np.reshape(actions, (action_shape[0], action_shape[2]))\n",
    "        action_shape = list(np.shape(actions))\n",
    "\n",
    "    gym_env_model = 'model_' + gym_env\n",
    "    cloning = BehavioralCloning(X_shape=obs_shape, Y_shape=action_shape, scope_name=gym_env, restore_mode=True, session=session)\n",
    "    cloning.restore_model(gym_env_model)\n",
    "\n",
    "    return lambda x : cloning.infer(x)\n",
    "    \n",
    "def rollout_by_policy(gym_env, max_timesteps, num_rollouts, policy_fn=None, render=False) :\n",
    "    policy_type = 'learned'\n",
    "    \n",
    "    if policy_fn is None : # default policy_fn is expert policy\n",
    "        print('loading and building expert policy')\n",
    "        policy_fn = load_expert_policy_fn(gym_env)\n",
    "        print('loaded and built')\n",
    "        policy_type = 'expert'\n",
    "\n",
    "    with tf.Session():\n",
    "        tf_util.initialize()\n",
    "\n",
    "        import gym\n",
    "        env = gym.make(gym_env)\n",
    "        max_steps = max_timesteps or env.spec.timestep_limit\n",
    "\n",
    "        returns = []\n",
    "        observations = []\n",
    "        actions = []\n",
    "        for i in range(num_rollouts):\n",
    "            # print('iter', i)\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            totalr = 0.\n",
    "            steps = 0\n",
    "            while not done:\n",
    "                action = policy_fn(obs[None,:])\n",
    "                observations.append(obs)\n",
    "                actions.append(action)\n",
    "                try :\n",
    "                    if np.shape(action)[1] == 1 :\n",
    "                        action_shape = np.shape(action)\n",
    "                        action = np.reshape(action, (action_shape[0], action_shape[2]))\n",
    "                    obs, r, done, _ = env.step(action) # observation, reward, done\n",
    "                except ValueError as e :\n",
    "                    print('action:', action, ', shape:', np.shape(action), ', policy_type:', policy_type)\n",
    "                    print('actions:', actions, ', shape:', np.shape(actions))\n",
    "                    traceback.print_exc()     \n",
    "\n",
    "                totalr += r\n",
    "                steps += 1\n",
    "                if render:\n",
    "                    env.render()\n",
    "                # if steps % 100 == 0: print(\"%i/%i\"%(steps, max_steps))\n",
    "                if steps >= max_steps:\n",
    "                    break\n",
    "            returns.append(totalr)\n",
    "\n",
    "        print('returns', returns)\n",
    "        print('mean return', np.mean(returns))\n",
    "        print('std of return', np.std(returns))\n",
    "\n",
    "        rollout_data = {'observations': np.array(observations),\n",
    "                        'actions': np.array(actions),\n",
    "                        'returns': np.array(returns)}\n",
    "\n",
    "        if not os.path.exists('rollout_data') :\n",
    "            os.mkdir('rollout_data')\n",
    "        with open(os.path.join('rollout_data', policy_type + '-' + gym_env + '.pkl'), 'wb') as f:\n",
    "            pk.dump(rollout_data, f, pk.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        return rollout_data, policy_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0708 14:33:18.295669 47796 deprecation_wrapper.py:119] From C:\\Exception\\Works\\GitHub\\cs294\\yoonforh\\hw1\\load_policy.py:55: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and building expert policy\n",
      "obs (1, 111) (1, 111)\n",
      "loaded and built\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0708 14:33:20.132718 47796 deprecation.py:323] From C:\\Exception\\Works\\GitHub\\cs294\\yoonforh\\hw1\\tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "W0708 14:33:20.133758 47796 deprecation_wrapper.py:119] From C:\\Exception\\Works\\GitHub\\cs294\\yoonforh\\hw1\\tf_util.py:74: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0708 14:33:20.134711 47796 deprecation.py:323] From C:\\Works\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:193: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n",
      "C:\\Works\\tensorflow\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "returns [4815.712362381495, 4860.509479271855, 4732.55623362235, 4898.275253678652, 4868.086334528764, 4866.558692675091, 4733.660141053892, 5014.526213313029, 4786.373443759955, 4843.819035550165, 4803.902974023982, 4802.345895038068, 4678.4477480623045, 4469.588583070921, 4824.500076809985, 4728.360551528681, 4774.040177936831, 4798.805203919687, 4761.277411069205, 4768.343019613617, 1727.3567634482831, 4587.410759847964, 4720.161783621888, 5043.468609473684, 4819.416529199315, 4759.126119713492, 4963.846568408031, 4992.408719208572, 4794.117997864689, 5031.471723101488, 4643.866597961307, 4605.135266573518, 4556.030992129319, 4915.94615197301, 4857.883734399419, 4615.163833353687, 4755.176464370085, 4720.881696857902, 4895.396916418257, 4962.681685045341, 4946.0487163336575, 4882.198097739911, 4736.298086921639, 4925.027660542305, 4797.690222730127, 4968.693646878834, 4969.489915761915, 4829.6306513158615, 4827.745158568047, 4834.440399230945, 4875.380658537668, 4757.835173651218, 4744.7738590946865, 4745.673624983008, 4684.660851874298, 4836.885435224769, 4865.303374413501, 4665.5775911283035, 4767.854096508539, 4758.196049153851, 4961.535940650232, 4935.218822757255, 4771.999142946587, 4664.257420680417, 4861.1017914730455, 4685.404605414296, 4777.359955848976, 4758.975621284542, 4748.425815101924, 4793.730974508638, 4834.35908871821, 5007.220121518594, 4875.392177704091, 4788.509169227343, 4682.22304780624, 4863.579719907441, 4814.046710034163, 4941.363596873049, 4719.98939144462, 4763.595798680287, 4785.682539902957, 4972.187848715873, 4879.341510901253, 4726.16665732845, 4814.910484092092, 4953.457695296784, 4920.5191612521185, 4797.978345338025, 4942.673361247873, 4725.057621239539, 4492.4512757736475, 4968.041112236859, 4748.38264649624, 4639.562761984493, 5053.498356207528, 4841.763449617099, 4838.878562801483, 4808.125138482978, 4973.708985767706, 4719.9028579295355, 4847.06961733527, 4884.198577699061, 5002.217067610898, 4688.481076990859, 4796.777803282808, 4776.466680998774, 4858.654390626538, 4961.152142557753, 4955.519389998469, 4701.534839374488, 4838.780920076631, 4867.733978239749, 4867.09936876899, 4707.102229512929, 4813.32656676292, 4927.242108140972, 4810.488060640848, 4712.452308512753, 4913.306895803349, 4531.7037947959425, 4828.9874447179845, 4903.982761714824, 4808.367435943919, 4862.549873040124, 4882.279668628467, 4804.926184378945, 4801.624202879715, 4915.6072117492, 4881.607746733537, 4896.480849631834, 4938.323631505597, 4918.843829838885, 4848.45762588886, 4895.1877753703175, 4871.230834442087, 4746.484128011875, 4759.830968064837, 4896.987005569313, 4827.056482496163, 4961.945868409189, 4851.477412513693, 4789.383636307246, 4849.461008210646, 4872.137904090504, 4778.36865839478, 4836.344589366752, 4916.128263954756, 4794.641818346105, 4853.824522188709, 4915.063934131915, 4832.753695117041, 4980.557041718965, 4712.24385589715, 4730.602452161244, 4842.448209305287, 4862.28168464539, 4878.733952819121, 4801.7959918384195, 831.6962399117334, 4861.7627123887205, 4704.029470093031, 4687.785544382586, 4643.084778404833, 1153.2162425147317, 4657.591519443883, 4842.47881527793, 4763.772052973119, 4848.760317221211, 4802.8065359604525, 4766.102894687147, 4845.306964271831, 4746.151840142695, 4934.315371370596, 4634.685165924124, 4768.788569606029, 4826.542049635532, 4671.71976518422, 4641.25754425765, 4940.939653797566, 4757.929836181418, 4714.866186533129, 4705.213100059042, 4651.586379633567, 4789.682063265148, 4793.065584890137, 4731.514994615353, 4779.960942372437, 4799.100998228137, 4661.2226820058295, 4850.067198521479, 4893.209280831199, 4641.211645364926, 4913.613295699273, 4573.429460485959, 4923.089557906631, 4942.531441609829, 5073.294274624714, 4851.958065494934, 4934.713888156177, 4807.898602473523]\n",
      "mean return 4761.194459019096\n",
      "std of return 450.18534292166703\n",
      "Rollout result. env: Ant-v2 , policy_type: expert , returns: 200 / 4761.194459019096 / 450.18534292166703\n",
      "It took         602  seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0708 14:43:21.608249 47796 lazy_loader.py:50] \n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "W0708 14:43:21.610246 47796 deprecation.py:323] From <ipython-input-3-a36c30d12edb>:82: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "W0708 14:43:21.803728 47796 deprecation.py:323] From <ipython-input-3-a36c30d12edb>:83: dropout (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dropout instead.\n",
      "W0708 14:43:21.986242 47796 deprecation.py:323] From C:\\Works\\tensorflow\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "W0708 14:43:22.001199 47796 deprecation.py:323] From <ipython-input-3-a36c30d12edb>:276: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n",
      "W0708 14:43:22.187697 47796 deprecation.py:323] From C:\\Works\\tensorflow\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved dir: model_Ant-v2\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-0/kernel/Adadelta\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-0/bias/Adadelta\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-0/kernel/Adadelta_1\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-0/bias/Adadelta_1\n",
      "Ant-v2-dnn/layer-last/kernel\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-1/kernel/Adadelta_1\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-1/bias/Adadelta\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-1/kernel/Adadelta\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-1/bias/Adadelta_1\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-last/kernel/Adadelta_1\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-last/kernel/Adadelta\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-2/bias/Adadelta_1\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-2/bias/Adadelta\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-last/bias/Adadelta_1\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-2/kernel/Adadelta\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-last/bias/Adadelta\n",
      "Ant-v2-dnn/Ant-v2-dnn/layer-2/kernel/Adadelta_1\n",
      "Ant-v2-dnn/layer-0/bias\n",
      "Ant-v2-dnn/layer-0/kernel\n",
      "Ant-v2-dnn/layer-1/bias\n",
      "Ant-v2-dnn/layer-1/kernel\n",
      "Ant-v2-dnn/layer-2/bias\n",
      "Ant-v2-dnn/layer-2/kernel\n",
      "Ant-v2-dnn/layer-last/bias\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Works\\tensorflow\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "returns [868.4459212217599, 865.8122639924735, 872.5906929180148, 938.3335370510764, 922.2772937897998, 965.5925338448723, 977.8336673756318, 919.7090148592508, 1346.041814004916, 1042.6546851825262, 966.4015466167235, 847.9473299489222, 849.7664802099775, 834.637548853359, 860.4844250395989, 904.4353563271085, 1096.482185637914, 892.956889589427, 844.1757994117505, 821.0397564527692, 862.6652812197552, 938.8787964243793, 966.2053924403405, 926.4934601040493, 1113.4526985040643, 820.9250521758067, 888.2309688315237, 822.2244916417429, 993.072352174861, 900.7750037267194, 889.2232076347838, 915.7695994175803, 895.6418445351877, 946.7040074230665, 855.0622577357185, 898.3206485861014, 986.5610732961339, 895.3817429536167, 823.6333080118515, 1207.7520905430983, 903.3912974837722, 897.9565026859408, 871.5470552339805, 219.77591086387787, 955.024654479456, 942.3689031478702, 953.2978716117611, 874.9586600644553, 868.8100579782731, 944.8668664601805, 952.0264121990274, 886.9450622503929, 940.4819557839461, 987.3354187721199, 835.221119442425, 847.8199329905726, 979.8229254871377, 927.5037290870063, 868.1382527410458, 908.7772470768272, 942.8082216662998, 891.4232438754256, 863.7790034825151, 862.1648521564626, 837.5532883140467, 1191.871780868728, 968.7581756713198, 1020.0775127701403, 907.5606901172323, 1096.0970264128302, 279.0589925485996, 873.9182016469335, 196.78622743578646, 981.4950238280572, 865.3679877718473, 944.832109236869, 904.0453299039123, 863.4274711591825, 855.9102741014939, 909.2099950265623, 883.6893630437646, 944.8778887792466, 939.6117113518942, 972.8149187474588, 873.5365793714737, 989.8886973639536, 924.0644920337778, 874.2450237939869, 851.5586434284155, 879.2110292805068, 874.1086031032982, 1210.7673299656756, 996.4300959338857, 986.7732483525564, 961.3761472619942, 946.779061797286, 892.1728885883578, 931.3685968787657, 994.8951076817899, 1059.4957398742163, 911.1609562638457, 976.4658970556636, 860.0233871165576, 824.8570929014425, 870.8587926489148, 812.2457654714779, 1060.971494133371, 1123.458102189536, 892.2687970843449, 927.0559749144907, 841.4379255155272, 906.7109878095981, 903.839797642283, 955.404101839903, 931.7785393116902, 757.5399352295816, 962.7697667948992, 895.2187360171747, 923.6133986865387, 857.9778454297167, 849.1493019473907, 1020.1765001001105, 1027.863081081328, 872.1917069715579, 880.1988182790408, 1253.8682556985862, 895.3084922153796, 918.0086779209272, 900.9485137077512, 879.9571525059038, 833.3842257006061, 917.0089410956865, 1054.600092206138, 830.9751311865695, 909.2332500378812, 875.792720296501, 852.6007465858522, 1142.6550803799644, 903.346535665287, 238.79321693927713, 888.1176178101175, 873.6501340393021, 845.8516771000757, 917.6297460608105, 903.7169573285156, 1038.2420178735144, 918.2870162024963, 890.2630877918224, 886.945654616199, 1225.6067960769394, 878.3176947692273, 969.2981424128144, 1309.5005401503593, 876.2220768188802, 1256.0808368043881, 921.7556553212198, 917.5808365495882, 948.9696927179094, 866.8507273940985, 869.7334448040874, 914.4924695627951, 823.1518930600699, 1050.5672385445448, 947.9374750134588, 970.4597832031902, 888.1642335609628, 1009.3282872341941, 943.5369350931862, 1125.4017048993396, 994.5453347818288, 913.6691421942936, 966.649653478436, 936.9471703263736, 917.3102902390057, 916.7556648936206, 932.1016865499406, 954.8485324886542, 949.2709850155413, 845.7794840532241, 842.8880875299284, 878.2082416367576, 1273.2364419223243, 1027.4281128241853, 1214.5764077771762, 816.6624660261532, 918.3723671028309, 1045.015500283596, 877.9550760770429, 892.6450514381605, 865.8534738762863, 1007.2027196223025, 879.8775085536125, 955.5793452527804, 1093.1656415187365, 928.0355653485556, 976.4084978725687, 1048.250057164134, 832.7754048638128, 917.1291407238473, 1005.713285147715]\n",
      "mean return 923.7335343707534\n",
      "std of return 139.09964090308193\n",
      "Rollout result. env: Ant-v2 , policy_type: learned , returns: 200 / 923.7335343707534 / 139.09964090308193\n",
      "It took         722  seconds.\n",
      "loading and building expert policy\n",
      "obs (1, 11) (1, 11)\n",
      "loaded and built\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "returns [3773.2442409060145, 3779.0348207255793, 3779.5715278238335, 3776.0987343150155, 3776.9294791771963, 3787.0731840245085, 3778.8886288646513, 3781.925317450812, 3775.466980204554, 3777.685901597874, 3775.8738229268424, 3773.8499393924917, 3775.6122084295193, 3779.194393501249, 3780.9795226416372, 3778.948562462192, 3775.531394340711, 3780.234928529694, 3777.7778201612423, 3780.4207852558416, 3780.4405009056404, 3782.228612827278, 3787.166837000435, 3779.346822628416, 3781.3846438829546, 3779.2436257104405, 3776.4241367043087, 3771.68758438722, 3776.6941760322875, 3776.7049816148456, 3778.061520034053, 3775.0504083416577, 3785.126632972303, 3781.560077346019, 3783.271379084746, 3771.836830876246, 3777.8872802793367, 3778.2294772210475, 3780.655076599706, 3774.8189232038344, 3787.4095605195353, 3781.0171738138674, 3778.784451459949, 3781.2031174558665, 3769.15376705578, 3779.142517791088, 3778.7745640298904, 3779.298549896546, 3770.850531387069, 3776.4923086079248, 3782.223444928702, 3774.9184977108926, 3779.8291180204815, 3777.724447035113, 3774.6499608513086, 3777.130972180464, 3776.8958837544087, 3780.796833397426, 3777.3221553805174, 3778.7456681663975, 3774.3683032802537, 3777.369887740408, 3772.2919180574254, 3774.2219040775267, 3786.9990710867255, 3783.096440145899, 3776.218006017104, 3773.0320891418883, 3781.8759338743043, 3777.0288060674825, 3781.153409785617, 3773.027524815891, 3780.75986302188, 3780.010210838671, 3780.1521582073897, 3779.6854720473225, 3775.8538971415433, 3773.6813764071776, 3778.1752222722816, 3781.583415393307, 3774.203155628594, 3775.674175577703, 3776.3752738337685, 3776.1548663044005, 3780.6331412071227, 3774.0628772421846, 3777.825893885986, 3773.841054094457, 3778.252684719719, 3779.6112037743374, 3782.0670928052814, 3773.894275575157, 3786.358087430665, 3777.446168620729, 3779.8053138573327, 3781.7628029753073, 3779.3575993549293, 3772.2299507418516, 3770.9453980911844, 3776.126622268297, 3787.7989269158843, 3775.365283071549, 3776.852980277742, 3782.495194430234, 3775.421033242254, 3775.3146064529647, 3778.6303506426707, 3782.2401775043795, 3772.227696031219, 3777.9738694710754, 3771.241859685841, 3773.7248644627207, 3774.061221061219, 3773.5360107749752, 3780.664726457932, 3778.209788220511, 3776.215344147352, 3779.0631551027195, 3775.55736274382, 3778.9520937309712, 3772.012276333865, 3777.851363896354, 3777.1976120893487, 3779.269950305574, 3778.175209178331, 3770.772269148461, 3778.0982613126566, 3773.9901950081558, 3779.717499434528, 3777.0169471088007, 3788.183651583647, 3778.7379430102856, 3779.7289948887387, 3779.0251711009664, 3773.6789501505436, 3776.477003448362, 3780.0571140152874, 3785.7003646584067, 3785.2647770624935, 3771.24954402824, 3771.226625272124, 3778.2640814278934, 3772.9770494567438, 3780.6146933948517, 3772.5811192801284, 3776.5154808936873, 3777.050970251115, 3780.145764301914, 3786.8113492939233, 3775.5716433439793, 3774.954303898436, 3772.1311694410897, 3778.4388059214725, 3780.1587316109913, 3772.8860258354503, 3777.6262295847, 3775.996639361782, 3774.875071598932, 3779.0533091421, 3774.3020534076873, 3779.1823028777158, 3780.691888944041, 3776.528503634786, 3778.764064106586, 3772.473477138382, 3781.372357048221, 3775.6250705263287, 3772.8487360262425, 3783.1536478239464, 3776.4632458296214, 3784.2703900425004, 3776.3062257109714, 3777.03621889578, 3779.2307988566563, 3779.97091748467, 3782.887818540312, 3774.805080548377, 3774.685629770918, 3789.7848371528735, 3779.153991955062, 3780.876684726403, 3782.52447268821, 3775.0853222112605, 3777.709875419351, 3780.084124664322, 3778.463758865647, 3773.0250878451247, 3781.244747257107, 3775.0253236843887, 3773.620944096374, 3779.871269489451, 3782.8711237920297, 3777.3604291428637, 3776.8578549156473, 3775.694330047869, 3771.85378587801, 3774.6358542061776, 3783.9167931386637, 3784.696646055855, 3774.894859465386]\n",
      "mean return 3777.9360952032544\n",
      "std of return 3.8906076868751507\n",
      "Rollout result. env: Hopper-v2 , policy_type: expert , returns: 200 / 3777.9360952032544 / 3.8906076868751507\n",
      "It took         449  seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Works\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved dir: model_Hopper-v2\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-0/kernel/Adadelta\n",
      "Hopper-v2-dnn/layer-last/bias\n",
      "Hopper-v2-dnn/layer-0/bias\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-0/bias/Adadelta\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-0/kernel/Adadelta_1\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-0/bias/Adadelta_1\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-1/kernel/Adadelta_1\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-1/bias/Adadelta\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-1/kernel/Adadelta\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-1/bias/Adadelta_1\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-2/bias/Adadelta\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-2/bias/Adadelta_1\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-2/kernel/Adadelta\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-2/kernel/Adadelta_1\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-last/bias/Adadelta\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-last/bias/Adadelta_1\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-last/kernel/Adadelta\n",
      "Hopper-v2-dnn/Hopper-v2-dnn/layer-last/kernel/Adadelta_1\n",
      "Hopper-v2-dnn/layer-last/kernel\n",
      "Hopper-v2-dnn/layer-0/kernel\n",
      "Hopper-v2-dnn/layer-1/bias\n",
      "Hopper-v2-dnn/layer-1/kernel\n",
      "Hopper-v2-dnn/layer-2/bias\n",
      "Hopper-v2-dnn/layer-2/kernel\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "returns [143.1929757742704, 146.16768898901708, 145.61656252350102, 152.09172740843022, 144.9108489142051, 143.5678306065478, 154.0400615452578, 151.58555347366917, 143.2879353713531, 147.1454138670355, 154.0364341430064, 148.9963695665236, 153.653987968143, 147.7872216846146, 147.20861103611125, 148.75679411978703, 150.79703520155263, 146.6650716106547, 152.61910972109368, 150.8098379948735, 149.66789561237238, 146.3002470341622, 147.8729369727004, 145.13297248220232, 146.76603135816273, 147.09532531314463, 147.45571931979254, 145.5925073113263, 147.17605925977603, 152.44946894851745, 147.85979880807002, 144.79462584713045, 147.21831982559578, 144.81231107857, 152.0453119829276, 144.2846720335383, 147.98343136676462, 151.1486662699224, 148.67898514671919, 148.09558555806825, 148.8311332775076, 148.1245207566565, 148.06622368498148, 151.4207342590318, 145.21932623134003, 145.89842559192232, 146.7356277528505, 149.18597060835063, 145.75358330807205, 150.45436980307284, 144.76988587257625, 145.16358910341555, 150.94645307182668, 150.43604642770433, 146.07935180303517, 148.98732848190224, 148.0713750374852, 147.7899429420908, 148.60144685135907, 145.68652324206195, 154.53403692955126, 149.03737459286037, 145.02600687350815, 145.30569680827801, 150.2844626619328, 147.6184472765954, 146.66316324397886, 150.36104583891768, 145.98012944683185, 148.37615620663956, 145.76051249238287, 145.42015354364523, 151.59960982028377, 145.61617735616045, 149.19438134808826, 147.1514738234767, 147.66764014455197, 155.56110493835322, 151.27690448727063, 145.6190341155787, 144.2532754930421, 146.09872381812073, 151.99455896365583, 151.07254143791502, 150.0571716863625, 152.6176734098187, 150.27812116993638, 145.05159085392265, 147.43122674782617, 147.69926500490865, 147.55503840334697, 145.47283573160286, 144.89759019501705, 144.8976496178358, 153.1943027156908, 151.91212944418191, 147.55502935325072, 147.66071420546734, 145.85321774082894, 151.09564316676736, 147.84556582273044, 148.7436798055791, 150.53646105253245, 149.50221265756238, 150.95051832308096, 153.28713990204287, 160.19262702826435, 146.1676683688342, 147.0251870871067, 146.065400257367, 147.79211002282815, 149.2951432467285, 147.01822980067024, 146.80601709973118, 150.46326457215562, 152.19903178935206, 149.5205761061956, 144.01082599417316, 153.26559928316985, 148.0480473951078, 150.54097403620312, 143.67955516083506, 150.47011967673072, 147.10603285352138, 153.70214029039218, 146.4756448756452, 152.33431948500652, 143.68177143148722, 149.80040734423676, 152.7445937538081, 150.02100392202908, 147.83608531371814, 147.42442490449918, 146.3238057149953, 145.6976285463482, 146.24764408578187, 144.935450653946, 151.33572159656418, 148.94116692047854, 148.54518517738785, 146.74013417430015, 149.27322900515267, 145.24519835848614, 149.54119254609455, 151.24391508526338, 152.66673501860265, 149.89265923667998, 150.0050360208684, 151.21939379971096, 151.09149271701696, 149.13410114918202, 145.3770083483811, 147.30852563137128, 149.77621305746092, 145.8489993329782, 152.30366603629054, 143.78108596208315, 149.7710884198963, 148.6879664331138, 145.04652188643533, 151.27830899657613, 145.35808878110282, 151.6760250155179, 145.8197035745246, 149.54023083586628, 149.82712308936962, 145.09432919258225, 149.00067769304533, 145.17320177109053, 148.06774001508487, 145.53235527955215, 152.63010780809316, 153.6517415505393, 145.03726017655057, 145.21686871917717, 150.95466439028317, 145.73820931112434, 147.16143579855367, 148.7762384570439, 148.447274079221, 146.42013422414863, 148.14596611406802, 144.4007725066915, 152.11882318522518, 145.06459543524804, 145.9423473386528, 149.10224251175785, 145.74745087691213, 147.17298562056325, 147.1350541512386, 146.87421117379503, 149.32569499511908, 151.09292134955334, 157.41044571260224, 146.48303644691399, 144.66570458462184, 151.3615278678529, 147.08130608606223, 156.07293240354596, 144.53831999186778]\n",
      "mean return 148.38481406477138\n",
      "std of return 2.9618506871957377\n",
      "Rollout result. env: Hopper-v2 , policy_type: learned , returns: 200 / 148.38481406477138 / 2.9618506871957377\n",
      "It took          44  seconds.\n",
      "loading and building expert policy\n",
      "obs (1, 11) (1, 11)\n",
      "loaded and built\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "returns [-3.7334123253369866, -5.4044968991802955, -4.666008648560787, -5.293192805105677, -7.339290366655374, -3.379769928466454, -3.5578097068765597, -2.77425695174863, -1.2192225392096092, -3.374716607511449, -3.735896319173545, -5.982902188611314, -2.5191825603717213, -3.0019207024615837, -4.055516883832692, -4.224013420676269, -2.125854286461759, -6.733530057021857, -6.310841959475731, -3.242027848632871, -4.959086556325613, -4.350337870088524, -4.653287101807329, -2.1009779530324444, -6.08982341786836, -2.737780764936489, -5.216091596885945, -2.1582261139173213, -1.1748429379450454, -2.2977288602065458, -7.546350790111336, -5.30117097970518, -4.613673818137531, -4.618103979423128, -5.430181846808707, -2.82209945735594, -5.13524149168645, -5.008180217260702, -5.371851767623658, -0.805398294125213, -2.394174258676563, -3.719782280544278, -2.2748424046999753, -1.5140797782195936, -7.230997252472973, -4.764920500490599, -3.9988690397179636, -0.9522232507509503, -5.0218690722182675, -3.209868234018639, -5.5219577219846645, -5.5956764885928, -5.2308342029006, -6.559933570239362, -6.327444144150616, -3.003886723077844, -5.713706789942796, -0.9425041847630097, -2.0577828141255896, -1.590838186993183, -4.294033100420873, -3.233986078996396, -4.255965103270365, -3.672074405386552, -0.5927944295873765, -3.109444638016573, -4.604655513452479, -6.148335465812773, -4.528575571191375, -1.0911555734484506, -2.3946171994488283, -7.0577944222309075, -3.1508540975079153, -3.7301466715213754, -4.190418939609581, -3.851052536413929, -2.6593416831275625, -4.079329027392021, -4.0976229726183035, -2.3837857034980545, -1.875232897966294, -3.404260841856444, -3.0268807857169797, -6.150079821638555, -2.6498660843444233, -5.033193846158402, -5.542780157326635, -2.2252245010369394, -3.409474603519188, -5.165724055247025, -5.14696703363881, -1.1925161150709551, -0.9943501225534136, -3.9209100641404793, -2.6575703576900316, -2.9897738371395848, -4.364001879446423, -4.267736586698207, -3.260968933883693, -5.511776064325261, -2.1007545663911946, -4.851178235482031, -3.027005363020858, -3.9977589955471036, -2.701533469588478, -4.915820243059514, -4.985721614591123, -3.566031608930571, -7.038477832769327, -3.3372933721328826, -4.1797747877656395, -3.2338086217773543, -2.2426508460997834, -1.4429601777713301, -7.591539961963042, -5.717669502793441, -4.808429931381272, -3.2456482790482357, -2.4538617608288837, -6.331578699316012, -4.652970907617747, -1.0021487455141078, -3.1317454155147804, -3.6317586552716388, -2.324524656847404, -6.9045783169826285, -5.148273918060398, -3.0313719418707885, -7.148536118946256, -3.247366978248105, -1.8687080260553737, -5.383862898741796, -1.7057073185424412, -1.1655324733560564, -3.2065217645478423, -6.939842005876622, -3.4127872804863797, -5.421012129002888, -2.720760884886984, -1.6076771627201363, -3.0279789912880206, -6.705808841941396, -5.097506137118318, -6.506157812986408, -3.655181042830142, -0.6966359773817931, -3.58370992200754, -4.498091289037852, -4.813646869325797, -6.294158388469482, -6.609929895757481, -2.77501656340588, -7.683722707118643, -2.932360502105332, -5.695715528449303, -4.695591277005307, -1.5668236012687298, -3.9620661498283387, -5.179096980189598, -1.8123270797554158, -3.63103039261192, -4.52209086178283, -4.029472916264461, -4.700687055859758, -1.44235214443433, -7.233905288591196, -5.735422720370329, -4.93697636814706, -5.000697029542412, -4.18218808343067, -3.269677238137354, -4.8145684600716905, -2.2993806622852357, -6.10667394294982, -3.0036763384450285, -3.1172335967546125, -2.2696450966596404, -4.948908005376178, -4.896539386225533, -1.4649250465097121, -6.64797230325318, -4.77103490355954, -6.491423100376384, -2.017211295784194, -4.950685278514211, -3.866477942360789, -2.981653356810389, -6.893075538850432, -2.739016126220547, -2.576189723522186, -5.4541911827971115, -7.03397527719228, -2.136492031903136, -3.6187359448825505, -5.733012561058903, -6.467858199080106, -4.216005083961653, -4.028371003934195, -8.5339713365832, -3.303609888730261]\n",
      "mean return -4.039447425909983\n",
      "std of return 1.7209107373274808\n",
      "Rollout result. env: Reacher-v2 , policy_type: expert , returns: 200 / -4.039447425909983 / 1.7209107373274808\n",
      "It took          18  seconds.\n",
      "saved dir: model_Reacher-v2\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-0/bias/Adadelta_1\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-0/bias/Adadelta\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-0/kernel/Adadelta_1\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-0/kernel/Adadelta\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-1/bias/Adadelta_1\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-1/kernel/Adadelta_1\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-1/bias/Adadelta\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-1/kernel/Adadelta\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-last/kernel/Adadelta\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-2/bias/Adadelta\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-last/kernel/Adadelta_1\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-2/bias/Adadelta_1\n",
      "Reacher-v2-dnn/layer-last/bias\n",
      "Reacher-v2-dnn/layer-0/bias\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-last/bias/Adadelta\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-2/kernel/Adadelta\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-last/bias/Adadelta_1\n",
      "Reacher-v2-dnn/Reacher-v2-dnn/layer-2/kernel/Adadelta_1\n",
      "Reacher-v2-dnn/layer-last/kernel\n",
      "Reacher-v2-dnn/layer-0/kernel\n",
      "Reacher-v2-dnn/layer-1/bias\n",
      "Reacher-v2-dnn/layer-1/kernel\n",
      "Reacher-v2-dnn/layer-2/bias\n",
      "Reacher-v2-dnn/layer-2/kernel\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "returns [-17.092156340924465, -12.344565047251777, -4.723441246753633, -8.12838184876937, -14.264877483080603, -12.323035335985093, -9.310883668658702, -8.558053319164644, -12.622212527051612, -15.021750863513994, -16.841162393195265, -13.856934083596439, -19.50203742312461, -9.055782404284136, -17.0683597501203, -18.30147090466369, -12.864570041548006, -8.64586168876771, -12.424228546300222, -14.65544328130697, -16.13318646675233, -8.365582425805314, -15.203806077692388, -12.27741111740203, -16.889767477696708, -18.0199857738827, -12.852285317661511, -16.046738215752036, -3.131517320301272, -19.729994115743104, -3.5530909979086416, -4.587626891526157, -5.6715628568845, -11.871520409391868, -9.596928236350223, -16.473496539196464, -15.241833100257718, -17.32260179811375, -8.621176153946713, -14.308645560002565, -15.50015164877977, -16.755242825335557, -13.707426297716957, -8.599846502079748, -7.6196571370155235, -11.040714531058773, -15.174819366762346, -13.842744905029175, -14.793674615986744, -5.1124992680905565, -16.163905566946564, -16.215892253725457, -11.412824563492213, -9.248691989887496, -10.11016282163714, -2.578327605167382, -11.647291927363039, -19.12764689443027, -17.918144121430576, -13.367087098658356, -17.210510794554043, -14.152771698100787, -14.385354319371487, -15.266643875343961, -10.805853955546029, -17.096494480870774, -9.905672582601566, -10.408642848946151, -16.287187547769637, -14.116047738861687, -6.832839315914362, -9.999597027423276, -16.14916038434651, -17.398948575690472, -12.816805941918956, -3.457178655155037, -16.441726552767854, -9.952345892134439, -9.100876844500467, -15.025341743865898, -14.785977058679975, -16.089917883688425, -9.86702043113423, -9.173902367864766, -9.712061550782327, -5.957871727651248, -10.010674896059845, -8.022403627656816, -12.776599757543444, -15.118230536752518, -18.699434883864896, -16.3071717274791, -8.148001029962574, -6.660022547813774, -12.099622257676625, -8.338852131823266, -15.01994368203486, -11.027288882747087, -15.317256756779448, -5.7336308132463065, -17.07481836841013, -16.18100332922987, -16.087021802650863, -15.000089846694884, -13.519512952600621, -15.703220911440946, -14.300705034580423, -16.61391044540394, -17.877809968532098, -17.537003682346004, -17.09055284189784, -10.641977227389225, -18.332328197648664, -3.5145028429164658, -12.801458473152287, -16.029072425893435, -14.826457195107428, -15.83850470640238, -9.531823463238828, -14.178409629578539, -12.22992534282579, -14.608815925437607, -16.04722294354692, -15.500340279014466, -8.337537031960352, -4.6702900426459, -10.192894787856233, -14.155793464554483, -19.434918386640962, -13.879716857083265, -13.423990822621283, -14.771948543170373, -10.974688213965138, -17.290120324514177, -12.946266655428252, -10.67775389654959, -5.0688128016230705, -9.038816687816167, -5.205309616849577, -13.868481359850742, -14.608035152714137, -12.64422055579538, -17.186747889010608, -14.35971542156911, -8.334960452610472, -6.27471810801576, -13.205251711876805, -11.181923604774678, -5.949831030471259, -10.907973806237731, -12.840764544961706, -14.460771042374157, -6.350586383404171, -15.659668323089315, -13.412938586272627, -5.700469021223667, -14.45249474986757, -11.88455889307515, -10.489675167372058, -13.182227240307242, -4.867534814329952, -19.62282682131455, -19.193520134713836, -18.682478721598414, -17.6196393134065, -11.44505673788648, -7.993072807131707, -11.279495727200974, -14.48815869196045, -16.516893416661787, -12.030647978290867, -9.110862574689694, -17.329411436932517, -4.879950676613095, -13.671981361784578, -20.02879907139506, -18.7220253764259, -15.008700860244565, -14.837697903710115, -15.93059909471256, -17.401146050376713, -11.15617215547447, -9.152591296903562, -10.320294700659572, -11.096809667713126, -7.67524126029292, -15.912380827208077, -4.770605606472876, -5.536355571241857, -18.180985951627253, -17.907791315817963, -7.353613691252308, -7.5454943168548505, -5.7508481499344475, -17.60641907961565, -15.68784193278623, -2.6511213095077535, -16.5755809297263, -9.55838644674315, -15.11894846711578]\n",
      "mean return -12.587129717901343\n",
      "std of return 4.246398209350347\n",
      "Rollout result. env: Reacher-v2 , policy_type: learned , returns: 200 / -12.587129717901343 / 4.246398209350347\n",
      "It took          24  seconds.\n",
      "loading and building expert policy\n",
      "obs (1, 17) (1, 17)\n",
      "loaded and built\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "returns [4104.409217362766, 4164.154443931926, 4115.904034629651, 4158.728316977831, 4181.2505962836285, 4167.14787141063, 4132.315169763377, 4235.638218465252, 4073.2196699658543, 4186.514315150221, 4231.761530740154, 4286.67988451333, 4194.777305666385, 4172.397089788362, 4133.208024265507, 4196.6214666074475, 3953.0108972376293, 4107.477782185298, 4222.886972106273, 4218.108044242748, 4124.81840988836, 4201.331085614768, 4147.467936947366, 4034.299702120793, 4088.330071259702, 4146.023315866235, 4164.2262733633415, 4337.271058950631, 4187.544466044964, 4041.0571145981967, 4046.6288921879186, 4066.1525416452587, 4195.069085714137, 4107.239165326436, 4084.066809878785, 4138.0482898313085, 4150.012210877406, 4174.1427857734325, 4040.1912558906047, 4092.478274663246, 4210.167647548213, 4116.285164655482, 4008.1863614830368, 4141.239620443369, 4122.256894452189, 4080.680485051822, 4087.832840978241, 4057.719057984378, 4168.574352165634, 4066.7985670457806, 4139.600478334809, 4029.613472825776, 4176.969408520754, 4231.299885911367, 4180.6805424280565, 3993.3629029837025, 4140.7896681849825, 4094.4765868488494, 4075.7120598178194, 4151.242926987976, 4234.444801710753, 4137.65890597706, 4190.107873144629, 4329.717381059465, 4123.891195644904, 4129.571702396634, 4233.187259256122, 4240.3924094104495, 3993.267158014324, 4073.6309937909223, 4068.338927199565, 4099.639516721441, 4149.978251544959, 4103.866728888124, 4170.352828827199, 4127.625457134353, 4091.785177967419, 3993.0607840772773, 4170.082023284251, 3987.2023952133695, 4085.92158913675, 4104.501673735475, 4108.475350676546, 4212.428725259122, 4044.225887118043, 4128.523707187959, 4060.5583335809056, 4121.272108845681, 4032.0185598876924, 4054.0079562896467, 4176.758905939964, 4136.006485857249, 4117.259915231212, 4252.573408825492, 3952.7276923762965, 4098.647542150683, 4148.025757215465, 4089.817824042087, 4104.421580962449, 4224.986680092991, 4110.3050175564995, 4349.241536536694, 4307.4506180885655, 4176.076298996981, 4205.7216041078955, 4126.078391159257, 4160.481047800966, 4141.022420502295, 4060.6484454067913, 4058.7945869977357, 4180.580231726576, 4075.3620422758518, 4040.1364843798633, 4202.392702554769, 4223.976941877922, 4219.889660458286, 4303.442029823659, 4178.429944122201, 4134.206076606283, 4127.756501833804, 4246.553469885772, 4108.641796067995, 4187.086797660422, 4217.716442143805, 4284.693435863712, 4079.5842570488153, 4158.334449952131, 4172.012371389251, 4171.394520672007, 4128.390138781266, 4094.1301891866337, 4214.15248554961, 4330.362078843262, 4157.939788332971, 4121.033879538526, 4145.182841548464, 4112.164230512767, 4249.558361441148, 4131.877134937862, 4149.326933499149, 4067.802195806803, 4084.0052840410544, 4259.126543471086, 4109.002272151837, 4206.137078771327, 4132.874466856462, 4072.298084170922, 4027.189234522275, 4123.953232938537, 4126.961260826647, 4137.085639903696, 4050.833327972411, 3958.795129818211, 4137.329348254738, 4137.796252658649, 4195.862502758612, 4121.75471868621, 4108.10385367526, 4090.560693146353, 4125.069470449534, 4057.86509728824, 4105.691587105207, 4057.7155910244155, 4045.879205322773, 4195.209009514602, 4142.071779202859, 4087.7435462803196, 4047.2811309438976, 4073.3287951935026, 4105.004923557162, 4153.225666730679, 4068.9932476503072, 4000.04763234528, 4200.8775080526, 4047.1933094410388, 4036.0926378706895, 4137.889516571927, 4126.733584531208, 4180.576087166611, 4212.767252913394, 4029.3447814354545, 4080.7270608228323, 4251.181459613034, 4356.961996116915, 4072.685966614131, 4155.107026876671, 4154.440859356026, 4144.987046993051, 3989.52483277925, 4075.648508243412, 4132.430625421083, 4044.9655538824463, 4118.526923167101, 4219.625710802293, 4282.400541765605, 4252.774601496167, 4010.1480427117135, 4199.54847351018, 4194.132218502178, 4011.775754921658]\n",
      "mean return 4134.754269241757\n",
      "std of return 77.58885285699668\n",
      "Rollout result. env: HalfCheetah-v2 , policy_type: expert , returns: 200 / 4134.754269241757 / 77.58885285699668\n",
      "It took         395  seconds.\n",
      "saved dir: model_HalfCheetah-v2\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-0/kernel/Adadelta\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-0/bias/Adadelta\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-0/kernel/Adadelta_1\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-0/bias/Adadelta_1\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-1/kernel/Adadelta_1\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-1/bias/Adadelta\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-1/kernel/Adadelta\n",
      "HalfCheetah-v2-dnn/layer-2/kernel\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-1/bias/Adadelta_1\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-last/kernel/Adadelta_1\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-last/kernel/Adadelta\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-2/bias/Adadelta_1\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-2/bias/Adadelta\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-last/bias/Adadelta_1\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-2/kernel/Adadelta\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-last/bias/Adadelta\n",
      "HalfCheetah-v2-dnn/HalfCheetah-v2-dnn/layer-2/kernel/Adadelta_1\n",
      "HalfCheetah-v2-dnn/layer-0/bias\n",
      "HalfCheetah-v2-dnn/layer-0/kernel\n",
      "HalfCheetah-v2-dnn/layer-1/bias\n",
      "HalfCheetah-v2-dnn/layer-1/kernel\n",
      "HalfCheetah-v2-dnn/layer-last/kernel\n",
      "HalfCheetah-v2-dnn/layer-2/bias\n",
      "HalfCheetah-v2-dnn/layer-last/bias\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "returns [2351.9872247154367, 2110.8253158238376, 2230.469379268302, 2475.822213363944, 2246.282969487143, 2350.9238289678765, 2678.611791514101, 2354.6381313423817, 2132.0227103086027, 2187.584077750444, 2321.4795878155337, 2254.2262103532858, 2588.84789817354, 2194.8278368513957, 2403.5426811990583, 1991.8518796505095, 2127.7026987912814, 2281.19441029907, 2354.0788816577738, 2265.226722716552, 2236.3943190092386, 2459.7713438848955, 2333.384813893446, 2488.9665340970164, 2117.1773048499144, 2262.967262313116, 2220.003514850541, 2288.0127504507286, 2287.332527194288, 2652.159772785016, 2258.1648390733462, 2179.0907296033542, 2242.4223854000093, 2211.4916085070136, 2360.9317021184384, 1922.7847336157972, 2143.25830770824, 2024.2589056303061, 2161.910873492032, 2334.903616426608, 2102.211630187569, 2091.0721843614265, 2135.721929298174, 2131.8110655566597, 2360.0585999754385, 2126.2401931923987, 2452.1085615650063, 2331.7983848110625, 2039.6790567955516, 2380.742068519622, 2093.4027625702824, 2094.9323592147357, 2101.5858467134067, 2165.130396473703, 2414.661205193757, 2666.8500226640476, 2453.851688548548, 2091.4228358849773, 2192.5614106425674, 2302.8238361417384, 2454.2444349095667, 2082.820746563409, 2391.7728768746592, 2240.197268590316, 2345.267590663983, 2216.4386752262103, 2402.592546952139, 2628.397519226005, 2135.3131022773746, 2530.1991225985557, 2549.909300511626, 2524.1550161461655, 2280.871248082655, 2188.04177184544, 2354.1320676135997, 1864.9756379774747, 2341.4574945634467, 2108.758349467602, 2069.489674925414, 2113.753156879296, 2174.0753625800908, 2386.614997468436, 2470.2126106452997, 2401.828408252299, 2440.9806469129794, 1729.772868617586, 2272.336092462404, 2155.0295616824983, 2359.7674133048777, 2416.6776068679223, 2177.3274043594474, 2342.570100870139, 2068.0211156727187, 2360.7476271405203, 2084.8076598660627, 2123.61999173052, 2361.8556294637683, 2010.9111071484376, 2391.6308254817554, 2452.0190197002685, 2549.364047040942, 2274.51561346112, 2207.354681843192, 2296.50905834917, 1894.6370023469374, 2466.66371607216, 2139.7904956639686, 2279.9756125611793, 2434.591102883431, 2427.3605293333603, 1898.7043727592138, 1969.9270563255996, 2356.5599073467297, 2188.430983432769, 2381.699573370844, 2145.6746989402923, 2276.986221182607, 2159.2941258713745, 2760.8348357618415, 2254.4685190680448, 2326.66599911762, 2408.299276478538, 2335.4078734125796, 1917.4556377427891, 2490.3451800995754, 1840.4924289721323, 2123.7475118418856, 2363.180395645942, 2074.1917491160307, 2378.924403464087, 2213.423157990911, 2094.236285707236, 2230.1071509898393, 2180.9116494009018, 1984.5909489038904, 2091.5659504867613, 2138.117698229725, 2374.005917900846, 2307.601639178442, 2206.8421745878704, 2341.593726086278, 1901.8125102955335, 2056.606946639635, 2333.6849819842437, 2377.667574949876, 2349.141607804235, 2272.5380035493667, 2127.4126159399193, 2126.5957489680354, 2108.894578548174, 2172.7750491827496, 2153.5882107735015, 2264.979455474374, 1988.5038054296824, 2036.6875507111824, 2256.462742177649, 2324.7477725673225, 2301.991674886525, 2354.2342667527446, 2223.8104719177577, 2314.0127144461158, 2320.516406404414, 2289.015219706808, 2102.597147291549, 2437.9168278231546, 2485.924425234507, 2319.2137617981507, 2418.128136230854, 2305.265629268878, 2146.5055032374858, 2330.627962346909, 2282.675842203334, 2145.3520323927874, 2413.678877068757, 2301.6245967980904, 2119.353365829733, 2108.5161726712395, 2404.759856800265, 2118.205920014801, 2082.740599663735, 2328.6183551467116, 2435.62005889406, 2195.388885001855, 2461.3126363538704, 2219.4006315154506, 2271.5178574583447, 2094.3324610143263, 2230.257948918639, 2304.0580352466895, 2309.8406239649707, 2180.8544492542446, 2192.3074616442177, 2557.6501669312474, 2261.842646952488, 2311.743952759424, 2250.950035833546, 2253.5453524371974, 2438.3077424374524, 2091.845991408836, 1959.6782871994683]\n",
      "mean return 2255.663415202564\n",
      "std of return 167.54703818054736\n",
      "Rollout result. env: HalfCheetah-v2 , policy_type: learned , returns: 200 / 2255.663415202564 / 167.54703818054736\n",
      "It took         429  seconds.\n",
      "loading and building expert policy\n",
      "obs (1, 376) (1, 376)\n",
      "loaded and built\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "returns [10420.968044204772, 10481.737062460932, 10327.22651944299, 10304.073208577469, 10384.244447536028, 10387.809396230852, 10446.089796607403, 10500.642776626608, 10474.95306938372, 10406.011000132296, 10289.405001340354, 10403.598076767585, 10369.943842199782, 10330.348640209748, 10376.52537563942, 10424.128547261604, 10339.35555646585, 10422.54527157088, 10366.221754132686, 10493.407757696092, 10359.79970136197, 10386.64586629807, 10343.837056841392, 10380.833365602968, 10356.082068090916, 10369.42442870214, 10450.391226704429, 10450.772612441435, 10440.822395541, 10436.517516052309, 10435.446554411626, 10510.516051077817, 10406.940834925012, 10480.638307750898, 10508.52204744391, 10380.691858513019, 10449.583068282063, 10188.478967299447, 10416.482492793317, 10410.666740557663, 10420.245133321794, 10454.554262414475, 10418.96906087316, 10470.0156148081, 10387.602932633188, 10385.078439261968, 10527.428950257197, 10372.525828995547, 10371.143997912626, 10389.035826309851, 10354.39897010716, 10417.651362650298, 10340.751341247478, 10533.969801216877, 10325.22377966698, 10329.748301546364, 10362.212168307706, 10338.121457103734, 10359.973505811686, 10373.049264659743, 10391.26986004454, 10503.162603656465, 10460.935117505436, 10395.440664010206, 10384.999151905587, 10189.382129831307, 10447.309794101082, 10427.454300676263, 10323.422956125056, 10439.181057209717, 10429.691444183827, 10466.707537164784, 10417.894019898638, 10400.610533894138, 10426.490421457875, 10442.288879462212, 10421.79285564935, 10462.610911363452, 10364.975011262073, 10445.310446711812, 10367.729969911406, 10377.261909071423, 10386.947885867263, 10454.300898929381, 10397.70873478671, 10448.918803526974, 10451.062057056326, 10454.689759708861, 10449.235954394126, 10426.141172611802, 10468.214081299184, 10394.73676927406, 10375.992952116052, 10407.163992030899, 10466.202333734253, 10299.090916624546, 10430.903836506644, 10325.742638093905, 10482.170307780612, 10372.473863138652, 10509.784691101508, 10406.018486866642, 10341.617238532252, 10417.882267729028, 10365.040592399657, 10489.937641744113, 10375.67725100206, 10436.949715779314, 10359.81598325239, 10435.102545573141, 10490.047238612837, 10426.464873682537, 10351.979741193605, 10399.573625854078, 10322.344017571146, 10418.211331103228, 10298.988053921985, 10388.305203821994, 10421.810323913018, 10402.50901225166, 10482.592399958497, 10210.420643028765, 10449.616071200451, 10402.572704289498, 10331.347460161423, 10407.49560570611, 10505.062311327889, 10451.249869217005, 10513.963880321116, 10485.89525267812, 10485.24399650277, 10341.949097929999, 10419.889624261727, 10357.00971241567, 10423.508929313632, 10407.463376406457, 10407.309638606492, 10426.852513079339, 10317.379326843145, 10351.817397692656, 10385.560229128996, 10329.914977566053, 10366.153704185786, 10465.962168362506, 10374.81234475882, 10397.11794186946, 10316.447300665366, 10336.54039336664, 10390.490927512768, 10442.658126067196, 10436.51931388892, 10434.294640610875, 10376.179535445342, 10216.56576143728, 10366.298021407496, 10407.805881885804, 10335.784465559944, 10416.857947229366, 10420.310501051024, 10531.115312031918, 10355.253782544914, 10384.651046473942, 10326.244903904695, 10418.302909216172, 10435.564834946184, 10328.60000090138, 10387.14469302821, 10446.969178981764, 10417.385722641868, 10384.020426396752, 507.1839552701626, 10472.938590595719, 10357.58361543467, 10289.370134760044, 10350.68336282059, 10439.075052381262, 10399.755865585432, 10417.481042589547, 10247.790592442054, 10450.787573196767, 10351.08656494123, 10392.671081580675, 10393.443789036422, 10488.04865257963, 10429.121240415008, 10438.7872343295, 10308.810164710992, 10412.011470025845, 10342.991181641322, 10431.104497578652, 10379.248399880451, 10383.857290260376, 10424.114039631426, 10482.969674462984, 10350.89326842855, 10446.815491026135, 10388.717896364484, 10407.837585502295, 10325.407552431236, 10375.92467042829]\n",
      "mean return 10351.381925035537\n",
      "std of return 700.46005897692\n",
      "Rollout result. env: Humanoid-v2 , policy_type: expert , returns: 200 / 10351.381925035537 / 700.46005897692\n",
      "It took         624  seconds.\n",
      "saved dir: model_Humanoid-v2\n",
      "Humanoid-v2-dnn/layer-1/kernel\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-0/kernel/Adadelta\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-0/bias/Adadelta\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-0/kernel/Adadelta_1\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-0/bias/Adadelta_1\n",
      "Humanoid-v2-dnn/layer-2/kernel\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-1/kernel/Adadelta_1\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-1/bias/Adadelta\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-1/kernel/Adadelta\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-1/bias/Adadelta_1\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-2/bias/Adadelta\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-2/bias/Adadelta_1\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-2/kernel/Adadelta\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-2/kernel/Adadelta_1\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-last/bias/Adadelta\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-last/bias/Adadelta_1\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-last/kernel/Adadelta\n",
      "Humanoid-v2-dnn/Humanoid-v2-dnn/layer-last/kernel/Adadelta_1\n",
      "Humanoid-v2-dnn/layer-0/kernel\n",
      "Humanoid-v2-dnn/layer-0/bias\n",
      "Humanoid-v2-dnn/layer-1/bias\n",
      "Humanoid-v2-dnn/layer-2/bias\n",
      "Humanoid-v2-dnn/layer-last/bias\n",
      "Humanoid-v2-dnn/layer-last/kernel\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "returns [444.06203176827194, 374.6386625374141, 497.61913311225595, 349.57999746887646, 398.7675531203295, 466.01322569219707, 514.9173172890212, 395.33861305367316, 376.77906757305493, 415.14494294536, 479.6683799618458, 502.65526650570234, 483.459728872142, 542.6449667241752, 476.10977863961443, 406.09258190877426, 435.8668490788053, 389.5987258301197, 409.50543236373693, 355.5958355256951, 409.3785028754773, 377.3064480977703, 403.8886693783821, 410.88758440150616, 449.9201998146511, 415.56441851150385, 428.19067177443054, 404.70985310657494, 368.07585045752177, 466.57576156849075, 448.8886549128425, 462.4847671234158, 362.38430729583905, 416.9061269489224, 446.9910666006331, 380.86678148784404, 481.384327254473, 394.9928831390837, 421.2760135157685, 408.28687775500885, 365.0714995925788, 510.01847389870613, 495.1954587631431, 413.255807132115, 464.70885678520705, 358.9033825703537, 486.35219614557184, 437.27375983431534, 607.3261998807606, 476.35479219458176, 410.1997514927301, 485.0556850201191, 499.6092781949873, 368.82623347918883, 534.4093037086849, 469.6540112689204, 420.80455899039737, 378.24129379382816, 364.04869872610425, 434.7760931356931, 593.1059402518736, 400.69742250509734, 401.4879733251393, 535.0469573945561, 473.63513738026217, 423.0091237158655, 390.97968086722176, 426.9385828591105, 390.8992008849656, 425.63435756652797, 440.95032646737553, 490.609954962575, 445.49175209718067, 436.56512427700966, 469.1765748180191, 358.60251991530635, 549.3387845829726, 483.07364484241697, 372.21537315986956, 492.7255182245167, 386.6689652980171, 450.3311516584313, 419.5148630995347, 537.5332408653808, 426.59536354608946, 466.44440149212744, 364.1304709530936, 513.5524431570357, 452.28428371354295, 497.79217546055617, 379.25438092121027, 400.4498745666092, 526.0188172654273, 400.24047962025173, 465.2974424661135, 488.7989993410655, 436.01005058343213, 585.5192298912798, 380.664646820151, 362.9071971262379, 401.9731430072311, 353.79539613598007, 396.0346593310286, 484.0380259981966, 514.1395309263197, 365.5132844356968, 423.4822552044811, 416.2025685702564, 526.0039957631676, 437.6007058681934, 493.63288991107447, 448.0410801210041, 412.0433683401196, 388.46142627241574, 437.8129118735747, 512.8996544678888, 373.7192667627823, 436.29239820315286, 527.8445773855755, 426.34386767029406, 440.6077625948113, 370.8774302478898, 402.458596487431, 523.1443910075908, 413.09033910243005, 451.9169017614394, 501.5845040189371, 377.8808796374267, 354.9124436212306, 394.168911851022, 416.0462773682015, 473.7554255431829, 456.526870334476, 389.9619478019714, 395.31051493868443, 408.93554055375876, 429.36953201253, 439.7814518358609, 365.919207690097, 412.84204161112666, 495.9977033435512, 447.876967786485, 448.7332504836594, 426.12849037025813, 448.22125162832276, 474.69198091705186, 364.3551482585805, 492.5112943058764, 447.4182261475217, 501.16628387764337, 405.46549149418433, 459.9088452334519, 458.4962559402786, 445.0256589796139, 449.1608938995006, 430.3787887613069, 367.20420610733754, 436.83226618378416, 351.25298476262867, 517.478312049033, 490.7901260988086, 533.0124787568936, 416.85182332682285, 453.5407585628833, 404.7956156354994, 417.5607429827029, 397.34514173937873, 509.0033319321157, 400.16180659790615, 500.8776607031121, 369.06977117334327, 445.7239870755308, 378.2625457970913, 394.5925863364705, 424.97467372116114, 528.0928921688356, 600.0341248889114, 397.615182177782, 395.3222994008622, 470.0558113773526, 441.0725293110616, 405.5891813588228, 411.93273192525174, 411.6111610053301, 439.97756950558846, 395.8796962531395, 500.47007988611193, 460.65151436924776, 555.1894290039398, 369.9763501452061, 408.0267733745919, 363.5092399586391, 470.7290714561957, 451.25180226596143, 409.53744082213615, 512.9220907095804, 467.8559128905453, 402.43041802836115, 436.0388613974902, 408.7789076737045]\n",
      "mean return 438.53840008567806\n",
      "std of return 54.034271296325436\n",
      "Rollout result. env: Humanoid-v2 , policy_type: learned , returns: 200 / 438.53840008567806 / 54.034271296325436\n",
      "It took          65  seconds.\n",
      "loading and building expert policy\n",
      "obs (1, 17) (1, 17)\n",
      "loaded and built\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "returns [5549.11795568662, 5481.017065578342, 5569.628601964801, 5362.902848756231, 5613.319667461647, 5499.067629559436, 5560.1819131859775, 5471.29210256125, 5480.685724750016, 5597.752817892533, 5528.746846206822, 5428.670013174021, 5518.0304838907905, 5558.798044373062, 5561.683413956786, 5573.512779077428, 5483.000050622262, 5490.320324464886, 5546.463317967161, 5567.582450556764, 5586.1626125776265, 5511.2678649161335, 5425.95175449446, 5422.983888807188, 5513.148820812826, 5566.420411744958, 5149.805223895617, 5478.5542457465735, 5547.127675157535, 5457.029829359267, 5533.4539730598535, 5468.395576116896, 5450.9766667995, 5618.764640698721, 5586.930944096051, 5457.346313892962, 5531.213111596462, 5374.16756828712, 5431.95272273935, 5538.160137383163, 5470.497422585791, 5598.703248401569, 5567.266177236033, 5519.258421080278, 5585.2578480754, 5482.4382982408, 5541.0596844560405, 5536.10812421383, 5480.4007629111165, 5570.113635969543, 5557.44618251392, 5594.140118912514, 5573.495622847645, 5476.469346450346, 5507.093134699752, 5562.589733930562, 5586.030575480568, 5425.857291940742, 5582.42173593207, 5593.369595232363, 5436.236737635694, 5469.760846810784, 5601.264728880751, 5466.5093486295855, 5449.728144771158, 5504.6379484804265, 5565.328882630923, 5499.388765295738, 5497.528197270665, 5507.857002015835, 5588.668088977949, 5597.234585341203, 5391.559554830796, 5492.272167410893, 5439.427409649259, 5514.266394709851, 5577.418464010907, 5442.689010674382, 5539.1453981180375, 5489.0610668064555, 5480.995958028551, 5362.161304631532, 5456.936023358111, 5556.172049709697, 5507.358950632546, 5551.7461539024425, 5410.515657955689, 5583.264567330389, 5488.674203830217, 5446.753007877382, 5538.49619117741, 5504.344184152573, 5544.102177691418, 5565.488029351192, 5412.0504394537975, 5557.861112144272, 5439.329016049083, 5412.946894164928, 5485.6017499951495, 5560.076891239673, 5542.278596091291, 5529.168593795277, 5473.239541420918, 5470.271961899713, 5496.801586490332, 5589.169126499175, 5521.531006700761, 5548.255541236437, 5373.543846396254, 5495.674558677664, 5534.1435352036, 5507.909829628063, 5511.8103981986815, 5522.345681671204, 5571.642930979465, 5426.81231524957, 5540.793692734627, 5557.985596318333, 5609.764777385865, 5489.633740848983, 5603.2851173481395, 5496.645135536696, 5542.1137588098845, 5526.4143219726275, 5599.8676787363265, 5461.63833926902, 5592.644647209522, 5463.293760601405, 5592.3699889471045, 5524.839603629926, 5525.866334669934, 5559.197690979108, 5577.764500868182, 5564.390766040178, 5480.447237553625, 5458.0826682743555, 5562.327702590772, 5520.947254975402, 5479.987777232602, 5527.27365852612, 5520.116323983412, 5577.0957791306255, 5357.286338681786, 5472.284968639581, 5491.946637274166, 5547.366728275637, 5529.719763262162, 5610.077545408773, 5540.492757331696, 5524.677434364615, 5552.344853611003, 5096.644974061262, 5494.83810438921, 5570.067864256057, 5562.007441639776, 5573.314420212089, 5489.205468732673, 5529.396795517363, 5505.682548241273, 5515.425619562744, 5527.171674446151, 5554.314746451234, 5545.846788044728, 5567.62179606547, 5577.3931404333825, 5480.10583279492, 5625.640621639685, 5508.248300673865, 5432.130404103202, 5520.184808427506, 5507.33616693047, 5517.537701456576, 5540.690507200307, 5579.365515775421, 5592.636759246089, 5568.160539264281, 5432.331277029986, 5504.798302172667, 5497.790570581569, 5543.018208326987, 5418.008918799698, 5587.502594476356, 5521.742587119239, 5522.302553185147, 5560.797160882347, 5543.383674932566, 5440.136135558516, 5489.232980063032, 5546.2722000697995, 5472.833984132977, 5504.582774438206, 5579.991223391385, 5560.049963983498, 5519.060562754092, 5543.385609275269, 5486.510451355351, 5508.00218757115, 5586.672511618859, 5503.86002882003, 5569.559486820346]\n",
      "mean return 5514.774203593463\n",
      "std of return 68.39709553070118\n",
      "Rollout result. env: Walker2d-v2 , policy_type: expert , returns: 200 / 5514.774203593463 / 68.39709553070118\n",
      "It took         421  seconds.\n",
      "saved dir: model_Walker2d-v2\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-0/bias/Adadelta\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-0/kernel/Adadelta\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-0/bias/Adadelta_1\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-0/kernel/Adadelta_1\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-1/kernel/Adadelta_1\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-1/bias/Adadelta\n",
      "Walker2d-v2-dnn/layer-1/bias\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-1/kernel/Adadelta\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-1/bias/Adadelta_1\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-2/bias/Adadelta\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-2/bias/Adadelta_1\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-2/kernel/Adadelta_1\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-2/kernel/Adadelta\n",
      "Walker2d-v2-dnn/layer-last/bias\n",
      "Walker2d-v2-dnn/layer-0/kernel\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-last/bias/Adadelta_1\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-last/bias/Adadelta\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-last/kernel/Adadelta\n",
      "Walker2d-v2-dnn/Walker2d-v2-dnn/layer-last/kernel/Adadelta_1\n",
      "Walker2d-v2-dnn/layer-last/kernel\n",
      "Walker2d-v2-dnn/layer-0/bias\n",
      "Walker2d-v2-dnn/layer-1/kernel\n",
      "Walker2d-v2-dnn/layer-2/bias\n",
      "Walker2d-v2-dnn/layer-2/kernel\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "returns [1026.4335584171008, 1024.5906705385137, 1082.4279180296398, 1072.0064322891426, 13.972107542287517, 1025.3364685359413, 1074.622201255164, 1068.2704520022044, 1065.631887276356, 1025.698992017305, 1073.7117268469178, 1081.6499431700422, 13.607296395459521, 1064.6577292489694, 1026.2571822769378, 1033.3173771882612, 1024.3661601359765, 1064.658107958435, 1079.4138202650538, 1078.714460089275, 1025.498156912373, 1023.6250150954459, 1053.2346826138955, 1023.6902443749428, 13.363032730204564, 1025.5959178602598, 1083.217463861821, 1068.145591087754, 1059.7378533880578, 1024.761819193461, 1073.0576112496592, 1067.850577303099, 1023.6304255762615, 1069.0737404861416, 1065.2381851387324, 1074.1346626194968, 1066.881297801041, 1068.057569439008, 1060.359093731901, 1032.0098761432776, 1066.4932579452416, 13.140467199570935, 12.944098692334585, 1070.8216766706184, 1026.610082782256, 1064.3386315063121, 1031.5395953563016, 1066.8347078960755, 1023.6786690118269, 1059.1144102619694, 12.9335420501528, 1073.2528521443785, 1079.8092020940999, 1026.7970076005163, 1070.4635812095605, 1025.8152959219908, 1022.6464310462837, 1061.9052062710118, 13.915393774862928, 1024.0063331370284, 1063.9365816475836, 13.45086055837618, 1065.913558765574, 1066.7014719741717, 1026.693868634001, 1067.6121828200912, 1052.102594800696, 1066.2560627307923, 1065.2125090045931, 1025.4602486070196, 1025.747916702796, 1070.8926743066315, 13.567817256630004, 1026.510902937723, 1032.7707900152514, 1066.6259253955957, 1071.0324933549098, 1022.6293179208808, 1031.744345706132, 12.69573715206597, 1075.0812076753925, 1067.6184876616496, 1027.8520768222509, 1027.3133866151516, 1065.7300416907115, 1072.3226324273246, 13.411847127347107, 1068.2015307422164, 1059.1897369353735, 1069.8567044697302, 1079.0157289145309, 1031.9881642728317, 1026.766186415544, 13.96904439335261, 1028.8721218512146, 571.792448311741, 1066.0091019060055, 14.019487935006852, 1067.3553752193443, 1025.9176120175753, 1083.7323252855097, 1023.4526368353341, 1072.753888601571, 1028.0699309690385, 1026.1933742204553, 1023.6208923929976, 1025.7936195603058, 1027.5396483490356, 13.104147540000202, 1023.6194594772568, 13.405638362376894, 1064.6637312941677, 1080.814261250149, 1071.1535220295448, 1060.8089042010802, 1033.705174144939, 1025.0343519171408, 1022.1613199270315, 1085.818082472038, 1063.9658283913127, 1024.7476176724267, 13.375229477274814, 1067.9099794043073, 1024.509254487209, 1060.7809221339662, 1069.9842308258133, 1081.493753893171, 1061.6087208437714, 1061.7472525068667, 1072.4933058801296, 1072.3704232026896, 13.68256683545041, 1022.3365468628668, 1060.590778738378, 1024.0963620310717, 1024.9440566771168, 1070.7237121840787, 1070.5452735049096, 1068.293616213691, 1071.3934481315212, 13.996449845551947, 14.176234499758722, 1025.8711859737837, 12.502698113200601, 1080.390929833568, 1069.6950403288472, 1024.22308220946, 1031.6808228819968, 1081.7447831019908, 13.723454397571588, 1067.8551851516922, 581.1287370550784, 13.19185877967572, 1023.938253464489, 1074.1023112079017, 1065.2269047777888, 1025.2245287554008, 1070.976631768762, 1025.4702273211567, 1019.1516502334931, 1024.6488269081594, 14.158355894421328, 1018.5185293398678, 1064.049212210666, 1066.929291384308, 1061.3096626370243, 1024.2981257262043, 1063.9765301256414, 1025.0698335039808, 1060.917408835427, 1067.1808470788592, 12.570771179170904, 1066.929260023668, 12.554959545951387, 13.239521621054172, 13.841480412566304, 13.222495199636546, 1068.210739429892, 1023.2496975648877, 1065.5816301594941, 13.765264556202547, 1069.7458561728085, 1067.4589535266396, 1025.6528321613641, 1062.5726868593617, 1025.2380456137594, 1068.7661013796674, 1060.923035691846, 1072.28389619895, 1067.367847234868, 1062.9649323837007, 1070.6416326075137, 1060.6602697475944, 1069.3301361355946, 1066.5433799387436, 12.345124500373178, 1023.8153263195262, 1068.4715577378863, 1024.322777356361, 1074.8977456176074]\n",
      "mean return 891.5847093665221\n",
      "std of return 372.4602159588023\n",
      "Rollout result. env: Walker2d-v2 , policy_type: learned , returns: 200 / 891.5847093665221 / 372.4602159588023\n",
      "It took         461  seconds.\n"
     ]
    }
   ],
   "source": [
    "# rollout and check\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "max_timesteps = None\n",
    "num_rollouts = 200\n",
    "\n",
    "df = pd.DataFrame(columns=['gymenv', 'policy_type', 'rollouts', 'mean', 'std'])\n",
    "\n",
    "for gym_env in gym_envs :\n",
    "    for expert_policy in [ True, False] :\n",
    "        start_time = dt.datetime.now()\n",
    "        rollout_data, policy_type = rollout_by_policy(gym_env, max_timesteps, num_rollouts,\n",
    "                                                      policy_fn=None if expert_policy else load_learned_policy_fn(gym_env),\n",
    "                                                      render=False)\n",
    "        returns = rollout_data['returns']\n",
    "        end_time = dt.datetime.now()\n",
    "        print('Rollout result. env:', gym_env, ', policy_type:', policy_type, ', returns:', len(returns), '/', np.mean(returns), '/', np.std(returns))\n",
    "        df = df.append({'gymenv':gym_env, 'policy_type':policy_type, 'rollouts': len(returns), 'mean': np.mean(returns), 'std': np.std(returns)}, ignore_index=True)\n",
    "        print('It took ', '%10d' % ((end_time - start_time).total_seconds()), ' seconds.')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gymenv</th>\n",
       "      <th>policy_type</th>\n",
       "      <th>rollouts</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ant-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>200</td>\n",
       "      <td>4761.194459</td>\n",
       "      <td>450.185343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ant-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>200</td>\n",
       "      <td>923.733534</td>\n",
       "      <td>139.099641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hopper-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>200</td>\n",
       "      <td>3777.936095</td>\n",
       "      <td>3.890608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hopper-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>200</td>\n",
       "      <td>148.384814</td>\n",
       "      <td>2.961851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reacher-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>200</td>\n",
       "      <td>-4.039447</td>\n",
       "      <td>1.720911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reacher-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>200</td>\n",
       "      <td>-12.587130</td>\n",
       "      <td>4.246398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HalfCheetah-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>200</td>\n",
       "      <td>4134.754269</td>\n",
       "      <td>77.588853</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HalfCheetah-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>200</td>\n",
       "      <td>2255.663415</td>\n",
       "      <td>167.547038</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Humanoid-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>200</td>\n",
       "      <td>10351.381925</td>\n",
       "      <td>700.460059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Humanoid-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>200</td>\n",
       "      <td>438.538400</td>\n",
       "      <td>54.034271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Walker2d-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>200</td>\n",
       "      <td>5514.774204</td>\n",
       "      <td>68.397096</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Walker2d-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>200</td>\n",
       "      <td>891.584709</td>\n",
       "      <td>372.460216</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            gymenv policy_type rollouts          mean         std\n",
       "0           Ant-v2      expert      200   4761.194459  450.185343\n",
       "1           Ant-v2     learned      200    923.733534  139.099641\n",
       "2        Hopper-v2      expert      200   3777.936095    3.890608\n",
       "3        Hopper-v2     learned      200    148.384814    2.961851\n",
       "4       Reacher-v2      expert      200     -4.039447    1.720911\n",
       "5       Reacher-v2     learned      200    -12.587130    4.246398\n",
       "6   HalfCheetah-v2      expert      200   4134.754269   77.588853\n",
       "7   HalfCheetah-v2     learned      200   2255.663415  167.547038\n",
       "8      Humanoid-v2      expert      200  10351.381925  700.460059\n",
       "9      Humanoid-v2     learned      200    438.538400   54.034271\n",
       "10     Walker2d-v2      expert      200   5514.774204   68.397096\n",
       "11     Walker2d-v2     learned      200    891.584709  372.460216"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## DAgger\n",
    "\n",
    "1. Implement DAgger. See the code provided in run expert.py to see how to query the expert policy and perform roll-outs in the environment.\n",
    "\n",
    "2. Run DAgger and report results on one task in which DAgger can learn a better policy than behavioral cloning.\n",
    "Report your results in the form of a learning curve, plotting the number of DAgger iterations vs. the policy’s mean return,\n",
    "with error bars to show the standard deviation.\n",
    "\n",
    "Include the performance of the expert policy and the behavioral cloning agent on the same plot.\n",
    "In the caption, state which task you used, and any details regarding network architecture, amount of data, etc. (as in the previous section).\n",
    "\n",
    "### note\n",
    "1. DAgger needs labeling by human experts.\n",
    "1. The main idea is that the trajectories are collected by the learned policy. but the action is relabeled by the expert policy.\n",
    "1. DAgger addresses the problem of distributional “drift”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "def rollout_by_dagger(gym_env, max_timesteps, num_rollouts, render=False) :\n",
    "    policy_type = 'dagger'\n",
    "    \n",
    "    print('loading and building learned policy')\n",
    "    policy_fn = load_learned_policy_fn(gym_env)\n",
    "    print('loaded and built')\n",
    "\n",
    "    print('starting dagger ', gym_env, dt.datetime.now())\n",
    "    train_observations, train_actions = load_expert_data(gym_env)\n",
    "    obs_shape, action_shape = np.shape(train_observations), np.shape(train_actions)\n",
    "    if action_shape[1] == 1 :\n",
    "        train_actions = np.reshape(train_actions, (action_shape[0], action_shape[2]))\n",
    "        action_shape = np.shape(train_actions)\n",
    "    print(gym_env, ' observation shape: ', obs_shape, ', actions shape:', action_shape)\n",
    "    \n",
    "    with tf.Session(graph=tf.Graph()) as session, session.graph.as_default() : # for session nesting, the graphs should be isolated for each tf sessions\n",
    "        print('loading and building expert policy for DAgger')\n",
    "        expert_policy_fn = load_expert_policy_fn(gym_env)\n",
    "        print('loaded and built for DAgger')\n",
    "\n",
    "        tf_util.initialize()\n",
    "\n",
    "        gym_env_model = 'model_' + gym_env\n",
    "        gym_env_dagger_model = 'model_dagger_' + gym_env # new model file to save after lite training\n",
    "        light_model_config = default_model_config.copy()\n",
    "        light_model_config['num_epochs'] = 50\n",
    "        cloning_model = None\n",
    "\n",
    "        env = gym.make(gym_env)\n",
    "        max_steps = max_timesteps or env.spec.timestep_limit\n",
    "\n",
    "        returns = []\n",
    "        observations = []\n",
    "        actions = []\n",
    "\n",
    "        for i in range(num_rollouts):\n",
    "            # print('iter', i)\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            totalr = 0.\n",
    "            steps = 0\n",
    "\n",
    "            while not done:\n",
    "                action = policy_fn(obs[None,:])\n",
    "                # print('before append>>>> observations shape:', np.shape(observations), ', train_observations shape:', np.shape(train_observations), ', obs shape:', np.shape(obs))\n",
    "                observations.append(obs)\n",
    "                train_observations = np.append(train_observations, obs[None, :], axis=0)\n",
    "                # print('after append>>>> observations shape:', np.shape(observations), ', train_observations shape:', np.shape(train_observations), ', obs shape:', np.shape(obs))\n",
    "\n",
    "                expert_action = expert_policy_fn(obs[None,:]) # None makes additional dimension. to reduce, use np.hstack\n",
    "                actions.append(expert_action)\n",
    "                # print('before append>>>> actions shape:', np.shape(actions), ', train_actions shape:', np.shape(train_actions), ', expert_action shape:', np.shape(expert_action))\n",
    "                train_actions = np.append(train_actions, expert_action, axis=0)\n",
    "                # print('after append>>>> actions shape:', np.shape(actions), ', train_actions shape:', np.shape(train_actions), ', expert_action shape:', np.shape(expert_action))\n",
    "\n",
    "                try :\n",
    "                    if np.shape(action)[1] == 1 :\n",
    "                        action_shape = np.shape(action)\n",
    "                        action = np.reshape(action, (action_shape[0], action_shape[2]))\n",
    "                        action_shape = np.shape(action)\n",
    "                    obs, r, done, _ = env.step(action) # observation, reward, done\n",
    "                except ValueError as e :\n",
    "                    print('action:', action, ', shape:', np.shape(action), ', policy_type:', policy_type)\n",
    "                    print('actions:', actions, ', shape:', np.shape(actions))\n",
    "                    print('expert_action:', expert_action, ', shape:', np.shape(expert_action))\n",
    "                    traceback.print_exc()     \n",
    "\n",
    "                totalr += r\n",
    "                steps += 1\n",
    "                if render:\n",
    "                    env.render()\n",
    "                # if steps % 100 == 0: print(\"%i/%i\"%(steps, max_steps))\n",
    "                if steps >= max_steps:\n",
    "                    break\n",
    "            returns.append(totalr)\n",
    "\n",
    "            # retrain on every new rollouts\n",
    "            n_samples = train_observations.shape[0]\n",
    "            n_train = int(n_samples * (1 - TEST_PERCENT))\n",
    "\n",
    "            print('train_observations shape:', train_observations.shape, ', train_actions shape:', train_actions.shape)\n",
    "            print('observations shape:', np.shape(observations), ', actions shape:', np.shape(actions))\n",
    "            try :\n",
    "                train_observations, train_actions = shuffle_XY(train_observations, train_actions)\n",
    "            except IndexError as e :\n",
    "                print('train_observations:', train_observations, ', train_actions:', train_actions)\n",
    "                traceback.print_exc() \n",
    "\n",
    "            print('train input : train_observations shape:', train_observations[:n_train].shape, ', train_actions shape:', train_actions[:n_train].shape)\n",
    "\n",
    "            with tf.Session(graph=tf.Graph()) as nested_session, nested_session.graph.as_default():\n",
    "                saved_model = gym_env_model if cloning_model is None else gym_env_dagger_model\n",
    "                cloning_model = BehavioralCloning(X_shape=obs_shape, Y_shape=action_shape,\n",
    "                                                  model_config = light_model_config, scope_name=gym_env, restore_mode=True, session=nested_session)\n",
    "                cloning_model.restore_model(saved_model)        \n",
    "                training_costs, validation_costs, validation_measures = cloning_model.train(train_observations[:n_train], train_actions[:n_train])\n",
    "                cloning_model.save_model(gym_env_dagger_model)\n",
    "\n",
    "        with tf.Session(graph=tf.Graph()) as nested_session, nested_session.graph.as_default():\n",
    "            cloning = BehavioralCloning(X_shape=obs_shape, Y_shape=action_shape,\n",
    "                                        model_config = light_model_config, scope_name=gym_env, restore_mode=True, session=nested_session)\n",
    "            cloning.restore_model(gym_env_dagger_model)\n",
    "            test_hyps, test_costs, test_measures = cloning.test(train_observations[n_train:], train_actions[n_train:])\n",
    "\n",
    "        print('returns', returns)\n",
    "        print('mean return', np.mean(returns))\n",
    "        print('std of return', np.std(returns))\n",
    "\n",
    "        rollout_data = {'observations': np.array(observations),\n",
    "                        'actions': np.array(actions),\n",
    "                        'returns': np.array(returns)}\n",
    "\n",
    "        if not os.path.exists('rollout_data') :\n",
    "            os.mkdir('rollout_data')\n",
    "        with open(os.path.join('rollout_data', policy_type + '-' + gym_env + '.pkl'), 'wb') as f:\n",
    "            pk.dump(rollout_data, f, pk.HIGHEST_PROTOCOL)\n",
    "\n",
    "        return rollout_data, policy_type\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and building learned policy\n",
      "saved dir: model_Ant-v2\n",
      "loaded and built\n",
      "starting dagger  Ant-v2 2019-07-09 08:04:58.419304\n",
      "Ant-v2  observation shape:  (19992, 111) , actions shape: (19992, 8)\n",
      "loading and building expert policy for DAgger\n",
      "obs (1, 111) (1, 111)\n",
      "loaded and built for DAgger\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "train_observations shape: (20992, 111) , train_actions shape: (20992, 8)\n",
      "observations shape: (1000, 111) , actions shape: (1000, 1, 8)\n",
      "train input : train_observations shape: (16793, 111) , train_actions shape: (16793, 8)\n",
      "saved dir: model_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-09 08:05:06.873484\n",
      "Epoch: 0000 average training cost = 0.040661234 validation cost = 0.018868102 validation measure = 0.762766302 2019-07-09 08:05:07.514770\n",
      "Epoch: 0500 average training cost = 0.028853862 validation cost = 0.014281301 validation measure = 0.820437372 2019-07-09 08:09:39.166667\n",
      "Training(learning) Finished! 2019-07-09 08:14:13.587053\n",
      "Training took         546  seconds.\n",
      "train_observations shape: (21992, 111) , train_actions shape: (21992, 8)\n",
      "observations shape: (2000, 111) , actions shape: (2000, 1, 8)\n",
      "train input : train_observations shape: (17593, 111) , train_actions shape: (17593, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-09 08:14:23.377052\n",
      "Epoch: 0000 average training cost = 0.025830956 validation cost = 0.014556212 validation measure = 0.819969118 2019-07-09 08:14:24.143416\n",
      "Epoch: 0500 average training cost = 0.022775553 validation cost = 0.013715847 validation measure = 0.830362737 2019-07-09 08:19:26.822158\n",
      "Training(learning) Finished! 2019-07-09 08:24:27.261804\n",
      "Training took         603  seconds.\n",
      "train_observations shape: (22992, 111) , train_actions shape: (22992, 8)\n",
      "observations shape: (3000, 111) , actions shape: (3000, 1, 8)\n",
      "train input : train_observations shape: (18393, 111) , train_actions shape: (18393, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-09 08:24:37.238122\n",
      "Epoch: 0000 average training cost = 0.021706007 validation cost = 0.012105504 validation measure = 0.852767646 2019-07-09 08:24:38.008104\n",
      "Epoch: 0500 average training cost = 0.020430671 validation cost = 0.011456035 validation measure = 0.860666752 2019-07-09 08:29:50.170822\n",
      "Training(learning) Finished! 2019-07-09 08:35:02.647630\n",
      "Training took         625  seconds.\n",
      "train_observations shape: (23992, 111) , train_actions shape: (23992, 8)\n",
      "observations shape: (4000, 111) , actions shape: (4000, 1, 8)\n",
      "train input : train_observations shape: (19193, 111) , train_actions shape: (19193, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-09 08:35:12.799485\n",
      "Epoch: 0000 average training cost = 0.018805476 validation cost = 0.011763833 validation measure = 0.858328879 2019-07-09 08:35:13.574420\n",
      "Epoch: 0500 average training cost = 0.017877569 validation cost = 0.011193614 validation measure = 0.865195990 2019-07-09 08:40:44.657561\n",
      "Training(learning) Finished! 2019-07-09 08:46:17.497594\n",
      "Training took         664  seconds.\n",
      "train_observations shape: (24992, 111) , train_actions shape: (24992, 8)\n",
      "observations shape: (5000, 111) , actions shape: (5000, 1, 8)\n",
      "train input : train_observations shape: (19993, 111) , train_actions shape: (19993, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-09 08:46:27.698219\n",
      "Epoch: 0000 average training cost = 0.016999375 validation cost = 0.010347857 validation measure = 0.876150727 2019-07-09 08:46:28.503030\n",
      "Epoch: 0500 average training cost = 0.016138176 validation cost = 0.009853259 validation measure = 0.882070363 2019-07-09 08:52:20.678599\n",
      "Training(learning) Finished! 2019-07-09 08:58:11.237048\n",
      "Training took         703  seconds.\n",
      "train_observations shape: (25992, 111) , train_actions shape: (25992, 8)\n",
      "observations shape: (6000, 111) , actions shape: (6000, 1, 8)\n",
      "train input : train_observations shape: (20793, 111) , train_actions shape: (20793, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-09 08:58:21.677189\n",
      "Epoch: 0000 average training cost = 0.015524133 validation cost = 0.009371462 validation measure = 0.889335334 2019-07-09 08:58:22.558782\n",
      "Epoch: 0500 average training cost = 0.014916497 validation cost = 0.008960987 validation measure = 0.894182503 2019-07-09 09:04:37.906773\n",
      "Training(learning) Finished! 2019-07-09 09:10:53.299410\n",
      "Training took         751  seconds.\n",
      "train_observations shape: (26992, 111) , train_actions shape: (26992, 8)\n",
      "observations shape: (7000, 111) , actions shape: (7000, 1, 8)\n",
      "train input : train_observations shape: (21593, 111) , train_actions shape: (21593, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-09 09:11:04.358755\n",
      "Epoch: 0000 average training cost = 0.014144038 validation cost = 0.008619878 validation measure = 0.899924397 2019-07-09 09:11:05.272468\n",
      "Epoch: 0500 average training cost = 0.013686695 validation cost = 0.008270578 validation measure = 0.903979719 2019-07-09 09:17:29.718011\n",
      "Training(learning) Finished! 2019-07-09 09:23:54.845301\n",
      "Training took         770  seconds.\n",
      "train_observations shape: (27992, 111) , train_actions shape: (27992, 8)\n",
      "observations shape: (8000, 111) , actions shape: (8000, 1, 8)\n",
      "train input : train_observations shape: (22393, 111) , train_actions shape: (22393, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-09 09:24:05.903737\n",
      "Epoch: 0000 average training cost = 0.013239468 validation cost = 0.008036008 validation measure = 0.906583309 2019-07-09 09:24:06.773472\n",
      "Epoch: 0500 average training cost = 0.012800712 validation cost = 0.007762362 validation measure = 0.909764349 2019-07-09 09:30:41.714634\n",
      "Training(learning) Finished! 2019-07-09 09:37:13.898932\n",
      "Training took         787  seconds.\n",
      "train_observations shape: (28013, 111) , train_actions shape: (28013, 8)\n",
      "observations shape: (8021, 111) , actions shape: (8021, 1, 8)\n",
      "train input : train_observations shape: (22410, 111) , train_actions shape: (22410, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-09 09:37:14.851633\n",
      "Epoch: 0000 average training cost = 0.012419038 validation cost = 0.007294154 validation measure = 0.915247321 2019-07-09 09:37:15.734488\n",
      "Epoch: 0500 average training cost = 0.011887108 validation cost = 0.007080055 validation measure = 0.917734981 2019-07-09 09:43:36.719461\n",
      "Training(learning) Finished! 2019-07-09 09:49:58.486515\n",
      "Training took         763  seconds.\n",
      "train_observations shape: (29013, 111) , train_actions shape: (29013, 8)\n",
      "observations shape: (9021, 111) , actions shape: (9021, 1, 8)\n",
      "train input : train_observations shape: (23210, 111) , train_actions shape: (23210, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-09 09:50:09.860544\n",
      "Epoch: 0000 average training cost = 0.011829365 validation cost = 0.006909515 validation measure = 0.921578944 2019-07-09 09:50:10.811002\n",
      "Epoch: 0500 average training cost = 0.011553247 validation cost = 0.006657235 validation measure = 0.924442291 2019-07-09 09:57:11.900419\n",
      "Training(learning) Finished! 2019-07-09 10:04:11.961248\n",
      "Training took         842  seconds.\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-09 10:04:12.553128  and finished at  2019-07-09 10:04:12.584047\n",
      "returns [908.5654961646536, 1087.3711575470907, 881.6605537325263, 869.3251266816216, 877.4739590066217, 981.5410693101742, 941.2894186350519, 887.2601827797297, 14.966787921014541, 1221.5422616086414]\n",
      "mean return 867.0996013387124\n",
      "std of return 303.53433730818955\n",
      "Rollout result. env: Ant-v2 , policy_type: dagger , returns: 10 / 867.0996013387124 / 303.53433730818955\n",
      "It took        7154  seconds.\n",
      "loading and building learned policy\n",
      "saved dir: model_Hopper-v2\n",
      "loaded and built\n",
      "starting dagger  Hopper-v2 2019-07-09 10:04:13.035838\n",
      "Hopper-v2  observation shape:  (20000, 11) , actions shape: (20000, 3)\n",
      "loading and building expert policy for DAgger\n",
      "obs (1, 11) (1, 11)\n",
      "loaded and built for DAgger\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "train_observations shape: (20082, 11) , train_actions shape: (20082, 3)\n",
      "observations shape: (82, 11) , actions shape: (82, 1, 3)\n",
      "train input : train_observations shape: (16065, 11) , train_actions shape: (16065, 3)\n",
      "saved dir: model_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-09 10:04:14.522350\n",
      "Epoch: 0000 average training cost = 0.197049186 validation cost = 0.098329633 validation measure = 0.953608572 2019-07-09 10:04:15.195912\n",
      "Epoch: 0500 average training cost = 0.164469957 validation cost = 0.083604053 validation measure = 0.960556030 2019-07-09 10:08:42.911880\n",
      "Training(learning) Finished! 2019-07-09 10:13:08.662883\n",
      "Training took         534  seconds.\n",
      "train_observations shape: (20164, 11) , train_actions shape: (20164, 3)\n",
      "observations shape: (164, 11) , actions shape: (164, 1, 3)\n",
      "train input : train_observations shape: (16131, 11) , train_actions shape: (16131, 3)\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-09 10:13:09.874513\n",
      "Epoch: 0000 average training cost = 0.159819067 validation cost = 0.080277704 validation measure = 0.962669253 2019-07-09 10:13:10.574641\n",
      "Epoch: 0500 average training cost = 0.146199822 validation cost = 0.072725207 validation measure = 0.966181338 2019-07-09 10:17:42.169104\n",
      "Training(learning) Finished! 2019-07-09 10:22:11.808801\n",
      "Training took         541  seconds.\n",
      "train_observations shape: (20247, 11) , train_actions shape: (20247, 3)\n",
      "observations shape: (247, 11) , actions shape: (247, 1, 3)\n",
      "train input : train_observations shape: (16197, 11) , train_actions shape: (16197, 3)\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-09 10:22:12.950586\n",
      "Epoch: 0000 average training cost = 0.149034232 validation cost = 0.066719115 validation measure = 0.967562199 2019-07-09 10:22:13.627734\n",
      "Epoch: 0500 average training cost = 0.136127234 validation cost = 0.059536673 validation measure = 0.971054137 2019-07-09 10:26:48.143929\n",
      "Training(learning) Finished! 2019-07-09 10:31:23.197885\n",
      "Training took         550  seconds.\n",
      "train_observations shape: (20329, 11) , train_actions shape: (20329, 3)\n",
      "observations shape: (329, 11) , actions shape: (329, 1, 3)\n",
      "train input : train_observations shape: (16263, 11) , train_actions shape: (16263, 3)\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-09 10:31:24.340356\n",
      "Epoch: 0000 average training cost = 0.132607758 validation cost = 0.065515362 validation measure = 0.969630659 2019-07-09 10:31:25.046421\n",
      "Epoch: 0500 average training cost = 0.126586422 validation cost = 0.059541818 validation measure = 0.972399652 2019-07-09 10:36:06.533416\n",
      "Training(learning) Finished! 2019-07-09 10:40:47.078427\n",
      "Training took         562  seconds.\n",
      "train_observations shape: (20411, 11) , train_actions shape: (20411, 3)\n",
      "observations shape: (411, 11) , actions shape: (411, 1, 3)\n",
      "train input : train_observations shape: (16328, 11) , train_actions shape: (16328, 3)\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-09 10:40:48.318716\n",
      "Epoch: 0000 average training cost = 0.124189913 validation cost = 0.057508480 validation measure = 0.974101305 2019-07-09 10:40:49.005831\n",
      "Epoch: 0500 average training cost = 0.116494305 validation cost = 0.052183207 validation measure = 0.976499557 2019-07-09 10:45:17.389201\n"
     ]
    }
   ],
   "source": [
    "num_rollouts = 10 # incremental learning is too slow \n",
    "\n",
    "for gym_env in gym_envs :\n",
    "    start_time = dt.datetime.now()\n",
    "    rollout_data, policy_type = rollout_by_dagger(gym_env, max_timesteps, num_rollouts,\n",
    "                                                  render=False)\n",
    "    returns = rollout_data['returns']\n",
    "    end_time = dt.datetime.now()\n",
    "    print('Rollout result. env:', gym_env, ', policy_type:', policy_type, ', returns:', len(returns), '/', np.mean(returns), '/', np.std(returns))\n",
    "    df = df.append({'gymenv':gym_env, 'policy_type':policy_type, 'rollouts': len(returns), 'mean': np.mean(returns), 'std': np.std(returns)}, ignore_index=True)\n",
    "    print('It took ', '%10d' % ((end_time - start_time).total_seconds()), ' seconds.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## just rollout and render using the policies. enjoy the visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rollout and check\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "max_timesteps = None\n",
    "num_rollouts = 1\n",
    "\n",
    "df2 = pd.DataFrame(columns=['gymenv', 'policy_type', 'rollouts', 'mean', 'std'])\n",
    "\n",
    "for gym_env in gym_envs :\n",
    "    for expert_policy in [ True, False] :\n",
    "        start_time = dt.datetime.now()\n",
    "        rollout_data, policy_type = rollout_by_policy(gym_env, max_timesteps, num_rollouts,\n",
    "                                                      policy_fn=None if expert_policy else load_learned_policy_fn(gym_env),\n",
    "                                                      render=True)\n",
    "        returns = rollout_data['returns']\n",
    "        end_time = dt.datetime.now()\n",
    "        print('Rollout result. env:', gym_env, ', policy_type:', policy_type, ', returns:', len(returns), '/', np.mean(returns), '/', np.std(returns))\n",
    "        df2 = df2.append({'gymenv':gym_env, 'policy_type':policy_type, 'rollouts': len(returns), 'mean': np.mean(returns), 'std': np.std(returns)}, ignore_index=True)\n",
    "        print('It took ', '%10d' % ((end_time - start_time).total_seconds()), ' seconds.')\n",
    "\n",
    "    with tf.Session(graph=tf.Graph()) as session, session.graph.as_default() : # for session nesting, the graphs should be isolated for each tf sessions\n",
    "        start_time = dt.datetime.now()\n",
    "        rollout_data, policy_type = rollout_by_dagger(gym_env, max_timesteps, num_rollouts,\n",
    "                                                      render=True, session=session)\n",
    "        returns = rollout_data['returns']\n",
    "        end_time = dt.datetime.now()\n",
    "        print('Rollout result. env:', gym_env, ', policy_type:', policy_type, ', returns:', len(returns), '/', np.mean(returns), '/', np.std(returns))\n",
    "        df2 = df2.append({'gymenv':gym_env, 'policy_type':policy_type, 'rollouts': len(returns), 'mean': np.mean(returns), 'std': np.std(returns)}, ignore_index=True)\n",
    "        print('It took ', '%10d' % ((end_time - start_time).total_seconds()), ' seconds.')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "##  Bonus: Alternative Policy Architectures\n",
    "\n",
    "1. (Optional) Experiment with a different policy architecture, e.g. using recurrence or changing the size or nonlinearities used.\n",
    "\n",
    "Compare performance between your new and original policy architectures using behavioral cloning and/or DAgger,\n",
    "and report your results in the same form as above, with a caption describing what you did.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "hw1.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
