{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "autoscroll": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Assignments\n",
    "1. Homework 1: Imitation learning (control via supervised learning)\n",
    "2. Homework 2: Policy gradients (“REINFORCE”)\n",
    "3. Homework 3: Q learning and actor-critic algorithms\n",
    "4. Homework 4: Model-based reinforcement learning\n",
    "5. Homework 5: Advanced model-free RL algorithms\n",
    "6. Final project: Research-level project of your choice (form a group of up to 2-3 students, you’re welcome to start early!)\n",
    "\n",
    "##### Emacs IPython Notebook Commands/Keybinds\n",
    "* http://millejoh.github.io/emacs-ipython-notebook/#commands-keybinds\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "# Homework 1 Imitation Learning\n",
    "\n",
    "Instead of trying to produce a program to simulate the adult mind, why not rather try to produce one which simulates the child's?\n",
    "\n",
    "If this were then subjected to an appropriate course of education one would obtain the adult brain.\n",
    "\n",
    "\\- Alan Turing\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Behavioral Cloning\n",
    "\n",
    "1. The starter code provides an expert policy for each of the MuJoCo tasks in OpenAI Gym (See run expert.py). Generate roll-outs from the provided policies, and implement behavioral cloning. => expert_data/XXX.pkl\n",
    "\n",
    "2. Run behavioral cloning (BC) and report results on two tasks\n",
    " – one task where a behavioral cloning agent achieves comparable performance to the expert,\n",
    " and one task where it does not.\n",
    " When providing results, report the mean and standard deviation of the return over multiple rollouts in a table, and state which task was used.\n",
    " Be sure to set up a fair comparison, in terms of network size, amount of data, and number of training iterations, and provide these details (and any others you feel are appropriate) in the table caption.\n",
    "\n",
    "3. Experiment with one hyperparameter that affects the performance of the behavioral cloning agent, such as\n",
    "* the number of demonstrations,\n",
    "* the number of training epochs,\n",
    "* the variance of the expert policy, or\n",
    "* something that you come up with yourself.\n",
    " For one of the tasks used in the previous question, show a graph of how the BC agent’s performance varies with the value of this hyperparameter, and state the hyperparameter and a brief rationale for why you chose it in the caption for the graph.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1' # 0 is default GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "gym_envs = ('Ant-v2', 'Hopper-v2', 'Reacher-v2', 'HalfCheetah-v2', 'Humanoid-v2', 'Walker2d-v2')\n",
    "\n",
    "num_epochs_per_envs = {'Ant-v2' : 3000, 'Hopper-v2': 2000, 'Reacher-v2' : 3000, 'HalfCheetah-v2' : 2000, 'Humanoid-v2' : 3000, 'Walker2d-v2' : 3000 }\n",
    "num_rollouts_per_envs = {'Ant-v2' : 500, 'Hopper-v2': 500, 'Reacher-v2' : 2500, 'HalfCheetah-v2' : 500, 'Humanoid-v2' : 500, 'Walker2d-v2' : 500 }\n",
    "\n",
    "NUM_ROLLOUTS = 500\n",
    "MAX_TIMESTEPS = None # no check on max timesteps\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## running experts\n",
    "\n",
    "run experts of each gym environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# run expert\n",
    "\n",
    "import sys, os\n",
    "import datetime as dt\n",
    "import run_expert\n",
    "\n",
    "# https://ipython.readthedocs.io/en/stable/interactive/magics.html\n",
    "# path=os.environ['PATH']\n",
    "# %env PATH='/usr/local/bin:'+path\n",
    "\n",
    "RENDER = False\n",
    "\n",
    "for gym_env in gym_envs :\n",
    "    sys.argv = ['run_expert.py', 'experts/' + gym_env + '.pkl', gym_env, '--num_rollouts', str(num_rollouts_per_envs[gym_env]) ]\n",
    "    if RENDER :\n",
    "        sys.argv.append('--render')\n",
    "    run_expert.main()\n",
    "    print('finished run_expert ', gym_env, 'at', dt.datetime.now())\n",
    "\n",
    "print('finished run_expert on all gym_envs at', dt.datetime.now())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## train on each envs\n",
    "\n",
    "using the data gathered by expert policy\n",
    "\n",
    "* environment details : https://github.com/openai/gym/tree/master/gym/envs/mujoco/assets\n",
    "* source codes of each environments : https://github.com/openai/gym/blob/master/gym/envs/mujoco/\n",
    "* reference for an HW1 implementation :  https://hollygrimm.com/rl_bc\n",
    "\n",
    "for the regressor\n",
    "input : observation\n",
    "output : action\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "from load_policy import load_policy\n",
    "import pickle as pk\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def load_expert_data(gymenv) -> (np.array, np.array) : # observations, actions\n",
    "    with open(os.path.join('expert_data', gymenv + '.pkl'), 'rb') as f :\n",
    "        expert_data = pk.load(f)\n",
    "        return expert_data['observations'], expert_data['actions']\n",
    "\n",
    "def load_expert_policy_fn(gymenv) :\n",
    "    return load_policy('experts/' + gymenv + '.pkl')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale (1) minmax (2) normal_dist (3) preserve_sign\n",
    "\n",
    "def scale_minmax(data, minv=None, maxv=None):\n",
    "    if minv == -1 and maxv == -1 : # no scaling\n",
    "        return data, -1, -1\n",
    "    \n",
    "    if minv is None :\n",
    "        minv = np.min(data, 0)\n",
    "    if maxv is None :\n",
    "        maxv = np.max(data, 0)\n",
    "    ''' Min Max Normalization\n",
    "\n",
    "    Parameters\n",
    "        ----------\n",
    "        data : numpy.ndarray\n",
    "        input data to be normalized\n",
    "        shape: [Batch size, dimension]\n",
    "\n",
    "    Returns\n",
    "        ----------\n",
    "        data : numpy.ndarry\n",
    "        normalized data\n",
    "        shape: [Batch size, dimension]\n",
    "\n",
    "    References\n",
    "        ----------\n",
    "        .. [1] http://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n",
    "\n",
    "    '''\n",
    "    numerator = data - minv\n",
    "    denominator = maxv - minv\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7), minv, maxv\n",
    "\n",
    "def descale_minmax(data, minv, maxv):\n",
    "    if minv is None or maxv is None :\n",
    "        return data\n",
    "    if minv == -1 and maxv == -1 : # no scaling\n",
    "        return data\n",
    "    \n",
    "    # noise term prevents the zero division\n",
    "    return data * (maxv - minv + 1e-7) + minv\n",
    "\n",
    "def scale_signed(data, minv=None, maxv=None): # value 0 is preserved even after rescale\n",
    "    if minv == -1 and maxv == -1 : # no scaling\n",
    "        return data, -1, -1\n",
    "\n",
    "    if maxv is None :\n",
    "        maxv = np.max(np.abs(data), 0)\n",
    "    \n",
    "    numerator = data\n",
    "    denominator = maxv\n",
    "    # noise term prevents the zero division\n",
    "    return numerator / (denominator + 1e-7), 0.0, maxv\n",
    "\n",
    "def descale_signed(data, minv, maxv): # value 0 is preserved even after rescale\n",
    "    if minv is None or maxv is None :\n",
    "        return data\n",
    "    if minv == -1 and maxv == -1 : # no scaling\n",
    "        return data\n",
    "    \n",
    "    return data * (maxv + 1e-7)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## Behavior Cloning\n",
    "\n",
    "1. generate rollouts(= expert data) with expert policy (and record the returns)\n",
    "2. learn the rollouts changing some environments (network size, amount of data, and number of training iterations, ...)\n",
    "3. generate rollouts several times according to each policies learned above and show the returns in a table\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from enum import Enum, IntEnum\n",
    "import numpy as np\n",
    "import random\n",
    "import datetime as dt\n",
    "import time\n",
    "import math\n",
    "import pickle as pk\n",
    "import os\n",
    "\n",
    "default_model_config = dict(neurons = [400, 200],\n",
    "                            activation = tf.nn.elu, # Using ReLu, which is a discontinuous function, may raise issues. Try using other activation functions, such as tanh or sigmoid.\n",
    "                            last_activation = None, # final layer activation function. default is no activation\n",
    "                            optimizer = tf.train.AdadeltaOptimizer, # tf.train.AdamOptimizer, tf.train.ProximalAdagradOptimizer\n",
    "                            cost_function = tf.losses.mean_squared_error, # tf.losses.huber_loss (robust to outlier)\n",
    "                            measure_function = 'r_squared', # 'smape' means symmetric_mean_absolute_percentage_error\n",
    ")\n",
    "\n",
    "default_train_config = dict(start_learning_rate = 0.001,\n",
    "                            # minimum_learning_rate = 0.000001,\n",
    "                            num_epochs = 2000,\n",
    "                            batch_size = 100,\n",
    "                            keep_prob = 0.5, # for training only (dropout for hidden layer)\n",
    "                            keep_prob_input = 0.8, # for training only (dropout for input layer)\n",
    "                            validationset_percent = 0.2, # by default 20 percent is validation set\n",
    "                            break_accuracy = 0.99, # -1.0, # 0.999, # -1.0\n",
    "                            early_stopping_epoch_on_max_no_decrease = 500,\n",
    "                            shuffle_samples_epochs = 200, # shuffle samples per given epochs considering performance. -1 means no shuffling\n",
    "                            check_accuracy_epochs = 200, # 5000,\n",
    "                            use_tboard = True,\n",
    "                            print_cost_interval = 500,\n",
    "                            print_trained_model = False,\n",
    "                            )\n",
    "\n",
    "class BehavioralCloning(object) :\n",
    "    default_random_seed = 777\n",
    "\n",
    "    def __init__(self,\n",
    "                 X_shape = None, # X shape as list\n",
    "                 Y_shape = None, # Y shape as list\n",
    "                 model_config = default_model_config,\n",
    "                 scope_name = '',\n",
    "                 restore_mode=False,\n",
    "                 session=None) :\n",
    "        self.model_config = model_config\n",
    "        self.restore_mode = restore_mode\n",
    "        self.scope_name = scope_name\n",
    "        self.X_shape = list(X_shape)\n",
    "        self.X_shape[0] = None\n",
    "        self.Y_shape = list(Y_shape)\n",
    "        self.Y_shape[0] = None\n",
    "\n",
    "        tf.set_random_seed(BehavioralCloning.default_random_seed)  # reproducibility\n",
    "        np.random.seed(BehavioralCloning.default_random_seed)\n",
    "\n",
    "        # Launch new session before graph init\n",
    "        # interactive session will declare itself as a default session and won't be closed on context destroy (so, should explicity call sess.close()\n",
    "        if session is None :\n",
    "            tf.reset_default_graph()\n",
    "            self.session = tf.InteractiveSession()\n",
    "        else :\n",
    "            self.session = session\n",
    "        self._build_network()\n",
    "\n",
    "    def _build_network(self) :\n",
    "        g = tf.get_default_graph()\n",
    "\n",
    "        # build the network\n",
    "        with g.as_default(), self.session.as_default() :\n",
    "            self.X = tf.placeholder(tf.float32, shape=self.X_shape, name='X')\n",
    "            self.Y = tf.placeholder(tf.float32, shape=self.Y_shape, name='Y')\n",
    "            self.p_keep_prob = tf.placeholder(tf.float32, name='p_keep_prob')\n",
    "            self.p_keep_prob_input = tf.placeholder(tf.float32, name='p_keep_prob_input')\n",
    "            self.p_training = tf.placeholder(tf.bool, name='p_training')\n",
    "            self.p_lr = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "            with tf.variable_scope(self.scope_name + '-dnn', reuse=tf.AUTO_REUSE) as scope:\n",
    "                neurons = self.model_config['neurons']\n",
    "                layer = self.X\n",
    "                layer = tf.layers.dropout(layer, rate=1-self.p_keep_prob_input, training=self.p_training)\n",
    "                for i in range(len(neurons)) :\n",
    "                    neuron = neurons[i]\n",
    "\n",
    "                    layer = tf.layers.dense(layer, neuron,\n",
    "                                            kernel_initializer = tf.contrib.layers.xavier_initializer(seed=BehavioralCloning.default_random_seed),\n",
    "                                            activation=self.model_config['activation'],\n",
    "                                            name = 'layer-' + str(i))\n",
    "                    layer = tf.layers.dropout(layer, rate=1-self.p_keep_prob, training=self.p_training)\n",
    "                n_output = self.Y_shape[1]\n",
    "                layer = tf.layers.dense(layer, n_output,\n",
    "                                        kernel_initializer = tf.contrib.layers.xavier_initializer(seed=BehavioralCloning.default_random_seed),\n",
    "                                        activation=self.model_config['last_activation'],\n",
    "                                        name = 'layer-last')\n",
    "                    \n",
    "\n",
    "                self.hypothesis = layer\n",
    "                cost_fn = self.model_config['cost_function']\n",
    "                self.cost = cost_fn(self.Y, self.hypothesis)\n",
    "                tf.summary.scalar(\"cost\", self.cost)\n",
    "                measure_alg = self.model_config['measure_function']\n",
    "                if measure_alg == 'r_squared' :\n",
    "                    self.measure = self.r_squared(self.Y, self.hypothesis)\n",
    "                elif measure_alg == 'smape' :\n",
    "                    self.measure = self.smape(self.Y, self.hypothesis)\n",
    "                else :\n",
    "                    self.measure = None\n",
    "                optimizer_fn = self.model_config['optimizer']\n",
    "                opt = optimizer_fn(learning_rate=self.p_lr)\n",
    "                self.objective_tensor = opt.minimize(self.cost)\n",
    "\n",
    "            if not self.restore_mode :\n",
    "                self.session.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "    def train(self, X, Y, rescale_factor=None, train_config = default_train_config, scale_fn=scale_minmax) :\n",
    "        learning_rate = train_config['start_learning_rate']\n",
    "        num_epochs = train_config['num_epochs']\n",
    "        keep_prob = train_config['keep_prob']\n",
    "        keep_prob_input = train_config['keep_prob_input']\n",
    "        batch_size = train_config['batch_size']\n",
    "        vset_percent = train_config['validationset_percent']\n",
    "        break_accuracy = train_config['break_accuracy']\n",
    "        check_accuracy_epochs = train_config['check_accuracy_epochs']\n",
    "        early_stopping_epoch_on_max_no_decrease = train_config['early_stopping_epoch_on_max_no_decrease']\n",
    "        print_cost_interval = train_config['print_cost_interval']\n",
    "        shuffle_samples_epochs = train_config['shuffle_samples_epochs']\n",
    "        use_tboard = train_config['use_tboard']\n",
    "\n",
    "        training_costs = np.zeros(num_epochs, dtype=np.float32)\n",
    "        validation_costs = np.zeros(num_epochs, dtype=np.float32)\n",
    "        validation_measures = np.zeros(num_epochs, dtype=np.float32)\n",
    "        min_cost = np.inf\n",
    "        no_cost_decrease_epochs = 0\n",
    "\n",
    "        if rescale_factor is None :\n",
    "            minx, maxx, miny, maxy = None, None, None, None\n",
    "        else :\n",
    "            minx, maxx, miny, maxy = rescale_factor['minx'], rescale_factor['maxx'], rescale_factor['miny'], rescale_factor['maxy']\n",
    "            \n",
    "        X, minx, maxx = scale_fn(X, minv=minx, maxv=maxx) # rescale X\n",
    "        Y, miny, maxy = scale_fn(Y, minv=miny, maxv=maxy) # rescale Y\n",
    "        \n",
    "        n_samples = X.shape[0]\n",
    "        n_features = X.shape[1]\n",
    "        n_output = Y.shape[1]\n",
    "        n_train = int(n_samples * (1 - vset_percent))\n",
    "        n_validate = n_samples - n_train\n",
    "\n",
    "        batch_loop = (n_train - 1) // batch_size + 1\n",
    "\n",
    "        sess = self.session\n",
    "        if use_tboard :\n",
    "            merged_summary = tf.summary.merge_all()\n",
    "            writer = tf.summary.FileWriter(\"./tboard_logs\")\n",
    "            writer.add_graph(sess.graph)  # Show the graph\n",
    "        else :\n",
    "            merged_summary = None\n",
    "\n",
    "        train_X = X[:n_train]\n",
    "        train_Y = Y[:n_train]\n",
    "        validate_X = X[n_train:]\n",
    "        validate_Y = Y[n_train:]\n",
    "\n",
    "        if shuffle_samples_epochs > 0 :\n",
    "            current_XY = np.hstack((X, Y))\n",
    "\n",
    "        start_time = dt.datetime.now()\n",
    "        print('Learning starts. It will take some time...', start_time)\n",
    "        for epoch in range(num_epochs):\n",
    "            shuffle_samples = shuffle_samples_epochs > 0 and epoch % shuffle_samples_epochs == 0 # shuffle on 0th epoch\n",
    "            \n",
    "            if shuffle_samples :\n",
    "                np.random.shuffle(current_XY) # this will shuffle current_XY in place.\n",
    "                _, shuffled_X, shuffled_Y = np.split(current_XY, (0, n_features), axis=-1)\n",
    "                train_X = shuffled_X[:n_train]\n",
    "                train_Y = shuffled_Y[:n_train]\n",
    "                validate_X = shuffled_X[n_train:]\n",
    "                validate_Y = shuffled_Y[n_train:]\n",
    "\n",
    "            epoch_hyps = np.zeros(Y.shape, dtype=np.float32)\n",
    "            epoch_costs = np.zeros(batch_loop, dtype=np.float32)\n",
    "\n",
    "            for m in range(batch_loop) :\n",
    "                if m == batch_loop - 1 :\n",
    "                    m_X = train_X[batch_size * m :]\n",
    "                    m_Y = train_Y[batch_size * m :]\n",
    "                else :\n",
    "                    m_X = train_X[batch_size * m : batch_size * (m + 1)]\n",
    "                    m_Y = train_Y[batch_size * m : batch_size * (m + 1)]\n",
    "\n",
    "                feed_dict = {self.X:m_X, self.Y:m_Y,\n",
    "                             self.p_keep_prob:keep_prob,\n",
    "                             self.p_keep_prob_input:keep_prob_input,\n",
    "                             self.p_lr:learning_rate,\n",
    "                             self.p_training:True}\n",
    "                targets = [ self.hypothesis, self.cost, self.objective_tensor ]\n",
    "                if use_tboard :\n",
    "                    targets.append(merged_summary)\n",
    "                # print('m:', m, ', m_X:', np.shape(m_X), ', m_Y:', np.shape(m_Y), ', feed_dict:', feed_dict)\n",
    "                results = sess.run(targets, feed_dict = feed_dict)\n",
    "                if use_tboard :\n",
    "                    writer.add_summary(results[-1], global_step = epoch * batch_loop + m)\n",
    "\n",
    "                h_value = results[0]\n",
    "                epoch_hyps[batch_size * m : batch_size * m + m_Y.shape[0]] = h_value\n",
    "                cost_value = results[1]\n",
    "                epoch_costs[m] = cost_value\n",
    "\n",
    "            training_costs[epoch] = avg_cost = np.mean(epoch_costs)\n",
    "\n",
    "            validate_feed_dict = {self.X: validate_X, self.Y: validate_Y,\n",
    "                                  self.p_keep_prob:1.0, self.p_keep_prob_input:1.0, self.p_training:False}\n",
    "            validate_targets = [ self.hypothesis, self.cost, self.measure ]\n",
    "            vs_hyps, vs_cost, vs_measure = sess.run(validate_targets, feed_dict=validate_feed_dict)\n",
    "            validation_costs[epoch] = vs_cost\n",
    "            validation_measures[epoch] = vs_measure\n",
    "\n",
    "            if epoch % print_cost_interval == 0 or epoch == num_epochs - 1:\n",
    "                print('Epoch:', '%04d' % epoch, 'average training cost =', '{:.9f}'.format(avg_cost),\n",
    "                      'validation cost =', '{:.9f}'.format(vs_cost), 'validation measure =', '{:.9f}'.format(vs_measure), dt.datetime.now())\n",
    "\n",
    "            if epoch % check_accuracy_epochs == check_accuracy_epochs :\n",
    "                print('Epoch:', '%04d' % epoch, 'average training cost =', '{:.9f}'.format(avg_cost),\n",
    "                      'validation cost =', '{:.9f}'.format(vs_cost),\n",
    "                      'validation measure =', '{:.9f}'.format(vs_measure), dt.datetime.now())\n",
    "\n",
    "                if break_accuracy > 0 and break_accuracy >= vs_measure :\n",
    "                    print('Stops the training due to high validation measure', vs_measure, ' exceeded the criteria', break_accuracy)\n",
    "                    training_costs = training_costs[:epoch + 1] # strip un-run epochs\n",
    "                    validation_costs = validation_costs[:epoch + 1] # strip un-run epochs\n",
    "                    validation_measures = validation_measures[:epoch + 1] # strip un-run epochs\n",
    "                    break\n",
    "\n",
    "            if early_stopping_epoch_on_max_no_decrease > 0 :\n",
    "                if vs_cost < min_cost :\n",
    "                    min_cost = vs_cost\n",
    "                    no_cost_decrease_epochs = 0\n",
    "                else :\n",
    "                    no_cost_decrease_epochs = no_cost_decrease_epochs + 1\n",
    "                    if no_cost_decrease_epochs >= early_stopping_epoch_on_max_no_decrease :\n",
    "                        print('Epoch:', '%04d' % epoch, 'average training cost =', '{:.9f}'.format(avg_cost),\n",
    "                              'validation cost =', '{:.9f}'.format(vs_cost),\n",
    "                              'validation measure =', '{:.9f}'.format(vs_measure), dt.datetime.now())\n",
    "                        # FIXME : in reality, i need to restore variables saved when it was not decreasing but i do not. maybe in the future ..\n",
    "                        print('Stops the training since cost is not reduced during ', no_cost_decrease_epochs, ' epochs.')\n",
    "                        training_costs = training_costs[:epoch + 1] # strip un-run epochs\n",
    "                        validation_costs = validation_costs[:epoch + 1] # strip un-run epochs\n",
    "                        validation_measures = validation_measures[:epoch + 1] # strip un-run epochs\n",
    "                        break\n",
    "\n",
    "        end_time = dt.datetime.now()\n",
    "        print('Training(learning) Finished!', end_time)\n",
    "        print('Training took ', '%10d' % ((end_time - start_time).total_seconds()),\n",
    "              ' seconds.')\n",
    "   \n",
    "        rescale_factor = { 'minx':minx, 'maxx':maxx, 'miny':miny, 'maxy':maxy }\n",
    "        return training_costs, validation_costs, validation_measures, rescale_factor               \n",
    "\n",
    "    def test(self, X, Y, rescale_factor=None, scale_fn=scale_minmax, descale_fn=descale_minmax) :\n",
    "        start_time = dt.datetime.now()\n",
    "        g = tf.get_default_graph()\n",
    "\n",
    "        if rescale_factor is not None :\n",
    "            X, _, _ = scale_fn(X, rescale_factor['minx'], rescale_factor['maxx'])\n",
    "            Y, _, _ = scale_fn(Y, rescale_factor['miny'], rescale_factor['maxy'])\n",
    "            \n",
    "        with g.as_default() :\n",
    "            hyps, cost, measure = self._test_model(X, Y)\n",
    "            if rescale_factor is not None :\n",
    "                hyps = descale_fn(hyps, rescale_factor['miny'], rescale_factor['maxy'])\n",
    "            end_time = dt.datetime.now()\n",
    "            print('Prediction took ', '%10d' % ((end_time - start_time).total_seconds()),\n",
    "                  ' seconds.')\n",
    "            print('Started at ', start_time, ' and finished at ', end_time)\n",
    "            return hyps, cost, measure\n",
    "\n",
    "    def _test_model(self, X, Y) :\n",
    "        test_feed_dict = {self.X: X, self.Y: Y,\n",
    "                          self.p_keep_prob:1.0, self.p_keep_prob_input:1.0, self.p_training:False}\n",
    "        test_targets = [ self.hypothesis, self.cost, self.measure ]\n",
    "\n",
    "        sess = self.session\n",
    "        hyps, cost, measure = sess.run(test_targets, feed_dict=test_feed_dict)\n",
    "        return hyps, cost, measure\n",
    "\n",
    "    def infer(self, X, rescale_factor=None, scale_fn=scale_minmax, descale_fn=descale_minmax) :\n",
    "        g = tf.get_default_graph()\n",
    "\n",
    "        if rescale_factor is not None :\n",
    "            X, _, _ = scale_fn(X, rescale_factor['minx'], rescale_factor['maxx'])\n",
    "        \n",
    "        with g.as_default() :\n",
    "            hyps = self._infer_model(X)\n",
    "            if rescale_factor is not None :\n",
    "                hyps = descale_fn(hyps, rescale_factor['miny'], rescale_factor['maxy'])\n",
    "                \n",
    "            return hyps\n",
    "\n",
    "    def _infer_model(self, X) :\n",
    "        test_feed_dict = {self.X: X,\n",
    "                          self.p_keep_prob:1.0, self.p_keep_prob_input:1.0, self.p_training:False}\n",
    "        test_targets = [ self.hypothesis ]\n",
    "\n",
    "        sess = self.session\n",
    "        hyps = sess.run(test_targets, feed_dict=test_feed_dict)\n",
    "        return hyps\n",
    "    \n",
    "    def r_squared(self, y, h) :\n",
    "        # in tf.reduce_mean, if axis has no entries, all dimensions are reduced, and a tensor with a single element is returned\n",
    "        total_error = tf.reduce_sum(tf.square(tf.subtract(y, tf.reduce_mean(y, 0))))  # reduce_mean by 0-axis maintains vector dimension\n",
    "        unexplained_error = tf.reduce_sum(tf.square(tf.subtract(y, h)))\n",
    "        r_squared = tf.subtract(1.0, tf.div(unexplained_error, total_error))\n",
    "        return r_squared\n",
    "\n",
    "    def smape(self, y, h) :\n",
    "        return tf.reduce_mean(2.0 * tf.abs(tf.subtract(y, h)) / tf.maximum(1e-7, (tf.abs(y) + tf.abs(h)))) # tf.maximum is used to avoid nan\n",
    "        \n",
    "    def check_nan(self, value) :\n",
    "        return value is None or math.isnan(value)\n",
    "\n",
    "    def save_model(self, save_file_name) :\n",
    "        # self._dump_graph('save_model(' + save_file_name + ')')\n",
    "        \n",
    "        tf.train.Saver().save(self.session, save_file_name)\n",
    "\n",
    "    def save_scale(self, save_model_name, rescale_factor) :\n",
    "        with open(save_model_name + '.scale.pkl', 'wb') as f :\n",
    "            pk.dump(rescale_factor, f, pk.HIGHEST_PROTOCOL)\n",
    "        \n",
    "    def read_scale(self, save_model_name) :\n",
    "        with open(save_model_name + '.scale.pkl', 'rb') as f :\n",
    "            rescale_factor = pk.load(f)\n",
    "            return rescale_factor\n",
    "        \n",
    "    def _dump_graph(self, where) :\n",
    "        print('')\n",
    "\n",
    "        print('--- dumping tensorflow graph [', where, '] ---')\n",
    "        g = tf.get_default_graph()\n",
    "        print('default tf graph :', g)\n",
    "\n",
    "        # debug graphs\n",
    "        keys = g.get_all_collection_keys()\n",
    "        print('current name scope :', g.get_name_scope())\n",
    "        for key in keys :\n",
    "            print('all graph (', key, ')  :', g.get_collection(key))\n",
    "        print('') \n",
    "        print('')\n",
    "\n",
    "       \n",
    "    def restore_model(self, saved_dir) :\n",
    "        print('saved dir:', saved_dir)\n",
    "\n",
    "        with self.session.as_default() :\n",
    "            # self._dump_graph('restore_model(' + saved_dir + ')')\n",
    "            \n",
    "            reader = tf.train.NewCheckpointReader(saved_dir)\n",
    "            # for var_name in reader.get_variable_to_shape_map() :\n",
    "            #     print(var_name)\n",
    "        \n",
    "            tf.train.Saver().restore(self.session, saved_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# util\n",
    "\n",
    "NO_RESCALE = { 'minx':-1, 'maxx':-1, 'miny':-1, 'maxy':-1 } \n",
    "RESCALE_X = { 'minx':None, 'maxx':None, 'miny':-1, 'maxy':-1 } \n",
    "RESCALE_XY = None\n",
    "\n",
    "TEST_PERCENT = 0.2\n",
    "\n",
    "def shuffle_XY(X, Y) :\n",
    "    hstacked = np.hstack((X, Y))\n",
    "    np.random.shuffle(hstacked)\n",
    "    _, new_X, new_Y = np.split(hstacked, (0, X.shape[1]), axis=-1)\n",
    "    return new_X, new_Y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def train_gym(gym_env, num_epochs=None, no_early_stop=False, retrain=False, no_rescale=False) :\n",
    "    gym_env_model = 'model_' + gym_env\n",
    "    train_config = default_train_config.copy()\n",
    "    if num_epochs is not None :\n",
    "        train_config['num_epochs'] = num_epochs\n",
    "    if no_early_stop :\n",
    "        train_config['early_stopping_epoch_on_max_no_decrease'] = -1\n",
    "    \n",
    "    print('starting ', gym_env, dt.datetime.now())\n",
    "    observations, actions = load_expert_data(gym_env)\n",
    "    obs_shape, action_shape = np.shape(observations), np.shape(actions)\n",
    "    if action_shape[1] == 1 :\n",
    "        actions = np.reshape(actions, (action_shape[0], action_shape[2]))\n",
    "        action_shape = np.shape(actions)\n",
    "    print(gym_env, ' observation shape: ', obs_shape, ', actions shape:', action_shape)\n",
    "\n",
    "    if retrain :\n",
    "        cloning = BehavioralCloning(X_shape=obs_shape, Y_shape=action_shape, scope_name=gym_env, restore_mode=True, session=None)\n",
    "        cloning.restore_model(gym_env_model)\n",
    "        rescale_factor = cloning.read_scale(gym_env_model)\n",
    "    else :\n",
    "        cloning = BehavioralCloning(X_shape=obs_shape, Y_shape=action_shape, scope_name=gym_env)\n",
    "        if no_rescale :\n",
    "            rescale_factor = NO_RESCALE\n",
    "        else :\n",
    "            rescale_factor = RESCALE_X # do not rescale Y\n",
    "            # rescale_factor = RESCALE_XY # rescale both X and Y\n",
    "    \n",
    "    n_samples = observations.shape[0]\n",
    "    n_train = int(n_samples * (1 - TEST_PERCENT))\n",
    "\n",
    "    observations, actions = shuffle_XY(observations, actions)\n",
    "    training_costs, validation_costs, validation_measures, rescale_factor = cloning.train(observations[:n_train], actions[:n_train],\n",
    "                                                                                          train_config=train_config, rescale_factor=rescale_factor,\n",
    "                                                                                          scale_fn=scale_signed)\n",
    "    \n",
    "    cloning.save_model(gym_env_model)\n",
    "    if not retrain :\n",
    "        cloning.save_scale(gym_env_model, rescale_factor)\n",
    "    \n",
    "    cloning = BehavioralCloning(X_shape=obs_shape, Y_shape=action_shape, scope_name=gym_env, restore_mode=True)\n",
    "    cloning.restore_model(gym_env_model)\n",
    "    \n",
    "    test_hyps, test_costs, test_measures = cloning.test(observations[n_train:], actions[n_train:],\n",
    "                                                        rescale_factor=rescale_factor, scale_fn=scale_signed, descale_fn=descale_signed)\n",
    "    print('ending ', gym_env, dt.datetime.now())\n",
    "\n",
    "def retrain_gym(gym_env, num_epochs=None, no_early_stop=False) :\n",
    "    train_gym(gym_env, num_epochs=num_epochs, no_early_stop=no_early_stop, retrain=True)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting  HalfCheetah-v2 2019-07-20 19:16:00.466183\n",
      "HalfCheetah-v2  observation shape:  (500000, 17) , actions shape: (500000, 6)\n",
      "Learning starts. It will take some time... 2019-07-20 19:16:02.603910\n",
      "Epoch: 0000 average training cost = 0.579295337 validation cost = 0.385019094 validation measure = 0.295401573 2019-07-20 19:16:15.728855\n",
      "Epoch: 0500 average training cost = 0.149796918 validation cost = 0.092154197 validation measure = 0.831848323 2019-07-20 21:07:56.335238\n",
      "Epoch: 1000 average training cost = 0.127625644 validation cost = 0.071496472 validation measure = 0.869776368 2019-07-20 23:00:05.789216\n",
      "Epoch: 1500 average training cost = 0.117151290 validation cost = 0.062932454 validation measure = 0.884375155 2019-07-21 00:52:16.901180\n",
      "Epoch: 1999 average training cost = 0.110436104 validation cost = 0.058209136 validation measure = 0.893877625 2019-07-21 02:44:49.686552\n",
      "Training(learning) Finished! 2019-07-21 02:44:49.687549\n",
      "Training took       26927  seconds.\n",
      "saved dir: model_HalfCheetah-v2\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-21 02:44:50.141288  and finished at  2019-07-21 02:44:50.189206\n",
      "ending  HalfCheetah-v2 2019-07-21 02:44:50.190202\n",
      "starting  Humanoid-v2 2019-07-21 02:44:50.200175\n",
      "Humanoid-v2  observation shape:  (499084, 376) , actions shape: (499084, 17)\n",
      "Learning starts. It will take some time... 2019-07-21 02:44:55.777256\n",
      "Epoch: 0000 average training cost = 1.939667344 validation cost = 0.800780535 validation measure = 0.162512720 2019-07-21 02:45:10.644594\n",
      "Epoch: 0500 average training cost = 0.241340801 validation cost = 0.143940061 validation measure = 0.849391341 2019-07-21 04:41:38.743643\n",
      "Epoch: 1000 average training cost = 0.211228043 validation cost = 0.127307609 validation measure = 0.866923988 2019-07-21 06:37:41.615975\n",
      "Epoch: 1500 average training cost = 0.197811127 validation cost = 0.117663153 validation measure = 0.877014220 2019-07-21 08:32:21.295116\n",
      "Epoch: 2000 average training cost = 0.190314174 validation cost = 0.111867473 validation measure = 0.883170366 2019-07-21 10:28:05.179174\n",
      "Epoch: 2500 average training cost = 0.184837803 validation cost = 0.107370839 validation measure = 0.887741327 2019-07-21 12:24:09.918238\n",
      "Epoch: 2999 average training cost = 0.181144282 validation cost = 0.104541652 validation measure = 0.890886843 2019-07-21 14:19:31.597932\n",
      "Training(learning) Finished! 2019-07-21 14:19:31.597932\n",
      "Training took       41675  seconds.\n",
      "saved dir: model_Humanoid-v2\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-21 14:19:32.315807  and finished at  2019-07-21 14:19:32.650958\n",
      "ending  Humanoid-v2 2019-07-21 14:19:32.681877\n",
      "starting  Walker2d-v2 2019-07-21 14:19:32.851426\n",
      "Walker2d-v2  observation shape:  (499696, 17) , actions shape: (499696, 6)\n",
      "Learning starts. It will take some time... 2019-07-21 14:19:34.045028\n",
      "Epoch: 0000 average training cost = 1.439791679 validation cost = 0.999660373 validation measure = -0.005192637 2019-07-21 14:19:47.024190\n",
      "Epoch: 0500 average training cost = 0.407180548 validation cost = 0.266646564 validation measure = 0.731301308 2019-07-21 16:06:26.802144\n",
      "Epoch: 1000 average training cost = 0.374404579 validation cost = 0.243725881 validation measure = 0.754683495 2019-07-21 17:53:07.415894\n",
      "Epoch: 1500 average training cost = 0.353913575 validation cost = 0.227282763 validation measure = 0.770932078 2019-07-21 19:38:18.488968\n",
      "Epoch: 2000 average training cost = 0.338408619 validation cost = 0.213384986 validation measure = 0.785253406 2019-07-21 21:24:43.503739\n",
      "Epoch: 2500 average training cost = 0.327048004 validation cost = 0.201031268 validation measure = 0.797031879 2019-07-21 23:10:24.386528\n",
      "Epoch: 2999 average training cost = 0.319501847 validation cost = 0.191953659 validation measure = 0.806142449 2019-07-22 00:56:33.638083\n",
      "Training(learning) Finished! 2019-07-22 00:56:33.639080\n",
      "Training took       38219  seconds.\n",
      "saved dir: model_Walker2d-v2\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-22 00:56:34.145678  and finished at  2019-07-22 00:56:34.190560\n",
      "ending  Walker2d-v2 2019-07-22 00:56:34.192600\n"
     ]
    }
   ],
   "source": [
    "# train behavior cloning policies\n",
    "\n",
    "for gym_env in gym_envs :\n",
    "    train_gym(gym_env, num_epochs=num_epochs_per_envs[gym_env], no_early_stop=False, no_rescale=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gym_env in [ 'Reacher-v2', 'Ant-v2', 'Walker2d-v2'] :\n",
    "    retrain_gym(gym_env, num_epochs=num_epochs_per_envs[gym_env], no_early_stop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "# run_expert source code for reference\n",
    "\n",
    "import tf_util\n",
    "import pickle as pk\n",
    "import traceback\n",
    "\n",
    "def load_learned_policy_fn(gym_env, session=None, dagger=False) :\n",
    "    observations, actions = load_expert_data(gym_env)\n",
    "    obs_shape, action_shape = list(np.shape(observations)), list(np.shape(actions))\n",
    "    if action_shape[1] == 1 :\n",
    "        actions = np.reshape(actions, (action_shape[0], action_shape[2]))\n",
    "        action_shape = list(np.shape(actions))\n",
    "\n",
    "    gym_env_model = 'model_' + gym_env\n",
    "    gym_env_dagger_model = 'model_dagger_' + gym_env\n",
    "    \n",
    "    cloning = BehavioralCloning(X_shape=obs_shape, Y_shape=action_shape, scope_name=gym_env, restore_mode=True, session=session)\n",
    "    cloning.restore_model(gym_env_model if not dagger else gym_env_dagger_model)\n",
    "    rescale_factor = cloning.read_scale(gym_env_model)\n",
    "\n",
    "    return lambda x : cloning.infer(x, rescale_factor=rescale_factor, scale_fn=scale_signed, descale_fn=descale_signed)\n",
    "    \n",
    "def rollout_by_policy(gym_env, max_timesteps, num_rollouts, policy_fn=None, render=False) :\n",
    "    policy_type = 'learned'\n",
    "    \n",
    "    if policy_fn is None : # default policy_fn is expert policy\n",
    "        print('loading and building expert policy')\n",
    "        policy_fn = load_expert_policy_fn(gym_env)\n",
    "        print('loaded and built')\n",
    "        policy_type = 'expert'\n",
    "\n",
    "    with tf.Session():\n",
    "        tf_util.initialize()\n",
    "\n",
    "        import gym\n",
    "        env = gym.make(gym_env)\n",
    "        max_steps = max_timesteps or env.spec.timestep_limit\n",
    "\n",
    "        returns = []\n",
    "        observations = []\n",
    "        actions = []\n",
    "        for i in range(num_rollouts):\n",
    "            # print('iter', i)\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            totalr = 0.\n",
    "            steps = 0\n",
    "            while not done:\n",
    "                action = policy_fn(obs[None,:])\n",
    "                observations.append(obs)\n",
    "                actions.append(action)\n",
    "                try :\n",
    "                    if np.shape(action)[1] == 1 :\n",
    "                        action_shape = np.shape(action)\n",
    "                        action = np.reshape(action, (action_shape[0], action_shape[2]))\n",
    "                    obs, r, done, _ = env.step(action) # observation, reward, done\n",
    "                except ValueError as e :\n",
    "                    print('action:', action, ', shape:', np.shape(action), ', policy_type:', policy_type)\n",
    "                    print('actions:', actions, ', shape:', np.shape(actions))\n",
    "                    traceback.print_exc()     \n",
    "\n",
    "                totalr += r\n",
    "                steps += 1\n",
    "                if render:\n",
    "                    env.render()\n",
    "                # if steps % 100 == 0: print(\"%i/%i\"%(steps, max_steps))\n",
    "                if steps >= max_steps:\n",
    "                    break\n",
    "            returns.append(totalr)\n",
    "\n",
    "        print('shape of returns', np.shape(returns))\n",
    "        print('mean return', np.mean(returns))\n",
    "        print('std of return', np.std(returns))\n",
    "\n",
    "        rollout_data = {'observations': np.array(observations),\n",
    "                        'actions': np.array(actions),\n",
    "                        'returns': np.array(returns)}\n",
    "\n",
    "        if not os.path.exists('rollout_data') :\n",
    "            os.mkdir('rollout_data')\n",
    "        with open(os.path.join('rollout_data', policy_type + '-' + gym_env + '.pkl'), 'wb') as f:\n",
    "            pk.dump(rollout_data, f, pk.HIGHEST_PROTOCOL)\n",
    "            \n",
    "        return rollout_data, policy_type, env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0723 19:38:12.873242 69560 deprecation_wrapper.py:119] From C:\\Exception\\Works\\GitHub\\cs294\\yoonforh\\hw1\\load_policy.py:55: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0723 19:38:13.030773 69560 deprecation.py:323] From C:\\Exception\\Works\\GitHub\\cs294\\yoonforh\\hw1\\tf_util.py:91: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      "W0723 19:38:13.031772 69560 deprecation_wrapper.py:119] From C:\\Exception\\Works\\GitHub\\cs294\\yoonforh\\hw1\\tf_util.py:74: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0723 19:38:13.032768 69560 deprecation.py:323] From C:\\Works\\tensorflow\\lib\\site-packages\\tensorflow\\python\\util\\tf_should_use.py:193: initialize_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.variables_initializer` instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and building expert policy\n",
      "obs (1, 111) (1, 111)\n",
      "loaded and built\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Works\\tensorflow\\lib\\site-packages\\gym\\envs\\registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "shape of returns (500,)\n",
      "mean return 4792.043355892032\n",
      "std of return 260.3806693532324\n",
      "Rollout result. env: Ant-v2 , policy_type: expert , returns: 500 / 4792.043355892032 / 260.3806693532324\n",
      "It took        1424  seconds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Works\\tensorflow\\lib\\site-packages\\tensorflow\\python\\client\\session.py:1735: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved dir: model_Ant-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "shape of returns (500,)\n",
      "mean return 1509.1139724415493\n",
      "std of return 104.04023142009902\n",
      "Rollout result. env: Ant-v2 , policy_type: learned , returns: 500 / 1509.1139724415493 / 104.04023142009902\n",
      "It took        1568  seconds.\n",
      "loading and building expert policy\n",
      "obs (1, 11) (1, 11)\n",
      "loaded and built\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "shape of returns (500,)\n",
      "mean return 3778.0636147566383\n",
      "std of return 3.640208992602978\n",
      "Rollout result. env: Hopper-v2 , policy_type: expert , returns: 500 / 3778.0636147566383 / 3.640208992602978\n",
      "It took         941  seconds.\n",
      "saved dir: model_Hopper-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "shape of returns (500,)\n",
      "mean return 788.5323133820356\n",
      "std of return 335.40078296145805\n",
      "Rollout result. env: Hopper-v2 , policy_type: learned , returns: 500 / 788.5323133820356 / 335.40078296145805\n",
      "It took         322  seconds.\n",
      "loading and building expert policy\n",
      "obs (1, 11) (1, 11)\n",
      "loaded and built\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "shape of returns (500,)\n",
      "mean return -3.8339448712814757\n",
      "std of return 1.632587730217158\n",
      "Rollout result. env: Reacher-v2 , policy_type: expert , returns: 500 / -3.8339448712814757 / 1.632587730217158\n",
      "It took          39  seconds.\n",
      "saved dir: model_Reacher-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "shape of returns (500,)\n",
      "mean return -10.479361739432477\n",
      "std of return 4.560072994340788\n",
      "Rollout result. env: Reacher-v2 , policy_type: learned , returns: 500 / -10.479361739432477 / 4.560072994340788\n",
      "It took          46  seconds.\n",
      "loading and building expert policy\n",
      "obs (1, 17) (1, 17)\n",
      "loaded and built\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "shape of returns (500,)\n",
      "mean return 4142.096107478104\n",
      "std of return 81.87691057606041\n",
      "Rollout result. env: HalfCheetah-v2 , policy_type: expert , returns: 500 / 4142.096107478104 / 81.87691057606041\n",
      "It took         923  seconds.\n",
      "saved dir: model_HalfCheetah-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "shape of returns (500,)\n",
      "mean return 2628.466741500138\n",
      "std of return 195.23761097563758\n",
      "Rollout result. env: HalfCheetah-v2 , policy_type: learned , returns: 500 / 2628.466741500138 / 195.23761097563758\n",
      "It took         945  seconds.\n",
      "loading and building expert policy\n",
      "obs (1, 376) (1, 376)\n",
      "loaded and built\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "shape of returns (500,)\n",
      "mean return 10366.597294554245\n",
      "std of return 565.231564879782\n",
      "Rollout result. env: Humanoid-v2 , policy_type: expert , returns: 500 / 10366.597294554245 / 565.231564879782\n",
      "It took        1505  seconds.\n",
      "saved dir: model_Humanoid-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "shape of returns (500,)\n",
      "mean return 295.17825485377\n",
      "std of return 24.592578322669173\n",
      "Rollout result. env: Humanoid-v2 , policy_type: learned , returns: 500 / 295.17825485377 / 24.592578322669173\n",
      "It took         111  seconds.\n",
      "loading and building expert policy\n",
      "obs (1, 17) (1, 17)\n",
      "loaded and built\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "shape of returns (500,)\n",
      "mean return 5501.336432745725\n",
      "std of return 260.06738260270674\n",
      "Rollout result. env: Walker2d-v2 , policy_type: expert , returns: 500 / 5501.336432745725 / 260.06738260270674\n",
      "It took        1040  seconds.\n",
      "saved dir: model_Walker2d-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "shape of returns (500,)\n",
      "mean return 799.4722525447779\n",
      "std of return 493.23200069438747\n",
      "Rollout result. env: Walker2d-v2 , policy_type: learned , returns: 500 / 799.4722525447779 / 493.23200069438747\n",
      "It took         589  seconds.\n"
     ]
    }
   ],
   "source": [
    "# rollout and check\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(columns=['gymenv', 'policy_type', 'rollouts', 'mean', 'std'])\n",
    "\n",
    "for gym_env in gym_envs :\n",
    "    for expert_policy in [ True, False] :\n",
    "        start_time = dt.datetime.now()\n",
    "        rollout_data, policy_type, _ = rollout_by_policy(gym_env, MAX_TIMESTEPS, NUM_ROLLOUTS,\n",
    "                                                      policy_fn=None if expert_policy else load_learned_policy_fn(gym_env),\n",
    "                                                      render=False)\n",
    "        returns = rollout_data['returns']\n",
    "        end_time = dt.datetime.now()\n",
    "        print('Rollout result. env:', gym_env, ', policy_type:', policy_type, ', returns:', len(returns), '/', np.mean(returns), '/', np.std(returns))\n",
    "        df = df.append({'gymenv':gym_env, 'policy_type':policy_type, 'rollouts': len(returns), 'mean': np.mean(returns), 'std': np.std(returns)}, ignore_index=True)\n",
    "        print('It took ', '%10d' % ((end_time - start_time).total_seconds()), ' seconds.')\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gymenv</th>\n",
       "      <th>policy_type</th>\n",
       "      <th>rollouts</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ant-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>500</td>\n",
       "      <td>4792.043356</td>\n",
       "      <td>260.380669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ant-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>500</td>\n",
       "      <td>1509.113972</td>\n",
       "      <td>104.040231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hopper-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>500</td>\n",
       "      <td>3778.063615</td>\n",
       "      <td>3.640209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hopper-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>500</td>\n",
       "      <td>788.532313</td>\n",
       "      <td>335.400783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reacher-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>500</td>\n",
       "      <td>-3.833945</td>\n",
       "      <td>1.632588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reacher-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>500</td>\n",
       "      <td>-10.479362</td>\n",
       "      <td>4.560073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HalfCheetah-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>500</td>\n",
       "      <td>4142.096107</td>\n",
       "      <td>81.876911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HalfCheetah-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>500</td>\n",
       "      <td>2628.466742</td>\n",
       "      <td>195.237611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Humanoid-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>500</td>\n",
       "      <td>10366.597295</td>\n",
       "      <td>565.231565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Humanoid-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>500</td>\n",
       "      <td>295.178255</td>\n",
       "      <td>24.592578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Walker2d-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>500</td>\n",
       "      <td>5501.336433</td>\n",
       "      <td>260.067383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Walker2d-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>500</td>\n",
       "      <td>799.472253</td>\n",
       "      <td>493.232001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            gymenv policy_type rollouts          mean         std\n",
       "0           Ant-v2      expert      500   4792.043356  260.380669\n",
       "1           Ant-v2     learned      500   1509.113972  104.040231\n",
       "2        Hopper-v2      expert      500   3778.063615    3.640209\n",
       "3        Hopper-v2     learned      500    788.532313  335.400783\n",
       "4       Reacher-v2      expert      500     -3.833945    1.632588\n",
       "5       Reacher-v2     learned      500    -10.479362    4.560073\n",
       "6   HalfCheetah-v2      expert      500   4142.096107   81.876911\n",
       "7   HalfCheetah-v2     learned      500   2628.466742  195.237611\n",
       "8      Humanoid-v2      expert      500  10366.597295  565.231565\n",
       "9      Humanoid-v2     learned      500    295.178255   24.592578\n",
       "10     Walker2d-v2      expert      500   5501.336433  260.067383\n",
       "11     Walker2d-v2     learned      500    799.472253  493.232001"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "## DAgger\n",
    "\n",
    "1. Implement DAgger. See the code provided in run expert.py to see how to query the expert policy and perform roll-outs in the environment.\n",
    "\n",
    "2. Run DAgger and report results on one task in which DAgger can learn a better policy than behavioral cloning.\n",
    "Report your results in the form of a learning curve, plotting the number of DAgger iterations vs. the policy’s mean return,\n",
    "with error bars to show the standard deviation.\n",
    "\n",
    "Include the performance of the expert policy and the behavioral cloning agent on the same plot.\n",
    "In the caption, state which task you used, and any details regarding network architecture, amount of data, etc. (as in the previous section).\n",
    "\n",
    "### note\n",
    "1. DAgger needs labeling by human experts.\n",
    "1. The main idea is that the trajectories are collected by the learned policy. but the action is relabeled by the expert policy.\n",
    "1. DAgger addresses the problem of distributional “drift”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "def rollout_by_dagger(gym_env, max_timesteps, num_rollouts, num_epochs=50, render=False) :\n",
    "    policy_type = 'dagger'\n",
    "    \n",
    "    print('loading and building learned policy')\n",
    "    policy_fn = load_learned_policy_fn(gym_env)\n",
    "    print('loaded and built')\n",
    "\n",
    "    print('starting dagger ', gym_env, dt.datetime.now())\n",
    "    train_observations, train_actions = load_expert_data(gym_env)\n",
    "    obs_shape, action_shape = np.shape(train_observations), np.shape(train_actions)\n",
    "    if action_shape[1] == 1 :\n",
    "        train_actions = np.reshape(train_actions, (action_shape[0], action_shape[2]))\n",
    "        action_shape = np.shape(train_actions)\n",
    "    print(gym_env, ' observation shape: ', obs_shape, ', actions shape:', action_shape)\n",
    "    \n",
    "    with tf.Session(graph=tf.Graph()) as session, session.graph.as_default() : # for session nesting, the graphs should be isolated for each tf sessions\n",
    "        print('loading and building expert policy for DAgger')\n",
    "        expert_policy_fn = load_expert_policy_fn(gym_env)\n",
    "        print('loaded and built for DAgger')\n",
    "\n",
    "        tf_util.initialize()\n",
    "\n",
    "        gym_env_model = 'model_' + gym_env\n",
    "        gym_env_dagger_model = 'model_dagger_' + gym_env # new model file to save after lite training\n",
    "        light_train_config = default_train_config.copy()\n",
    "        light_train_config['num_epochs'] = num_epochs\n",
    "        light_train_config['early_stopping_epoch_on_max_no_decrease'] = -1\n",
    "        cloning_model = None\n",
    "\n",
    "        env = gym.make(gym_env)\n",
    "        max_steps = max_timesteps or env.spec.timestep_limit\n",
    "\n",
    "        returns = []\n",
    "        observations = []\n",
    "        actions = []\n",
    "\n",
    "        for i in range(num_rollouts):\n",
    "            # print('iter', i)\n",
    "            obs = env.reset()\n",
    "            done = False\n",
    "            totalr = 0.\n",
    "            steps = 0\n",
    "\n",
    "            while not done:\n",
    "                action = policy_fn(obs[None,:])\n",
    "                # print('before append>>>> observations shape:', np.shape(observations), ', train_observations shape:', np.shape(train_observations), ', obs shape:', np.shape(obs))\n",
    "                observations.append(obs)\n",
    "                train_observations = np.append(train_observations, obs[None, :], axis=0)\n",
    "                # print('after append>>>> observations shape:', np.shape(observations), ', train_observations shape:', np.shape(train_observations), ', obs shape:', np.shape(obs))\n",
    "\n",
    "                expert_action = expert_policy_fn(obs[None,:]) # None makes additional dimension. to reduce, use np.hstack\n",
    "                actions.append(expert_action)\n",
    "                # print('before append>>>> actions shape:', np.shape(actions), ', train_actions shape:', np.shape(train_actions), ', expert_action shape:', np.shape(expert_action))\n",
    "                train_actions = np.append(train_actions, expert_action, axis=0)\n",
    "                # print('after append>>>> actions shape:', np.shape(actions), ', train_actions shape:', np.shape(train_actions), ', expert_action shape:', np.shape(expert_action))\n",
    "\n",
    "                try :\n",
    "                    if np.shape(action)[1] == 1 :\n",
    "                        action_shape = np.shape(action)\n",
    "                        action = np.reshape(action, (action_shape[0], action_shape[2]))\n",
    "                        action_shape = np.shape(action)\n",
    "                    obs, r, done, _ = env.step(action) # observation, reward, done\n",
    "                except ValueError as e :\n",
    "                    print('action:', action, ', shape:', np.shape(action), ', policy_type:', policy_type)\n",
    "                    print('actions:', actions, ', shape:', np.shape(actions))\n",
    "                    print('expert_action:', expert_action, ', shape:', np.shape(expert_action))\n",
    "                    traceback.print_exc()     \n",
    "\n",
    "                totalr += r\n",
    "                steps += 1\n",
    "                #if render: # this is just for dagger training\n",
    "                #    env.render()\n",
    "                # if steps % 100 == 0: print(\"%i/%i\"%(steps, max_steps))\n",
    "                if steps >= max_steps:\n",
    "                    break\n",
    "            returns.append(totalr)\n",
    "\n",
    "            # retrain on every new rollouts\n",
    "            n_samples = train_observations.shape[0]\n",
    "            n_train = int(n_samples * (1 - TEST_PERCENT))\n",
    "\n",
    "            print('train_observations shape:', train_observations.shape, ', train_actions shape:', train_actions.shape)\n",
    "            print('observations shape:', np.shape(observations), ', actions shape:', np.shape(actions))\n",
    "            try :\n",
    "                train_observations, train_actions = shuffle_XY(train_observations, train_actions)\n",
    "            except IndexError as e :\n",
    "                print('train_observations:', train_observations, ', train_actions:', train_actions)\n",
    "                traceback.print_exc() \n",
    "\n",
    "            print('train input : train_observations shape:', train_observations[:n_train].shape, ', train_actions shape:', train_actions[:n_train].shape)\n",
    "\n",
    "            with tf.Session(graph=tf.Graph()) as nested_session, nested_session.graph.as_default():\n",
    "                saved_model = gym_env_model if cloning_model is None else gym_env_dagger_model\n",
    "                cloning_model = BehavioralCloning(X_shape=obs_shape, Y_shape=action_shape,\n",
    "                                                  scope_name=gym_env, restore_mode=True, session=nested_session)\n",
    "                cloning_model.restore_model(saved_model)        \n",
    "                rescale_factor = cloning_model.read_scale(gym_env_model)\n",
    "                training_costs, validation_costs, validation_measures, rescale_factor = cloning_model.train(train_observations[:n_train], train_actions[:n_train],\n",
    "                                                                                                            train_config = light_train_config, rescale_factor = rescale_factor,\n",
    "                                                                                                            scale_fn=scale_signed)\n",
    "                cloning_model.save_model(gym_env_dagger_model)\n",
    "\n",
    "        with tf.Session(graph=tf.Graph()) as nested_session, nested_session.graph.as_default():\n",
    "            cloning = BehavioralCloning(X_shape=obs_shape, Y_shape=action_shape,\n",
    "                                        scope_name=gym_env, restore_mode=True, session=nested_session)\n",
    "            cloning.restore_model(gym_env_dagger_model)\n",
    "            rescale_factor = cloning_model.read_scale(gym_env_model)\n",
    "            test_hyps, test_costs, test_measures = cloning.test(train_observations[n_train:], train_actions[n_train:],\n",
    "                                                                rescale_factor = rescale_factor, scale_fn=scale_signed, descale_fn=descale_signed)\n",
    "\n",
    "        print('shape of returns', np.shape(returns))\n",
    "        print('mean return', np.mean(returns))\n",
    "        print('std of return', np.std(returns))\n",
    "\n",
    "        rollout_data = {'observations': np.array(observations),\n",
    "                        'actions': np.array(actions),\n",
    "                        'returns': np.array(returns)}\n",
    "\n",
    "        if not os.path.exists('rollout_data') :\n",
    "            os.mkdir('rollout_data')\n",
    "        with open(os.path.join('rollout_data', policy_type + '-' + gym_env + '.pkl'), 'wb') as f:\n",
    "            pk.dump(rollout_data, f, pk.HIGHEST_PROTOCOL)\n",
    "\n",
    "    rollout_data, policy_type, win_env = rollout_by_policy(gym_env, max_timesteps, num_rollouts,\n",
    "                                    policy_fn=load_learned_policy_fn(gym_env, dagger=True),\n",
    "                                    render=render)\n",
    "    return rollout_data, 'dagger', win_env\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and building learned policy\n",
      "saved dir: model_Ant-v2\n",
      "loaded and built\n",
      "starting dagger  Ant-v2 2019-07-24 14:57:11.272614\n",
      "Ant-v2  observation shape:  (496130, 111) , actions shape: (496130, 8)\n",
      "loading and building expert policy for DAgger\n",
      "obs (1, 111) (1, 111)\n",
      "loaded and built for DAgger\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "train_observations shape: (497130, 111) , train_actions shape: (497130, 8)\n",
      "observations shape: (1000, 111) , actions shape: (1000, 1, 8)\n",
      "train input : train_observations shape: (397704, 111) , train_actions shape: (397704, 8)\n",
      "saved dir: model_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-24 14:59:22.308587\n",
      "Epoch: 0000 average training cost = 0.026412237 validation cost = 0.017820382 validation measure = 0.768059015 2019-07-24 14:59:35.407298\n",
      "Epoch: 0049 average training cost = 0.026329933 validation cost = 0.017799372 validation measure = 0.768332481 2019-07-24 15:08:12.327821\n",
      "Training(learning) Finished! 2019-07-24 15:08:12.327821\n",
      "Training took         530  seconds.\n",
      "train_observations shape: (498130, 111) , train_actions shape: (498130, 8)\n",
      "observations shape: (2000, 111) , actions shape: (2000, 1, 8)\n",
      "train input : train_observations shape: (398504, 111) , train_actions shape: (398504, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-24 15:10:43.798310\n",
      "Epoch: 0000 average training cost = 0.026543226 validation cost = 0.017732093 validation measure = 0.768783569 2019-07-24 15:10:55.367627\n",
      "Epoch: 0049 average training cost = 0.026444914 validation cost = 0.017706761 validation measure = 0.769113898 2019-07-24 15:19:39.061880\n",
      "Training(learning) Finished! 2019-07-24 15:19:39.061880\n",
      "Training took         535  seconds.\n",
      "train_observations shape: (499130, 111) , train_actions shape: (499130, 8)\n",
      "observations shape: (3000, 111) , actions shape: (3000, 1, 8)\n",
      "train input : train_observations shape: (399304, 111) , train_actions shape: (399304, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-24 15:22:13.103442\n",
      "Epoch: 0000 average training cost = 0.026578609 validation cost = 0.017992446 validation measure = 0.766899943 2019-07-24 15:22:23.623831\n",
      "Epoch: 0049 average training cost = 0.026616132 validation cost = 0.017981237 validation measure = 0.767045140 2019-07-24 15:29:50.688680\n",
      "Training(learning) Finished! 2019-07-24 15:29:50.688680\n",
      "Training took         457  seconds.\n",
      "train_observations shape: (500130, 111) , train_actions shape: (500130, 8)\n",
      "observations shape: (4000, 111) , actions shape: (4000, 1, 8)\n",
      "train input : train_observations shape: (400104, 111) , train_actions shape: (400104, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-24 15:32:23.994254\n",
      "Epoch: 0000 average training cost = 0.026768539 validation cost = 0.018065117 validation measure = 0.765010715 2019-07-24 15:32:35.648032\n",
      "Epoch: 0049 average training cost = 0.026741669 validation cost = 0.018036174 validation measure = 0.765387237 2019-07-24 15:41:31.149026\n",
      "Training(learning) Finished! 2019-07-24 15:41:31.149026\n",
      "Training took         547  seconds.\n",
      "train_observations shape: (501130, 111) , train_actions shape: (501130, 8)\n",
      "observations shape: (5000, 111) , actions shape: (5000, 1, 8)\n",
      "train input : train_observations shape: (400904, 111) , train_actions shape: (400904, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-24 15:43:52.477467\n",
      "Epoch: 0000 average training cost = 0.026787674 validation cost = 0.018218549 validation measure = 0.764599562 2019-07-24 15:44:05.956723\n",
      "Epoch: 0049 average training cost = 0.026845118 validation cost = 0.018181188 validation measure = 0.765082300 2019-07-24 15:53:29.488382\n",
      "Training(learning) Finished! 2019-07-24 15:53:29.489377\n",
      "Training took         577  seconds.\n",
      "train_observations shape: (502130, 111) , train_actions shape: (502130, 8)\n",
      "observations shape: (6000, 111) , actions shape: (6000, 1, 8)\n",
      "train input : train_observations shape: (401704, 111) , train_actions shape: (401704, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-24 15:56:03.158560\n",
      "Epoch: 0000 average training cost = 0.027021881 validation cost = 0.018452335 validation measure = 0.762628794 2019-07-24 15:56:14.894887\n",
      "Epoch: 0049 average training cost = 0.026926370 validation cost = 0.018424582 validation measure = 0.762985826 2019-07-24 16:05:15.279900\n",
      "Training(learning) Finished! 2019-07-24 16:05:15.279900\n",
      "Training took         552  seconds.\n",
      "train_observations shape: (503130, 111) , train_actions shape: (503130, 8)\n",
      "observations shape: (7000, 111) , actions shape: (7000, 1, 8)\n",
      "train input : train_observations shape: (402504, 111) , train_actions shape: (402504, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-24 16:07:49.509239\n",
      "Epoch: 0000 average training cost = 0.027052810 validation cost = 0.018582687 validation measure = 0.761621594 2019-07-24 16:08:01.544749\n",
      "Epoch: 0049 average training cost = 0.027019098 validation cost = 0.018548302 validation measure = 0.762062669 2019-07-24 16:17:03.951609\n",
      "Training(learning) Finished! 2019-07-24 16:17:03.951609\n",
      "Training took         554  seconds.\n",
      "train_observations shape: (504130, 111) , train_actions shape: (504130, 8)\n",
      "observations shape: (8000, 111) , actions shape: (8000, 1, 8)\n",
      "train input : train_observations shape: (403304, 111) , train_actions shape: (403304, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-24 16:19:37.961637\n",
      "Epoch: 0000 average training cost = 0.027169799 validation cost = 0.018467028 validation measure = 0.762051702 2019-07-24 16:19:49.359744\n",
      "Epoch: 0049 average training cost = 0.027112056 validation cost = 0.018435147 validation measure = 0.762462497 2019-07-24 16:28:34.600359\n",
      "Training(learning) Finished! 2019-07-24 16:28:34.600359\n",
      "Training took         536  seconds.\n",
      "train_observations shape: (505130, 111) , train_actions shape: (505130, 8)\n",
      "observations shape: (9000, 111) , actions shape: (9000, 1, 8)\n",
      "train input : train_observations shape: (404104, 111) , train_actions shape: (404104, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-24 16:31:11.267878\n",
      "Epoch: 0000 average training cost = 0.027333461 validation cost = 0.018631395 validation measure = 0.760834873 2019-07-24 16:31:23.261647\n",
      "Epoch: 0049 average training cost = 0.027288338 validation cost = 0.018596858 validation measure = 0.761278212 2019-07-24 16:40:47.529672\n",
      "Training(learning) Finished! 2019-07-24 16:40:47.529672\n",
      "Training took         576  seconds.\n",
      "train_observations shape: (506130, 111) , train_actions shape: (506130, 8)\n",
      "observations shape: (10000, 111) , actions shape: (10000, 1, 8)\n",
      "train input : train_observations shape: (404904, 111) , train_actions shape: (404904, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-24 16:43:05.890669\n",
      "Epoch: 0000 average training cost = 0.027368126 validation cost = 0.018653588 validation measure = 0.759954691 2019-07-24 16:43:20.838041\n",
      "Epoch: 0049 average training cost = 0.027419236 validation cost = 0.018622003 validation measure = 0.760361135 2019-07-24 16:54:49.957732\n",
      "Training(learning) Finished! 2019-07-24 16:54:49.957732\n",
      "Training took         704  seconds.\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-24 16:54:50.520185  and finished at  2019-07-24 16:54:50.652876\n",
      "shape of returns (10,)\n",
      "mean return 1519.257422113195\n",
      "std of return 54.47586206953212\n",
      "saved dir: model_dagger_Ant-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "shape of returns (10,)\n",
      "mean return 1652.9598532966888\n",
      "std of return 329.3598967555054\n",
      "Rollout result. env: Ant-v2 , policy_type: dagger , returns: 10 / 1652.9598532966888 / 329.3598967555054\n",
      "It took        7097  seconds.\n",
      "loading and building learned policy\n",
      "saved dir: model_Hopper-v2\n",
      "loaded and built\n",
      "starting dagger  Hopper-v2 2019-07-24 16:55:28.288543\n",
      "Hopper-v2  observation shape:  (500000, 11) , actions shape: (500000, 3)\n",
      "loading and building expert policy for DAgger\n",
      "obs (1, 11) (1, 11)\n",
      "loaded and built for DAgger\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "train_observations shape: (500158, 11) , train_actions shape: (500158, 3)\n",
      "observations shape: (158, 11) , actions shape: (158, 1, 3)\n",
      "train input : train_observations shape: (400126, 11) , train_actions shape: (400126, 3)\n",
      "saved dir: model_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-24 16:55:32.323382\n",
      "Epoch: 0000 average training cost = 0.384651631 validation cost = 0.153890029 validation measure = 0.928574324 2019-07-24 16:55:44.338456\n",
      "Epoch: 0049 average training cost = 0.385538429 validation cost = 0.154062301 validation measure = 0.928494334 2019-07-24 17:05:01.289528\n",
      "Training(learning) Finished! 2019-07-24 17:05:01.290579\n",
      "Training took         568  seconds.\n",
      "train_observations shape: (500414, 11) , train_actions shape: (500414, 3)\n",
      "observations shape: (414, 11) , actions shape: (414, 1, 3)\n",
      "train input : train_observations shape: (400331, 11) , train_actions shape: (400331, 3)\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-24 17:05:07.313065\n",
      "Epoch: 0000 average training cost = 0.386508256 validation cost = 0.154548123 validation measure = 0.928454816 2019-07-24 17:05:19.397286\n",
      "Epoch: 0049 average training cost = 0.382926822 validation cost = 0.154231280 validation measure = 0.928601503 2019-07-24 17:15:16.235513\n",
      "Training(learning) Finished! 2019-07-24 17:15:16.238557\n",
      "Training took         608  seconds.\n",
      "train_observations shape: (500910, 11) , train_actions shape: (500910, 3)\n",
      "observations shape: (910, 11) , actions shape: (910, 1, 3)\n",
      "train input : train_observations shape: (400728, 11) , train_actions shape: (400728, 3)\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-24 17:15:26.748490\n",
      "Epoch: 0000 average training cost = 0.386784077 validation cost = 0.154955000 validation measure = 0.928186595 2019-07-24 17:15:40.747870\n",
      "Epoch: 0049 average training cost = 0.383828253 validation cost = 0.154058382 validation measure = 0.928602099 2019-07-24 17:26:27.632058\n",
      "Training(learning) Finished! 2019-07-24 17:26:27.632058\n",
      "Training took         660  seconds.\n",
      "train_observations shape: (501141, 11) , train_actions shape: (501141, 3)\n",
      "observations shape: (1141, 11) , actions shape: (1141, 1, 3)\n",
      "train input : train_observations shape: (400912, 11) , train_actions shape: (400912, 3)\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-24 17:26:33.207356\n",
      "Epoch: 0000 average training cost = 0.385460079 validation cost = 0.155150861 validation measure = 0.927851200 2019-07-24 17:26:45.666060\n",
      "Epoch: 0049 average training cost = 0.382198036 validation cost = 0.154551804 validation measure = 0.928129792 2019-07-24 17:36:33.139250\n",
      "Training(learning) Finished! 2019-07-24 17:36:33.139250\n",
      "Training took         599  seconds.\n",
      "train_observations shape: (501454, 11) , train_actions shape: (501454, 3)\n",
      "observations shape: (1454, 11) , actions shape: (1454, 1, 3)\n",
      "train input : train_observations shape: (401163, 11) , train_actions shape: (401163, 3)\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-24 17:36:40.226932\n",
      "Epoch: 0000 average training cost = 0.383384049 validation cost = 0.157383174 validation measure = 0.926880836 2019-07-24 17:36:52.444892\n",
      "Epoch: 0049 average training cost = 0.385078281 validation cost = 0.156967208 validation measure = 0.927074075 2019-07-24 17:46:31.369074\n",
      "Training(learning) Finished! 2019-07-24 17:46:31.369074\n",
      "Training took         591  seconds.\n",
      "train_observations shape: (501826, 11) , train_actions shape: (501826, 3)\n",
      "observations shape: (1826, 11) , actions shape: (1826, 1, 3)\n",
      "train input : train_observations shape: (401460, 11) , train_actions shape: (401460, 3)\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-24 17:46:39.532876\n",
      "Epoch: 0000 average training cost = 0.384383678 validation cost = 0.158388436 validation measure = 0.926838756 2019-07-24 17:46:51.998490\n",
      "Epoch: 0049 average training cost = 0.380747616 validation cost = 0.158580959 validation measure = 0.926749825 2019-07-24 17:56:12.541610\n",
      "Training(learning) Finished! 2019-07-24 17:56:12.541610\n",
      "Training took         573  seconds.\n",
      "train_observations shape: (502092, 11) , train_actions shape: (502092, 3)\n",
      "observations shape: (2092, 11) , actions shape: (2092, 1, 3)\n",
      "train input : train_observations shape: (401673, 11) , train_actions shape: (401673, 3)\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-24 17:56:18.775202\n",
      "Epoch: 0000 average training cost = 0.381820589 validation cost = 0.162374794 validation measure = 0.925028503 2019-07-24 17:56:32.393959\n",
      "Epoch: 0049 average training cost = 0.382115692 validation cost = 0.162086427 validation measure = 0.925161600 2019-07-24 18:06:55.843288\n",
      "Training(learning) Finished! 2019-07-24 18:06:55.843288\n",
      "Training took         637  seconds.\n",
      "train_observations shape: (502318, 11) , train_actions shape: (502318, 3)\n",
      "observations shape: (2318, 11) , actions shape: (2318, 1, 3)\n",
      "train input : train_observations shape: (401854, 11) , train_actions shape: (401854, 3)\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-24 18:07:01.340503\n",
      "Epoch: 0000 average training cost = 0.382203370 validation cost = 0.162509784 validation measure = 0.924607158 2019-07-24 18:07:14.103906\n",
      "Epoch: 0049 average training cost = 0.382245123 validation cost = 0.161658511 validation measure = 0.925002098 2019-07-24 18:17:04.031236\n",
      "Training(learning) Finished! 2019-07-24 18:17:04.031236\n",
      "Training took         602  seconds.\n",
      "train_observations shape: (502585, 11) , train_actions shape: (502585, 3)\n",
      "observations shape: (2585, 11) , actions shape: (2585, 1, 3)\n",
      "train input : train_observations shape: (402068, 11) , train_actions shape: (402068, 3)\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-24 18:17:10.834759\n",
      "Epoch: 0000 average training cost = 0.382602096 validation cost = 0.165811718 validation measure = 0.924349368 2019-07-24 18:17:24.877745\n",
      "Epoch: 0049 average training cost = 0.381823629 validation cost = 0.165243074 validation measure = 0.924608767 2019-07-24 18:28:10.643831\n",
      "Training(learning) Finished! 2019-07-24 18:28:10.643831\n",
      "Training took         659  seconds.\n",
      "train_observations shape: (502812, 11) , train_actions shape: (502812, 3)\n",
      "observations shape: (2812, 11) , actions shape: (2812, 1, 3)\n",
      "train input : train_observations shape: (402249, 11) , train_actions shape: (402249, 3)\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-24 18:28:16.254009\n",
      "Epoch: 0000 average training cost = 0.382297128 validation cost = 0.164594367 validation measure = 0.924462557 2019-07-24 18:28:29.839019\n",
      "Epoch: 0049 average training cost = 0.381419361 validation cost = 0.164801911 validation measure = 0.924367309 2019-07-24 18:39:01.773901\n",
      "Training(learning) Finished! 2019-07-24 18:39:01.773901\n",
      "Training took         645  seconds.\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-24 18:39:02.318397  and finished at  2019-07-24 18:39:02.363320\n",
      "shape of returns (10,)\n",
      "mean return 702.3590033127697\n",
      "std of return 350.19573167710934\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "shape of returns (10,)\n",
      "mean return 188.35510638321145\n",
      "std of return 2.3056559764724542\n",
      "Rollout result. env: Hopper-v2 , policy_type: dagger , returns: 10 / 188.35510638321145 / 2.3056559764724542\n",
      "It took        6218  seconds.\n",
      "loading and building learned policy\n",
      "saved dir: model_Reacher-v2\n",
      "loaded and built\n",
      "starting dagger  Reacher-v2 2019-07-24 18:39:06.451665\n",
      "Reacher-v2  observation shape:  (500000, 11) , actions shape: (500000, 2)\n",
      "loading and building expert policy for DAgger\n",
      "obs (1, 11) (1, 11)\n",
      "loaded and built for DAgger\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "train_observations shape: (500050, 11) , train_actions shape: (500050, 2)\n",
      "observations shape: (50, 11) , actions shape: (50, 1, 2)\n",
      "train input : train_observations shape: (400040, 11) , train_actions shape: (400040, 2)\n",
      "saved dir: model_Reacher-v2\n",
      "Learning starts. It will take some time... 2019-07-24 18:39:08.578449\n",
      "Epoch: 0000 average training cost = 0.005708159 validation cost = 0.004973564 validation measure = 0.366645932 2019-07-24 18:39:21.184986\n",
      "Epoch: 0049 average training cost = 0.005700910 validation cost = 0.004952187 validation measure = 0.369368136 2019-07-24 18:48:42.336097\n",
      "Training(learning) Finished! 2019-07-24 18:48:42.337094\n",
      "Training took         573  seconds.\n",
      "train_observations shape: (500100, 11) , train_actions shape: (500100, 2)\n",
      "observations shape: (100, 11) , actions shape: (100, 1, 2)\n",
      "train input : train_observations shape: (400080, 11) , train_actions shape: (400080, 2)\n",
      "saved dir: model_dagger_Reacher-v2\n",
      "Learning starts. It will take some time... 2019-07-24 18:48:44.450093\n",
      "Epoch: 0000 average training cost = 0.005666797 validation cost = 0.005080963 validation measure = 0.369535983 2019-07-24 18:48:55.518166\n",
      "Epoch: 0049 average training cost = 0.005654306 validation cost = 0.005081883 validation measure = 0.369421780 2019-07-24 18:57:54.163064\n",
      "Training(learning) Finished! 2019-07-24 18:57:54.163064\n",
      "Training took         549  seconds.\n",
      "train_observations shape: (500150, 11) , train_actions shape: (500150, 2)\n",
      "observations shape: (150, 11) , actions shape: (150, 1, 2)\n",
      "train input : train_observations shape: (400120, 11) , train_actions shape: (400120, 2)\n",
      "saved dir: model_dagger_Reacher-v2\n",
      "Learning starts. It will take some time... 2019-07-24 18:57:56.323320\n",
      "Epoch: 0000 average training cost = 0.005702491 validation cost = 0.005045706 validation measure = 0.367984951 2019-07-24 18:58:09.259046\n",
      "Epoch: 0049 average training cost = 0.005701197 validation cost = 0.005042738 validation measure = 0.368356764 2019-07-24 19:08:16.867289\n",
      "Training(learning) Finished! 2019-07-24 19:08:16.868331\n",
      "Training took         620  seconds.\n",
      "train_observations shape: (500200, 11) , train_actions shape: (500200, 2)\n",
      "observations shape: (200, 11) , actions shape: (200, 1, 2)\n",
      "train input : train_observations shape: (400160, 11) , train_actions shape: (400160, 2)\n",
      "saved dir: model_dagger_Reacher-v2\n",
      "Learning starts. It will take some time... 2019-07-24 19:08:19.029132\n",
      "Epoch: 0000 average training cost = 0.005682150 validation cost = 0.005071015 validation measure = 0.372723639 2019-07-24 19:08:32.913997\n",
      "Epoch: 0049 average training cost = 0.005662952 validation cost = 0.005073859 validation measure = 0.372371793 2019-07-24 19:19:12.265488\n",
      "Training(learning) Finished! 2019-07-24 19:19:12.265488\n",
      "Training took         653  seconds.\n",
      "train_observations shape: (500250, 11) , train_actions shape: (500250, 2)\n",
      "observations shape: (250, 11) , actions shape: (250, 1, 2)\n",
      "train input : train_observations shape: (400200, 11) , train_actions shape: (400200, 2)\n",
      "saved dir: model_dagger_Reacher-v2\n",
      "Learning starts. It will take some time... 2019-07-24 19:19:14.441711\n",
      "Epoch: 0000 average training cost = 0.005675489 validation cost = 0.005113367 validation measure = 0.368677855 2019-07-24 19:19:26.389458\n",
      "Epoch: 0049 average training cost = 0.005690417 validation cost = 0.005111590 validation measure = 0.368897080 2019-07-24 19:28:47.440751\n",
      "Training(learning) Finished! 2019-07-24 19:28:47.440751\n",
      "Training took         572  seconds.\n",
      "train_observations shape: (500300, 11) , train_actions shape: (500300, 2)\n",
      "observations shape: (300, 11) , actions shape: (300, 1, 2)\n",
      "train input : train_observations shape: (400240, 11) , train_actions shape: (400240, 2)\n",
      "saved dir: model_dagger_Reacher-v2\n",
      "Learning starts. It will take some time... 2019-07-24 19:28:49.552784\n",
      "Epoch: 0000 average training cost = 0.005661273 validation cost = 0.005150424 validation measure = 0.369746566 2019-07-24 19:29:01.610290\n",
      "Epoch: 0049 average training cost = 0.005679955 validation cost = 0.005149380 validation measure = 0.369874299 2019-07-24 19:38:25.645949\n",
      "Training(learning) Finished! 2019-07-24 19:38:25.646989\n",
      "Training took         576  seconds.\n",
      "train_observations shape: (500350, 11) , train_actions shape: (500350, 2)\n",
      "observations shape: (350, 11) , actions shape: (350, 1, 2)\n",
      "train input : train_observations shape: (400280, 11) , train_actions shape: (400280, 2)\n",
      "saved dir: model_dagger_Reacher-v2\n",
      "Learning starts. It will take some time... 2019-07-24 19:38:27.830491\n",
      "Epoch: 0000 average training cost = 0.005655720 validation cost = 0.005105641 validation measure = 0.368432701 2019-07-24 19:38:41.422952\n",
      "Epoch: 0049 average training cost = 0.005675591 validation cost = 0.005105034 validation measure = 0.368507802 2019-07-24 19:49:11.685449\n",
      "Training(learning) Finished! 2019-07-24 19:49:11.685449\n",
      "Training took         643  seconds.\n",
      "train_observations shape: (500400, 11) , train_actions shape: (500400, 2)\n",
      "observations shape: (400, 11) , actions shape: (400, 1, 2)\n",
      "train input : train_observations shape: (400320, 11) , train_actions shape: (400320, 2)\n",
      "saved dir: model_dagger_Reacher-v2\n",
      "Learning starts. It will take some time... 2019-07-24 19:49:13.763887\n",
      "Epoch: 0000 average training cost = 0.005651323 validation cost = 0.005161433 validation measure = 0.366724491 2019-07-24 19:49:26.491842\n",
      "Epoch: 0049 average training cost = 0.005651691 validation cost = 0.005160300 validation measure = 0.366863549 2019-07-24 19:59:29.429272\n",
      "Training(learning) Finished! 2019-07-24 19:59:29.430313\n",
      "Training took         615  seconds.\n",
      "train_observations shape: (500450, 11) , train_actions shape: (500450, 2)\n",
      "observations shape: (450, 11) , actions shape: (450, 1, 2)\n",
      "train input : train_observations shape: (400360, 11) , train_actions shape: (400360, 2)\n",
      "saved dir: model_dagger_Reacher-v2\n",
      "Learning starts. It will take some time... 2019-07-24 19:59:31.581148\n",
      "Epoch: 0000 average training cost = 0.005678958 validation cost = 0.005080495 validation measure = 0.367216110 2019-07-24 19:59:44.587451\n",
      "Epoch: 0049 average training cost = 0.005676202 validation cost = 0.005079120 validation measure = 0.367387414 2019-07-24 20:08:53.244594\n",
      "Training(learning) Finished! 2019-07-24 20:08:53.244594\n",
      "Training took         561  seconds.\n",
      "train_observations shape: (500500, 11) , train_actions shape: (500500, 2)\n",
      "observations shape: (500, 11) , actions shape: (500, 1, 2)\n",
      "train input : train_observations shape: (400400, 11) , train_actions shape: (400400, 2)\n",
      "saved dir: model_dagger_Reacher-v2\n",
      "Learning starts. It will take some time... 2019-07-24 20:08:55.406596\n",
      "Epoch: 0000 average training cost = 0.005695205 validation cost = 0.005036990 validation measure = 0.370528698 2019-07-24 20:09:07.731123\n",
      "Epoch: 0049 average training cost = 0.005696518 validation cost = 0.005037534 validation measure = 0.370460689 2019-07-24 20:18:51.986578\n",
      "Training(learning) Finished! 2019-07-24 20:18:51.986578\n",
      "Training took         596  seconds.\n",
      "saved dir: model_dagger_Reacher-v2\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-24 20:18:52.557183  and finished at  2019-07-24 20:18:52.601070\n",
      "shape of returns (10,)\n",
      "mean return -9.134551526619298\n",
      "std of return 4.331385887197037\n",
      "saved dir: model_dagger_Reacher-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "shape of returns (10,)\n",
      "mean return -10.67047854470968\n",
      "std of return 5.296682476122061\n",
      "Rollout result. env: Reacher-v2 , policy_type: dagger , returns: 10 / -10.67047854470968 / 5.296682476122061\n",
      "It took        5988  seconds.\n",
      "loading and building learned policy\n",
      "saved dir: model_HalfCheetah-v2\n",
      "loaded and built\n",
      "starting dagger  HalfCheetah-v2 2019-07-24 20:18:55.270817\n",
      "HalfCheetah-v2  observation shape:  (500000, 17) , actions shape: (500000, 6)\n",
      "loading and building expert policy for DAgger\n",
      "obs (1, 17) (1, 17)\n",
      "loaded and built for DAgger\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "train_observations shape: (501000, 17) , train_actions shape: (501000, 6)\n",
      "observations shape: (1000, 17) , actions shape: (1000, 1, 6)\n",
      "train input : train_observations shape: (400800, 17) , train_actions shape: (400800, 6)\n",
      "saved dir: model_HalfCheetah-v2\n",
      "Learning starts. It will take some time... 2019-07-24 20:19:21.866528\n",
      "Epoch: 0000 average training cost = 0.110662565 validation cost = 0.058333278 validation measure = 0.893269479 2019-07-24 20:19:35.595097\n",
      "Epoch: 0049 average training cost = 0.109972440 validation cost = 0.057887021 validation measure = 0.894086003 2019-07-24 20:30:15.274426\n",
      "Training(learning) Finished! 2019-07-24 20:30:15.274426\n",
      "Training took         653  seconds.\n",
      "train_observations shape: (502000, 17) , train_actions shape: (502000, 6)\n",
      "observations shape: (2000, 17) , actions shape: (2000, 1, 6)\n",
      "train input : train_observations shape: (401600, 17) , train_actions shape: (401600, 6)\n",
      "saved dir: model_dagger_HalfCheetah-v2\n",
      "Learning starts. It will take some time... 2019-07-24 20:30:45.591843\n",
      "Epoch: 0000 average training cost = 0.109991163 validation cost = 0.057680298 validation measure = 0.894695520 2019-07-24 20:30:58.850769\n",
      "Epoch: 0049 average training cost = 0.109428987 validation cost = 0.057234045 validation measure = 0.895510197 2019-07-24 20:41:13.765017\n",
      "Training(learning) Finished! 2019-07-24 20:41:13.765017\n",
      "Training took         628  seconds.\n",
      "train_observations shape: (503000, 17) , train_actions shape: (503000, 6)\n",
      "observations shape: (3000, 17) , actions shape: (3000, 1, 6)\n",
      "train input : train_observations shape: (402400, 17) , train_actions shape: (402400, 6)\n",
      "saved dir: model_dagger_HalfCheetah-v2\n",
      "Learning starts. It will take some time... 2019-07-24 20:41:44.250747\n",
      "Epoch: 0000 average training cost = 0.110185362 validation cost = 0.057357840 validation measure = 0.894378424 2019-07-24 20:41:58.003865\n",
      "Epoch: 0049 average training cost = 0.109378159 validation cost = 0.056991573 validation measure = 0.895052850 2019-07-24 20:52:42.342982\n",
      "Training(learning) Finished! 2019-07-24 20:52:42.343979\n",
      "Training took         658  seconds.\n",
      "train_observations shape: (504000, 17) , train_actions shape: (504000, 6)\n",
      "observations shape: (4000, 17) , actions shape: (4000, 1, 6)\n",
      "train input : train_observations shape: (403200, 17) , train_actions shape: (403200, 6)\n",
      "saved dir: model_dagger_HalfCheetah-v2\n",
      "Learning starts. It will take some time... 2019-07-24 20:53:12.789989\n",
      "Epoch: 0000 average training cost = 0.109267130 validation cost = 0.056929249 validation measure = 0.895660162 2019-07-24 20:53:26.819207\n",
      "Epoch: 0049 average training cost = 0.108571045 validation cost = 0.056561004 validation measure = 0.896335065 2019-07-24 21:04:28.941842\n",
      "Training(learning) Finished! 2019-07-24 21:04:28.941842\n",
      "Training took         676  seconds.\n",
      "train_observations shape: (505000, 17) , train_actions shape: (505000, 6)\n",
      "observations shape: (5000, 17) , actions shape: (5000, 1, 6)\n",
      "train input : train_observations shape: (404000, 17) , train_actions shape: (404000, 6)\n",
      "saved dir: model_dagger_HalfCheetah-v2\n",
      "Learning starts. It will take some time... 2019-07-24 21:04:59.442152\n",
      "Epoch: 0000 average training cost = 0.108973972 validation cost = 0.056797743 validation measure = 0.896008313 2019-07-24 21:05:12.522124\n",
      "Epoch: 0049 average training cost = 0.108331405 validation cost = 0.056455504 validation measure = 0.896634877 2019-07-24 21:15:52.115132\n",
      "Training(learning) Finished! 2019-07-24 21:15:52.115132\n",
      "Training took         652  seconds.\n",
      "train_observations shape: (506000, 17) , train_actions shape: (506000, 6)\n",
      "observations shape: (6000, 17) , actions shape: (6000, 1, 6)\n",
      "train input : train_observations shape: (404800, 17) , train_actions shape: (404800, 6)\n",
      "saved dir: model_dagger_HalfCheetah-v2\n",
      "Learning starts. It will take some time... 2019-07-24 21:16:22.938427\n",
      "Epoch: 0000 average training cost = 0.108537413 validation cost = 0.056364749 validation measure = 0.896283090 2019-07-24 21:16:36.547647\n",
      "Epoch: 0049 average training cost = 0.107895762 validation cost = 0.056028601 validation measure = 0.896901608 2019-07-24 21:27:13.069566\n",
      "Training(learning) Finished! 2019-07-24 21:27:13.069566\n",
      "Training took         650  seconds.\n",
      "train_observations shape: (507000, 17) , train_actions shape: (507000, 6)\n",
      "observations shape: (7000, 17) , actions shape: (7000, 1, 6)\n",
      "train input : train_observations shape: (405600, 17) , train_actions shape: (405600, 6)\n",
      "saved dir: model_dagger_HalfCheetah-v2\n",
      "Learning starts. It will take some time... 2019-07-24 21:27:43.712979\n",
      "Epoch: 0000 average training cost = 0.108054161 validation cost = 0.056369655 validation measure = 0.896619856 2019-07-24 21:27:57.289679\n",
      "Epoch: 0049 average training cost = 0.107436359 validation cost = 0.055952761 validation measure = 0.897384465 2019-07-24 21:38:22.306704\n",
      "Training(learning) Finished! 2019-07-24 21:38:22.307658\n",
      "Training took         638  seconds.\n",
      "train_observations shape: (508000, 17) , train_actions shape: (508000, 6)\n",
      "observations shape: (8000, 17) , actions shape: (8000, 1, 6)\n",
      "train input : train_observations shape: (406400, 17) , train_actions shape: (406400, 6)\n",
      "saved dir: model_dagger_HalfCheetah-v2\n",
      "Learning starts. It will take some time... 2019-07-24 21:38:52.916792\n",
      "Epoch: 0000 average training cost = 0.107835516 validation cost = 0.056047954 validation measure = 0.897556722 2019-07-24 21:39:06.180476\n",
      "Epoch: 0049 average training cost = 0.107426770 validation cost = 0.055761326 validation measure = 0.898080587 2019-07-24 21:49:14.310862\n",
      "Training(learning) Finished! 2019-07-24 21:49:14.311859\n",
      "Training took         621  seconds.\n",
      "train_observations shape: (509000, 17) , train_actions shape: (509000, 6)\n",
      "observations shape: (9000, 17) , actions shape: (9000, 1, 6)\n",
      "train input : train_observations shape: (407200, 17) , train_actions shape: (407200, 6)\n",
      "saved dir: model_dagger_HalfCheetah-v2\n",
      "Learning starts. It will take some time... 2019-07-24 21:49:44.973030\n",
      "Epoch: 0000 average training cost = 0.107535020 validation cost = 0.055764988 validation measure = 0.897446156 2019-07-24 21:49:58.060025\n",
      "Epoch: 0049 average training cost = 0.106932528 validation cost = 0.055391505 validation measure = 0.898133039 2019-07-24 22:00:18.789090\n",
      "Training(learning) Finished! 2019-07-24 22:00:18.789090\n",
      "Training took         633  seconds.\n",
      "train_observations shape: (510000, 17) , train_actions shape: (510000, 6)\n",
      "observations shape: (10000, 17) , actions shape: (10000, 1, 6)\n",
      "train input : train_observations shape: (408000, 17) , train_actions shape: (408000, 6)\n",
      "saved dir: model_dagger_HalfCheetah-v2\n",
      "Learning starts. It will take some time... 2019-07-24 22:00:49.482940\n",
      "Epoch: 0000 average training cost = 0.107128330 validation cost = 0.055242967 validation measure = 0.898230672 2019-07-24 22:01:02.746001\n",
      "Epoch: 0049 average training cost = 0.106868125 validation cost = 0.054966718 validation measure = 0.898739576 2019-07-24 22:11:06.987972\n",
      "Training(learning) Finished! 2019-07-24 22:11:06.989972\n",
      "Training took         617  seconds.\n",
      "saved dir: model_dagger_HalfCheetah-v2\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-24 22:11:07.513519  and finished at  2019-07-24 22:11:07.585383\n",
      "shape of returns (10,)\n",
      "mean return 2597.3350358712105\n",
      "std of return 184.6608208515583\n",
      "saved dir: model_dagger_HalfCheetah-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "shape of returns (10,)\n",
      "mean return 2787.339103033303\n",
      "std of return 157.3300855830788\n",
      "Rollout result. env: HalfCheetah-v2 , policy_type: dagger , returns: 10 / 2787.339103033303 / 157.3300855830788\n",
      "It took        6752  seconds.\n",
      "loading and building learned policy\n",
      "saved dir: model_Humanoid-v2\n",
      "loaded and built\n",
      "starting dagger  Humanoid-v2 2019-07-24 22:11:28.481627\n",
      "Humanoid-v2  observation shape:  (499084, 376) , actions shape: (499084, 17)\n",
      "loading and building expert policy for DAgger\n",
      "obs (1, 376) (1, 376)\n",
      "loaded and built for DAgger\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "train_observations shape: (499137, 376) , train_actions shape: (499137, 17)\n",
      "observations shape: (53, 376) , actions shape: (53, 1, 17)\n",
      "train input : train_observations shape: (399309, 376) , train_actions shape: (399309, 17)\n",
      "saved dir: model_Humanoid-v2\n",
      "Learning starts. It will take some time... 2019-07-24 22:11:56.963854\n",
      "Epoch: 0000 average training cost = 0.181212246 validation cost = 0.104360558 validation measure = 0.890858114 2019-07-24 22:12:12.214209\n",
      "Epoch: 0049 average training cost = 0.180816069 validation cost = 0.104076266 validation measure = 0.891155422 2019-07-24 22:23:48.183344\n",
      "Training(learning) Finished! 2019-07-24 22:23:48.183344\n",
      "Training took         711  seconds.\n",
      "train_observations shape: (499187, 376) , train_actions shape: (499187, 17)\n",
      "observations shape: (103, 376) , actions shape: (103, 1, 17)\n",
      "train input : train_observations shape: (399349, 376) , train_actions shape: (399349, 17)\n",
      "saved dir: model_dagger_Humanoid-v2\n",
      "Learning starts. It will take some time... 2019-07-24 22:24:14.335818\n",
      "Epoch: 0000 average training cost = 0.181053579 validation cost = 0.104479268 validation measure = 0.890825450 2019-07-24 22:24:28.182660\n",
      "Epoch: 0049 average training cost = 0.180871353 validation cost = 0.104170047 validation measure = 0.891148567 2019-07-24 22:34:44.995266\n",
      "Training(learning) Finished! 2019-07-24 22:34:44.995266\n",
      "Training took         630  seconds.\n",
      "train_observations shape: (499238, 376) , train_actions shape: (499238, 17)\n",
      "observations shape: (154, 376) , actions shape: (154, 1, 17)\n",
      "train input : train_observations shape: (399390, 376) , train_actions shape: (399390, 17)\n",
      "saved dir: model_dagger_Humanoid-v2\n",
      "Learning starts. It will take some time... 2019-07-24 22:35:11.401047\n",
      "Epoch: 0000 average training cost = 0.180572554 validation cost = 0.104001775 validation measure = 0.891404867 2019-07-24 22:35:26.169508\n",
      "Epoch: 0049 average training cost = 0.180207655 validation cost = 0.103773937 validation measure = 0.891642749 2019-07-24 22:46:42.417027\n",
      "Training(learning) Finished! 2019-07-24 22:46:42.418024\n",
      "Training took         691  seconds.\n",
      "train_observations shape: (499300, 376) , train_actions shape: (499300, 17)\n",
      "observations shape: (216, 376) , actions shape: (216, 1, 17)\n",
      "train input : train_observations shape: (399440, 376) , train_actions shape: (399440, 17)\n",
      "saved dir: model_dagger_Humanoid-v2\n",
      "Learning starts. It will take some time... 2019-07-24 22:47:14.064215\n",
      "Epoch: 0000 average training cost = 0.180272207 validation cost = 0.103699543 validation measure = 0.891596079 2019-07-24 22:47:28.533964\n",
      "Epoch: 0049 average training cost = 0.180147290 validation cost = 0.103396304 validation measure = 0.891913056 2019-07-24 22:58:34.830212\n",
      "Training(learning) Finished! 2019-07-24 22:58:34.831222\n",
      "Training took         680  seconds.\n",
      "train_observations shape: (499357, 376) , train_actions shape: (499357, 17)\n",
      "observations shape: (273, 376) , actions shape: (273, 1, 17)\n",
      "train input : train_observations shape: (399485, 376) , train_actions shape: (399485, 17)\n",
      "saved dir: model_dagger_Humanoid-v2\n",
      "Learning starts. It will take some time... 2019-07-24 22:59:03.786442\n",
      "Epoch: 0000 average training cost = 0.179973662 validation cost = 0.103674166 validation measure = 0.891660929 2019-07-24 22:59:18.350990\n",
      "Epoch: 0049 average training cost = 0.179709896 validation cost = 0.103420056 validation measure = 0.891926467 2019-07-24 23:10:14.118739\n",
      "Training(learning) Finished! 2019-07-24 23:10:14.118739\n",
      "Training took         670  seconds.\n",
      "train_observations shape: (499417, 376) , train_actions shape: (499417, 17)\n",
      "observations shape: (333, 376) , actions shape: (333, 1, 17)\n",
      "train input : train_observations shape: (399533, 376) , train_actions shape: (399533, 17)\n",
      "saved dir: model_dagger_Humanoid-v2\n",
      "Learning starts. It will take some time... 2019-07-24 23:10:44.450742\n",
      "Epoch: 0000 average training cost = 0.180111438 validation cost = 0.103231646 validation measure = 0.892230928 2019-07-24 23:10:57.938531\n",
      "Epoch: 0049 average training cost = 0.179507777 validation cost = 0.102926224 validation measure = 0.892549753 2019-07-24 23:20:38.756662\n",
      "Training(learning) Finished! 2019-07-24 23:20:38.756662\n",
      "Training took         594  seconds.\n",
      "train_observations shape: (499473, 376) , train_actions shape: (499473, 17)\n",
      "observations shape: (389, 376) , actions shape: (389, 1, 17)\n",
      "train input : train_observations shape: (399578, 376) , train_actions shape: (399578, 17)\n",
      "saved dir: model_dagger_Humanoid-v2\n",
      "Learning starts. It will take some time... 2019-07-24 23:21:07.538713\n",
      "Epoch: 0000 average training cost = 0.179627821 validation cost = 0.102833010 validation measure = 0.892401695 2019-07-24 23:21:22.859846\n",
      "Epoch: 0049 average training cost = 0.179346144 validation cost = 0.102578066 validation measure = 0.892668426 2019-07-24 23:33:00.331451\n",
      "Training(learning) Finished! 2019-07-24 23:33:00.331451\n",
      "Training took         712  seconds.\n",
      "train_observations shape: (499532, 376) , train_actions shape: (499532, 17)\n",
      "observations shape: (448, 376) , actions shape: (448, 1, 17)\n",
      "train input : train_observations shape: (399625, 376) , train_actions shape: (399625, 17)\n",
      "saved dir: model_dagger_Humanoid-v2\n",
      "Learning starts. It will take some time... 2019-07-24 23:33:30.357382\n",
      "Epoch: 0000 average training cost = 0.179348692 validation cost = 0.102717705 validation measure = 0.892579556 2019-07-24 23:33:43.848439\n",
      "Epoch: 0049 average training cost = 0.178854868 validation cost = 0.102460109 validation measure = 0.892848909 2019-07-24 23:43:59.839836\n",
      "Training(learning) Finished! 2019-07-24 23:43:59.839836\n",
      "Training took         629  seconds.\n",
      "train_observations shape: (499585, 376) , train_actions shape: (499585, 17)\n",
      "observations shape: (501, 376) , actions shape: (501, 1, 17)\n",
      "train input : train_observations shape: (399668, 376) , train_actions shape: (399668, 17)\n",
      "saved dir: model_dagger_Humanoid-v2\n",
      "Learning starts. It will take some time... 2019-07-24 23:44:27.222586\n",
      "Epoch: 0000 average training cost = 0.179332778 validation cost = 0.102337830 validation measure = 0.892764866 2019-07-24 23:44:42.519739\n",
      "Epoch: 0049 average training cost = 0.179034173 validation cost = 0.102120936 validation measure = 0.892992139 2019-07-24 23:56:16.120031\n",
      "Training(learning) Finished! 2019-07-24 23:56:16.120031\n",
      "Training took         708  seconds.\n",
      "train_observations shape: (499638, 376) , train_actions shape: (499638, 17)\n",
      "observations shape: (554, 376) , actions shape: (554, 1, 17)\n",
      "train input : train_observations shape: (399710, 376) , train_actions shape: (399710, 17)\n",
      "saved dir: model_dagger_Humanoid-v2\n",
      "Learning starts. It will take some time... 2019-07-24 23:56:44.888947\n",
      "Epoch: 0000 average training cost = 0.178710237 validation cost = 0.102260694 validation measure = 0.893059731 2019-07-24 23:56:59.274466\n",
      "Epoch: 0049 average training cost = 0.178574786 validation cost = 0.101986460 validation measure = 0.893346548 2019-07-25 00:08:12.980650\n",
      "Training(learning) Finished! 2019-07-25 00:08:12.981647\n",
      "Training took         688  seconds.\n",
      "saved dir: model_dagger_Humanoid-v2\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-25 00:08:13.698702  and finished at  2019-07-25 00:08:14.072703\n",
      "shape of returns (10,)\n",
      "mean return 300.53082918823964\n",
      "std of return 24.222554386222622\n",
      "saved dir: model_dagger_Humanoid-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "shape of returns (10,)\n",
      "mean return 348.9815539240043\n",
      "std of return 62.85142290723399\n",
      "Rollout result. env: Humanoid-v2 , policy_type: dagger , returns: 10 / 348.9815539240043 / 62.85142290723399\n",
      "It took        7011  seconds.\n",
      "loading and building learned policy\n",
      "saved dir: model_Walker2d-v2\n",
      "loaded and built\n",
      "starting dagger  Walker2d-v2 2019-07-25 00:08:18.157826\n",
      "Walker2d-v2  observation shape:  (499696, 17) , actions shape: (499696, 6)\n",
      "loading and building expert policy for DAgger\n",
      "obs (1, 17) (1, 17)\n",
      "loaded and built for DAgger\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "train_observations shape: (499873, 17) , train_actions shape: (499873, 6)\n",
      "observations shape: (177, 17) , actions shape: (177, 1, 6)\n",
      "train input : train_observations shape: (399898, 17) , train_actions shape: (399898, 6)\n",
      "saved dir: model_Walker2d-v2\n",
      "Learning starts. It will take some time... 2019-07-25 00:08:24.208790\n",
      "Epoch: 0000 average training cost = 0.319501489 validation cost = 0.193493918 validation measure = 0.805730999 2019-07-25 00:08:37.352631\n",
      "Epoch: 0049 average training cost = 0.318858773 validation cost = 0.192891538 validation measure = 0.806335807 2019-07-25 00:18:35.421412\n",
      "Training(learning) Finished! 2019-07-25 00:18:35.422410\n",
      "Training took         611  seconds.\n",
      "train_observations shape: (500053, 17) , train_actions shape: (500053, 6)\n",
      "observations shape: (357, 17) , actions shape: (357, 1, 6)\n",
      "train input : train_observations shape: (400042, 17) , train_actions shape: (400042, 6)\n",
      "saved dir: model_dagger_Walker2d-v2\n",
      "Learning starts. It will take some time... 2019-07-25 00:18:42.322519\n",
      "Epoch: 0000 average training cost = 0.318562746 validation cost = 0.193049192 validation measure = 0.806105494 2019-07-25 00:18:53.716945\n",
      "Epoch: 0049 average training cost = 0.317585260 validation cost = 0.192389384 validation measure = 0.806768179 2019-07-25 00:28:04.646646\n",
      "Training(learning) Finished! 2019-07-25 00:28:04.650650\n",
      "Training took         562  seconds.\n",
      "train_observations shape: (500329, 17) , train_actions shape: (500329, 6)\n",
      "observations shape: (633, 17) , actions shape: (633, 1, 6)\n",
      "train input : train_observations shape: (400263, 17) , train_actions shape: (400263, 6)\n",
      "saved dir: model_dagger_Walker2d-v2\n",
      "Learning starts. It will take some time... 2019-07-25 00:28:14.422708\n",
      "Epoch: 0000 average training cost = 0.319344848 validation cost = 0.192752123 validation measure = 0.805791199 2019-07-25 00:28:27.291615\n",
      "Epoch: 0049 average training cost = 0.317998946 validation cost = 0.191899404 validation measure = 0.806650400 2019-07-25 00:38:19.899952\n",
      "Training(learning) Finished! 2019-07-25 00:38:19.900894\n",
      "Training took         605  seconds.\n",
      "train_observations shape: (501329, 17) , train_actions shape: (501329, 6)\n",
      "observations shape: (1633, 17) , actions shape: (1633, 1, 6)\n",
      "train input : train_observations shape: (401063, 17) , train_actions shape: (401063, 6)\n",
      "saved dir: model_dagger_Walker2d-v2\n",
      "Learning starts. It will take some time... 2019-07-25 00:38:51.724907\n",
      "Epoch: 0000 average training cost = 0.318863362 validation cost = 0.192353725 validation measure = 0.806298733 2019-07-25 00:39:03.165007\n",
      "Epoch: 0049 average training cost = 0.317508429 validation cost = 0.191572011 validation measure = 0.807085931 2019-07-25 00:47:54.350736\n",
      "Training(learning) Finished! 2019-07-25 00:47:54.351782\n",
      "Training took         542  seconds.\n",
      "train_observations shape: (501505, 17) , train_actions shape: (501505, 6)\n",
      "observations shape: (1809, 17) , actions shape: (1809, 1, 6)\n",
      "train input : train_observations shape: (401204, 17) , train_actions shape: (401204, 6)\n",
      "saved dir: model_dagger_Walker2d-v2\n",
      "Learning starts. It will take some time... 2019-07-25 00:48:01.060859\n",
      "Epoch: 0000 average training cost = 0.318160802 validation cost = 0.190951273 validation measure = 0.807510138 2019-07-25 00:48:14.607133\n",
      "Epoch: 0049 average training cost = 0.317339182 validation cost = 0.190217257 validation measure = 0.808250070 2019-07-25 00:58:54.483240\n",
      "Training(learning) Finished! 2019-07-25 00:58:54.483240\n",
      "Training took         653  seconds.\n",
      "train_observations shape: (502505, 17) , train_actions shape: (502505, 6)\n",
      "observations shape: (2809, 17) , actions shape: (2809, 1, 6)\n",
      "train input : train_observations shape: (402004, 17) , train_actions shape: (402004, 6)\n",
      "saved dir: model_dagger_Walker2d-v2\n",
      "Learning starts. It will take some time... 2019-07-25 00:59:26.248133\n",
      "Epoch: 0000 average training cost = 0.318364531 validation cost = 0.191656247 validation measure = 0.807391763 2019-07-25 00:59:39.834010\n",
      "Epoch: 0049 average training cost = 0.317699224 validation cost = 0.190680861 validation measure = 0.808371961 2019-07-25 01:10:10.858406\n",
      "Training(learning) Finished! 2019-07-25 01:10:10.859390\n",
      "Training took         644  seconds.\n",
      "train_observations shape: (502684, 17) , train_actions shape: (502684, 6)\n",
      "observations shape: (2988, 17) , actions shape: (2988, 1, 6)\n",
      "train input : train_observations shape: (402147, 17) , train_actions shape: (402147, 6)\n",
      "saved dir: model_dagger_Walker2d-v2\n",
      "Learning starts. It will take some time... 2019-07-25 01:10:17.696342\n",
      "Epoch: 0000 average training cost = 0.317776322 validation cost = 0.190246299 validation measure = 0.807975471 2019-07-25 01:10:30.066969\n",
      "Epoch: 0049 average training cost = 0.317496240 validation cost = 0.189452916 validation measure = 0.808776259 2019-07-25 01:17:54.313652\n",
      "Training(learning) Finished! 2019-07-25 01:17:54.313652\n",
      "Training took         456  seconds.\n",
      "train_observations shape: (503684, 17) , train_actions shape: (503684, 6)\n",
      "observations shape: (3988, 17) , actions shape: (3988, 1, 6)\n",
      "train input : train_observations shape: (402947, 17) , train_actions shape: (402947, 6)\n",
      "saved dir: model_dagger_Walker2d-v2\n",
      "Learning starts. It will take some time... 2019-07-25 01:18:26.132681\n",
      "Epoch: 0000 average training cost = 0.317637295 validation cost = 0.190495029 validation measure = 0.807800174 2019-07-25 01:18:39.345288\n",
      "Epoch: 0049 average training cost = 0.316319764 validation cost = 0.189805299 validation measure = 0.808496058 2019-07-25 01:28:54.381030\n",
      "Training(learning) Finished! 2019-07-25 01:28:54.381030\n",
      "Training took         628  seconds.\n",
      "train_observations shape: (503860, 17) , train_actions shape: (503860, 6)\n",
      "observations shape: (4164, 17) , actions shape: (4164, 1, 6)\n",
      "train input : train_observations shape: (403088, 17) , train_actions shape: (403088, 6)\n",
      "saved dir: model_dagger_Walker2d-v2\n",
      "Learning starts. It will take some time... 2019-07-25 01:29:01.669302\n",
      "Epoch: 0000 average training cost = 0.317051589 validation cost = 0.190575331 validation measure = 0.808785856 2019-07-25 01:29:15.188837\n",
      "Epoch: 0049 average training cost = 0.315168053 validation cost = 0.189972356 validation measure = 0.809390843 2019-07-25 01:39:51.357445\n",
      "Training(learning) Finished! 2019-07-25 01:39:51.357445\n",
      "Training took         649  seconds.\n",
      "train_observations shape: (504860, 17) , train_actions shape: (504860, 6)\n",
      "observations shape: (5164, 17) , actions shape: (5164, 1, 6)\n",
      "train input : train_observations shape: (403888, 17) , train_actions shape: (403888, 6)\n",
      "saved dir: model_dagger_Walker2d-v2\n",
      "Learning starts. It will take some time... 2019-07-25 01:40:23.272238\n",
      "Epoch: 0000 average training cost = 0.315945357 validation cost = 0.189409345 validation measure = 0.809100509 2019-07-25 01:40:36.599200\n",
      "Epoch: 0049 average training cost = 0.315508217 validation cost = 0.188600555 validation measure = 0.809915662 2019-07-25 01:50:54.538977\n",
      "Training(learning) Finished! 2019-07-25 01:50:54.539974\n",
      "Training took         631  seconds.\n",
      "saved dir: model_dagger_Walker2d-v2\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-25 01:50:55.053552  and finished at  2019-07-25 01:50:55.107409\n",
      "shape of returns (10,)\n",
      "mean return 770.5263370102955\n",
      "std of return 499.53932369921785\n",
      "saved dir: model_dagger_Walker2d-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "shape of returns (10,)\n",
      "mean return 336.11567701375964\n",
      "std of return 60.51423051317056\n",
      "Rollout result. env: Walker2d-v2 , policy_type: dagger , returns: 10 / 336.11567701375964 / 60.51423051317056\n",
      "It took        6162  seconds.\n"
     ]
    }
   ],
   "source": [
    "max_timesteps = None\n",
    "num_rollouts = 10 # incremental learning is too slow \n",
    "\n",
    "for gym_env in gym_envs :\n",
    "    start_time = dt.datetime.now()\n",
    "    rollout_data, policy_type, _ = rollout_by_dagger(gym_env, max_timesteps, num_rollouts,\n",
    "                                                    render=False)\n",
    "    returns = rollout_data['returns']\n",
    "    end_time = dt.datetime.now()\n",
    "    print('Rollout result. env:', gym_env, ', policy_type:', policy_type, ', returns:', len(returns), '/', np.mean(returns), '/', np.std(returns))\n",
    "    df = df.append({'gymenv':gym_env, 'policy_type':policy_type, 'rollouts': len(returns), 'mean': np.mean(returns), 'std': np.std(returns)}, ignore_index=True)\n",
    "    print('It took ', '%10d' % ((end_time - start_time).total_seconds()), ' seconds.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gymenv</th>\n",
       "      <th>policy_type</th>\n",
       "      <th>rollouts</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ant-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>500</td>\n",
       "      <td>4792.043356</td>\n",
       "      <td>260.380669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ant-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>500</td>\n",
       "      <td>1509.113972</td>\n",
       "      <td>104.040231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Hopper-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>500</td>\n",
       "      <td>3778.063615</td>\n",
       "      <td>3.640209</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hopper-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>500</td>\n",
       "      <td>788.532313</td>\n",
       "      <td>335.400783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Reacher-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>500</td>\n",
       "      <td>-3.833945</td>\n",
       "      <td>1.632588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Reacher-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>500</td>\n",
       "      <td>-10.479362</td>\n",
       "      <td>4.560073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>HalfCheetah-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>500</td>\n",
       "      <td>4142.096107</td>\n",
       "      <td>81.876911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>HalfCheetah-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>500</td>\n",
       "      <td>2628.466742</td>\n",
       "      <td>195.237611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Humanoid-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>500</td>\n",
       "      <td>10366.597295</td>\n",
       "      <td>565.231565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Humanoid-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>500</td>\n",
       "      <td>295.178255</td>\n",
       "      <td>24.592578</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Walker2d-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>500</td>\n",
       "      <td>5501.336433</td>\n",
       "      <td>260.067383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Walker2d-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>500</td>\n",
       "      <td>799.472253</td>\n",
       "      <td>493.232001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Ant-v2</td>\n",
       "      <td>dagger</td>\n",
       "      <td>10</td>\n",
       "      <td>1494.733489</td>\n",
       "      <td>51.296003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Hopper-v2</td>\n",
       "      <td>dagger</td>\n",
       "      <td>10</td>\n",
       "      <td>623.799494</td>\n",
       "      <td>261.587512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Reacher-v2</td>\n",
       "      <td>dagger</td>\n",
       "      <td>10</td>\n",
       "      <td>-8.434138</td>\n",
       "      <td>3.786398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>HalfCheetah-v2</td>\n",
       "      <td>dagger</td>\n",
       "      <td>10</td>\n",
       "      <td>2614.389109</td>\n",
       "      <td>151.872455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Humanoid-v2</td>\n",
       "      <td>dagger</td>\n",
       "      <td>10</td>\n",
       "      <td>297.395509</td>\n",
       "      <td>27.723677</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Walker2d-v2</td>\n",
       "      <td>dagger</td>\n",
       "      <td>10</td>\n",
       "      <td>633.486223</td>\n",
       "      <td>451.628504</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>Ant-v2</td>\n",
       "      <td>dagger</td>\n",
       "      <td>10</td>\n",
       "      <td>1652.959853</td>\n",
       "      <td>329.359897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>Hopper-v2</td>\n",
       "      <td>dagger</td>\n",
       "      <td>10</td>\n",
       "      <td>188.355106</td>\n",
       "      <td>2.305656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Reacher-v2</td>\n",
       "      <td>dagger</td>\n",
       "      <td>10</td>\n",
       "      <td>-10.670479</td>\n",
       "      <td>5.296682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>HalfCheetah-v2</td>\n",
       "      <td>dagger</td>\n",
       "      <td>10</td>\n",
       "      <td>2787.339103</td>\n",
       "      <td>157.330086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Humanoid-v2</td>\n",
       "      <td>dagger</td>\n",
       "      <td>10</td>\n",
       "      <td>348.981554</td>\n",
       "      <td>62.851423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Walker2d-v2</td>\n",
       "      <td>dagger</td>\n",
       "      <td>10</td>\n",
       "      <td>336.115677</td>\n",
       "      <td>60.514231</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            gymenv policy_type rollouts          mean         std\n",
       "0           Ant-v2      expert      500   4792.043356  260.380669\n",
       "1           Ant-v2     learned      500   1509.113972  104.040231\n",
       "2        Hopper-v2      expert      500   3778.063615    3.640209\n",
       "3        Hopper-v2     learned      500    788.532313  335.400783\n",
       "4       Reacher-v2      expert      500     -3.833945    1.632588\n",
       "5       Reacher-v2     learned      500    -10.479362    4.560073\n",
       "6   HalfCheetah-v2      expert      500   4142.096107   81.876911\n",
       "7   HalfCheetah-v2     learned      500   2628.466742  195.237611\n",
       "8      Humanoid-v2      expert      500  10366.597295  565.231565\n",
       "9      Humanoid-v2     learned      500    295.178255   24.592578\n",
       "10     Walker2d-v2      expert      500   5501.336433  260.067383\n",
       "11     Walker2d-v2     learned      500    799.472253  493.232001\n",
       "12          Ant-v2      dagger       10   1494.733489   51.296003\n",
       "13       Hopper-v2      dagger       10    623.799494  261.587512\n",
       "14      Reacher-v2      dagger       10     -8.434138    3.786398\n",
       "15  HalfCheetah-v2      dagger       10   2614.389109  151.872455\n",
       "16     Humanoid-v2      dagger       10    297.395509   27.723677\n",
       "17     Walker2d-v2      dagger       10    633.486223  451.628504\n",
       "18          Ant-v2      dagger       10   1652.959853  329.359897\n",
       "19       Hopper-v2      dagger       10    188.355106    2.305656\n",
       "20      Reacher-v2      dagger       10    -10.670479    5.296682\n",
       "21  HalfCheetah-v2      dagger       10   2787.339103  157.330086\n",
       "22     Humanoid-v2      dagger       10    348.981554   62.851423\n",
       "23     Walker2d-v2      dagger       10    336.115677   60.514231"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## just rollout and render using the policies. enjoy the visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and building expert policy\n",
      "obs (1, 111) (1, 111)\n",
      "loaded and built\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Creating window glfw\n",
      "shape of returns (5,)\n",
      "mean return 4236.738439673416\n",
      "std of return 1082.6651409019382\n",
      "Rollout result. env: Ant-v2 , policy_type: expert , returns: 5 / 4236.738439673416 / 1082.6651409019382\n",
      "Ant-v2 - expert . It took          61  seconds.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved dir: model_Ant-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Creating window glfw\n",
      "shape of returns (5,)\n",
      "mean return 1456.4291966516435\n",
      "std of return 20.799161409530495\n",
      "Rollout result. env: Ant-v2 , policy_type: learned , returns: 5 / 1456.4291966516435 / 20.799161409530495\n",
      "Ant-v2 - learned . It took          73  seconds.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and building learned policy\n",
      "saved dir: model_Ant-v2\n",
      "loaded and built\n",
      "starting dagger  Ant-v2 2019-07-25 08:57:33.322723\n",
      "Ant-v2  observation shape:  (496130, 111) , actions shape: (496130, 8)\n",
      "loading and building expert policy for DAgger\n",
      "obs (1, 111) (1, 111)\n",
      "loaded and built for DAgger\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "train_observations shape: (497130, 111) , train_actions shape: (497130, 8)\n",
      "observations shape: (1000, 111) , actions shape: (1000, 1, 8)\n",
      "train input : train_observations shape: (397704, 111) , train_actions shape: (397704, 8)\n",
      "saved dir: model_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-25 08:59:48.727330\n",
      "Epoch: 0000 average training cost = 0.026391279 validation cost = 0.017780432 validation measure = 0.768477559 2019-07-25 09:00:01.987326\n",
      "Epoch: 0004 average training cost = 0.026288366 validation cost = 0.017761171 validation measure = 0.768728375 2019-07-25 09:00:49.947972\n",
      "Training(learning) Finished! 2019-07-25 09:00:49.948957\n",
      "Training took          61  seconds.\n",
      "train_observations shape: (498130, 111) , train_actions shape: (498130, 8)\n",
      "observations shape: (2000, 111) , actions shape: (2000, 1, 8)\n",
      "train input : train_observations shape: (398504, 111) , train_actions shape: (398504, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-25 09:03:10.632119\n",
      "Epoch: 0000 average training cost = 0.026516210 validation cost = 0.017715791 validation measure = 0.768960476 2019-07-25 09:03:24.472295\n",
      "Epoch: 0004 average training cost = 0.026429061 validation cost = 0.017706789 validation measure = 0.769077897 2019-07-25 09:04:13.799499\n",
      "Training(learning) Finished! 2019-07-25 09:04:13.799499\n",
      "Training took          63  seconds.\n",
      "train_observations shape: (499130, 111) , train_actions shape: (499130, 8)\n",
      "observations shape: (3000, 111) , actions shape: (3000, 1, 8)\n",
      "train input : train_observations shape: (399304, 111) , train_actions shape: (399304, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-25 09:06:39.649354\n",
      "Epoch: 0000 average training cost = 0.026581880 validation cost = 0.017990841 validation measure = 0.766896784 2019-07-25 09:06:50.648889\n",
      "Epoch: 0004 average training cost = 0.026570143 validation cost = 0.017990915 validation measure = 0.766895771 2019-07-25 09:07:31.572413\n",
      "Training(learning) Finished! 2019-07-25 09:07:31.573412\n",
      "Training took          51  seconds.\n",
      "train_observations shape: (500130, 111) , train_actions shape: (500130, 8)\n",
      "observations shape: (4000, 111) , actions shape: (4000, 1, 8)\n",
      "train input : train_observations shape: (400104, 111) , train_actions shape: (400104, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-25 09:09:53.076535\n",
      "Epoch: 0000 average training cost = 0.026802022 validation cost = 0.018073469 validation measure = 0.764847040 2019-07-25 09:10:02.816083\n",
      "Epoch: 0004 average training cost = 0.026799742 validation cost = 0.018056180 validation measure = 0.765072048 2019-07-25 09:10:35.793211\n",
      "Training(learning) Finished! 2019-07-25 09:10:35.793211\n",
      "Training took          42  seconds.\n",
      "train_observations shape: (501130, 111) , train_actions shape: (501130, 8)\n",
      "observations shape: (5000, 111) , actions shape: (5000, 1, 8)\n",
      "train input : train_observations shape: (400904, 111) , train_actions shape: (400904, 8)\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Learning starts. It will take some time... 2019-07-25 09:12:56.369801\n",
      "Epoch: 0000 average training cost = 0.026871502 validation cost = 0.018312205 validation measure = 0.763567328 2019-07-25 09:13:10.915891\n",
      "Epoch: 0004 average training cost = 0.026901208 validation cost = 0.018303962 validation measure = 0.763673782 2019-07-25 09:14:06.693150\n",
      "Training(learning) Finished! 2019-07-25 09:14:06.693150\n",
      "Training took          70  seconds.\n",
      "saved dir: model_dagger_Ant-v2\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-25 09:14:07.256595  and finished at  2019-07-25 09:14:07.383255\n",
      "shape of returns (5,)\n",
      "mean return 1562.6637933905422\n",
      "std of return 60.52888313910335\n",
      "saved dir: model_dagger_Ant-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Creating window glfw\n",
      "shape of returns (5,)\n",
      "mean return 1559.178639334858\n",
      "std of return 84.84065098161587\n",
      "Rollout result. env: Ant-v2 , policy_type: dagger , returns: 5 / 1559.178639334858 / 84.84065098161587\n",
      "Ant-v2 - dagger . It took        1068  seconds.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and building expert policy\n",
      "obs (1, 11) (1, 11)\n",
      "loaded and built\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Creating window glfw\n",
      "shape of returns (5,)\n",
      "mean return 3778.744256532997\n",
      "std of return 3.2459491355292407\n",
      "Rollout result. env: Hopper-v2 , policy_type: expert , returns: 5 / 3778.744256532997 / 3.2459491355292407\n",
      "Hopper-v2 - expert . It took          22  seconds.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved dir: model_Hopper-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Creating window glfw\n",
      "shape of returns (5,)\n",
      "mean return 753.8643438327778\n",
      "std of return 319.4460042712088\n",
      "Rollout result. env: Hopper-v2 , policy_type: learned , returns: 5 / 753.8643438327778 / 319.4460042712088\n",
      "Hopper-v2 - learned . It took           7  seconds.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and building learned policy\n",
      "saved dir: model_Hopper-v2\n",
      "loaded and built\n",
      "starting dagger  Hopper-v2 2019-07-25 09:16:04.689659\n",
      "Hopper-v2  observation shape:  (500000, 11) , actions shape: (500000, 3)\n",
      "loading and building expert policy for DAgger\n",
      "obs (1, 11) (1, 11)\n",
      "loaded and built for DAgger\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "train_observations shape: (500236, 11) , train_actions shape: (500236, 3)\n",
      "observations shape: (236, 11) , actions shape: (236, 1, 3)\n",
      "train input : train_observations shape: (400188, 11) , train_actions shape: (400188, 3)\n",
      "saved dir: model_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-25 09:16:09.902176\n",
      "Epoch: 0000 average training cost = 0.386448085 validation cost = 0.151185811 validation measure = 0.929909945 2019-07-25 09:16:23.027072\n",
      "Epoch: 0004 average training cost = 0.386174083 validation cost = 0.151039138 validation measure = 0.929977953 2019-07-25 09:17:12.807485\n",
      "Training(learning) Finished! 2019-07-25 09:17:12.808525\n",
      "Training took          62  seconds.\n",
      "train_observations shape: (500514, 11) , train_actions shape: (500514, 3)\n",
      "observations shape: (514, 11) , actions shape: (514, 1, 3)\n",
      "train input : train_observations shape: (400411, 11) , train_actions shape: (400411, 3)\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-25 09:17:19.234251\n",
      "Epoch: 0000 average training cost = 0.389213622 validation cost = 0.154515892 validation measure = 0.928040326 2019-07-25 09:17:30.376071\n",
      "Epoch: 0004 average training cost = 0.387691200 validation cost = 0.154083699 validation measure = 0.928241611 2019-07-25 09:18:10.142629\n",
      "Training(learning) Finished! 2019-07-25 09:18:10.142629\n",
      "Training took          50  seconds.\n",
      "train_observations shape: (500820, 11) , train_actions shape: (500820, 3)\n",
      "observations shape: (820, 11) , actions shape: (820, 1, 3)\n",
      "train input : train_observations shape: (400656, 11) , train_actions shape: (400656, 3)\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-25 09:18:17.157869\n",
      "Epoch: 0000 average training cost = 0.388378382 validation cost = 0.157523856 validation measure = 0.926762104 2019-07-25 09:18:30.881152\n",
      "Epoch: 0004 average training cost = 0.388939857 validation cost = 0.157395869 validation measure = 0.926821589 2019-07-25 09:19:23.216762\n",
      "Training(learning) Finished! 2019-07-25 09:19:23.216762\n",
      "Training took          66  seconds.\n",
      "train_observations shape: (501216, 11) , train_actions shape: (501216, 3)\n",
      "observations shape: (1216, 11) , actions shape: (1216, 1, 3)\n",
      "train input : train_observations shape: (400972, 11) , train_actions shape: (400972, 3)\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-25 09:19:32.462963\n",
      "Epoch: 0000 average training cost = 0.392518997 validation cost = 0.158592969 validation measure = 0.927031755 2019-07-25 09:19:44.858760\n",
      "Epoch: 0004 average training cost = 0.389816314 validation cost = 0.158333853 validation measure = 0.927150965 2019-07-25 09:20:29.951323\n",
      "Training(learning) Finished! 2019-07-25 09:20:29.952320\n",
      "Training took          57  seconds.\n",
      "train_observations shape: (501471, 11) , train_actions shape: (501471, 3)\n",
      "observations shape: (1471, 11) , actions shape: (1471, 1, 3)\n",
      "train input : train_observations shape: (401176, 11) , train_actions shape: (401176, 3)\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Learning starts. It will take some time... 2019-07-25 09:20:35.997864\n",
      "Epoch: 0000 average training cost = 0.393001497 validation cost = 0.159574792 validation measure = 0.925814807 2019-07-25 09:20:48.341803\n",
      "Epoch: 0004 average training cost = 0.392319143 validation cost = 0.159254774 validation measure = 0.925963581 2019-07-25 09:21:35.012955\n",
      "Training(learning) Finished! 2019-07-25 09:21:35.013953\n",
      "Training took          59  seconds.\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-25 09:21:35.573456  and finished at  2019-07-25 09:21:35.619377\n",
      "shape of returns (5,)\n",
      "mean return 785.2863676434831\n",
      "std of return 238.39293290603698\n",
      "saved dir: model_dagger_Hopper-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Creating window glfw\n",
      "shape of returns (5,)\n",
      "mean return 644.9318293290314\n",
      "std of return 276.00433028922373\n",
      "Rollout result. env: Hopper-v2 , policy_type: dagger , returns: 5 / 644.9318293290314 / 276.00433028922373\n",
      "Hopper-v2 - dagger . It took         338  seconds.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and building expert policy\n",
      "obs (1, 11) (1, 11)\n",
      "loaded and built\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Creating window glfw\n",
      "shape of returns (5,)\n",
      "mean return -4.224733309005781\n",
      "std of return 1.484114109365414\n",
      "Rollout result. env: Reacher-v2 , policy_type: expert , returns: 5 / -4.224733309005781 / 1.484114109365414\n",
      "Reacher-v2 - expert . It took           3  seconds.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved dir: model_Reacher-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Creating window glfw\n",
      "shape of returns (5,)\n",
      "mean return -10.555401295578095\n",
      "std of return 2.8969758059010666\n",
      "Rollout result. env: Reacher-v2 , policy_type: learned , returns: 5 / -10.555401295578095 / 2.8969758059010666\n",
      "Reacher-v2 - learned . It took           4  seconds.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and building learned policy\n",
      "saved dir: model_Reacher-v2\n",
      "loaded and built\n",
      "starting dagger  Reacher-v2 2019-07-25 09:22:05.693346\n",
      "Reacher-v2  observation shape:  (500000, 11) , actions shape: (500000, 2)\n",
      "loading and building expert policy for DAgger\n",
      "obs (1, 11) (1, 11)\n",
      "loaded and built for DAgger\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "train_observations shape: (500050, 11) , train_actions shape: (500050, 2)\n",
      "observations shape: (50, 11) , actions shape: (50, 1, 2)\n",
      "train input : train_observations shape: (400040, 11) , train_actions shape: (400040, 2)\n",
      "saved dir: model_Reacher-v2\n",
      "Learning starts. It will take some time... 2019-07-25 09:22:07.830201\n",
      "Epoch: 0000 average training cost = 0.005705089 validation cost = 0.004972897 validation measure = 0.366930246 2019-07-25 09:22:20.233093\n",
      "Epoch: 0004 average training cost = 0.005688434 validation cost = 0.004963896 validation measure = 0.368076205 2019-07-25 09:23:08.192222\n",
      "Training(learning) Finished! 2019-07-25 09:23:08.194216\n",
      "Training took          60  seconds.\n",
      "train_observations shape: (500100, 11) , train_actions shape: (500100, 2)\n",
      "observations shape: (100, 11) , actions shape: (100, 1, 2)\n",
      "train input : train_observations shape: (400080, 11) , train_actions shape: (400080, 2)\n",
      "saved dir: model_dagger_Reacher-v2\n",
      "Learning starts. It will take some time... 2019-07-25 09:23:10.335953\n",
      "Epoch: 0000 average training cost = 0.005672033 validation cost = 0.005091724 validation measure = 0.368225098 2019-07-25 09:23:24.119984\n",
      "Epoch: 0004 average training cost = 0.005644651 validation cost = 0.005089971 validation measure = 0.368442595 2019-07-25 09:24:18.021757\n",
      "Training(learning) Finished! 2019-07-25 09:24:18.021757\n",
      "Training took          67  seconds.\n",
      "train_observations shape: (500150, 11) , train_actions shape: (500150, 2)\n",
      "observations shape: (150, 11) , actions shape: (150, 1, 2)\n",
      "train input : train_observations shape: (400120, 11) , train_actions shape: (400120, 2)\n",
      "saved dir: model_dagger_Reacher-v2\n",
      "Learning starts. It will take some time... 2019-07-25 09:24:20.156533\n",
      "Epoch: 0000 average training cost = 0.005710172 validation cost = 0.005056530 validation measure = 0.366736054 2019-07-25 09:24:32.437810\n",
      "Epoch: 0004 average training cost = 0.005723758 validation cost = 0.005052925 validation measure = 0.367187619 2019-07-25 09:25:19.452042\n",
      "Training(learning) Finished! 2019-07-25 09:25:19.452042\n",
      "Training took          59  seconds.\n",
      "train_observations shape: (500200, 11) , train_actions shape: (500200, 2)\n",
      "observations shape: (200, 11) , actions shape: (200, 1, 2)\n",
      "train input : train_observations shape: (400160, 11) , train_actions shape: (400160, 2)\n",
      "saved dir: model_dagger_Reacher-v2\n",
      "Learning starts. It will take some time... 2019-07-25 09:25:21.581385\n",
      "Epoch: 0000 average training cost = 0.005687879 validation cost = 0.005080642 validation measure = 0.371581197 2019-07-25 09:25:34.526364\n",
      "Epoch: 0004 average training cost = 0.005676069 validation cost = 0.005081041 validation measure = 0.371531904 2019-07-25 09:26:24.721161\n",
      "Training(learning) Finished! 2019-07-25 09:26:24.722205\n",
      "Training took          63  seconds.\n",
      "train_observations shape: (500250, 11) , train_actions shape: (500250, 2)\n",
      "observations shape: (250, 11) , actions shape: (250, 1, 2)\n",
      "train input : train_observations shape: (400200, 11) , train_actions shape: (400200, 2)\n",
      "saved dir: model_dagger_Reacher-v2\n",
      "Learning starts. It will take some time... 2019-07-25 09:26:26.828759\n",
      "Epoch: 0000 average training cost = 0.005680248 validation cost = 0.005115680 validation measure = 0.368542612 2019-07-25 09:26:39.087047\n",
      "Epoch: 0004 average training cost = 0.005667682 validation cost = 0.005113396 validation measure = 0.368824482 2019-07-25 09:27:27.691465\n",
      "Training(learning) Finished! 2019-07-25 09:27:27.694455\n",
      "Training took          60  seconds.\n",
      "saved dir: model_dagger_Reacher-v2\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-25 09:27:28.205798  and finished at  2019-07-25 09:27:28.250679\n",
      "shape of returns (5,)\n",
      "mean return -9.839025648538719\n",
      "std of return 3.625648117679956\n",
      "saved dir: model_dagger_Reacher-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Creating window glfw\n",
      "shape of returns (5,)\n",
      "mean return -12.051469320512117\n",
      "std of return 2.3490634142637847\n",
      "Rollout result. env: Reacher-v2 , policy_type: dagger , returns: 5 / -12.051469320512117 / 2.3490634142637847\n",
      "Reacher-v2 - dagger . It took         326  seconds.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and building expert policy\n",
      "obs (1, 17) (1, 17)\n",
      "loaded and built\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Creating window glfw\n",
      "shape of returns (5,)\n",
      "mean return 4129.324865018089\n",
      "std of return 48.39784988679189\n",
      "Rollout result. env: HalfCheetah-v2 , policy_type: expert , returns: 5 / 4129.324865018089 / 48.39784988679189\n",
      "HalfCheetah-v2 - expert . It took          62  seconds.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved dir: model_HalfCheetah-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Creating window glfw\n",
      "shape of returns (5,)\n",
      "mean return 2551.8196935074275\n",
      "std of return 183.52685814516974\n",
      "Rollout result. env: HalfCheetah-v2 , policy_type: learned , returns: 5 / 2551.8196935074275 / 183.52685814516974\n",
      "HalfCheetah-v2 - learned . It took          65  seconds.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and building learned policy\n",
      "saved dir: model_HalfCheetah-v2\n",
      "loaded and built\n",
      "starting dagger  HalfCheetah-v2 2019-07-25 09:29:53.089264\n",
      "HalfCheetah-v2  observation shape:  (500000, 17) , actions shape: (500000, 6)\n",
      "loading and building expert policy for DAgger\n",
      "obs (1, 17) (1, 17)\n",
      "loaded and built for DAgger\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "train_observations shape: (501000, 17) , train_actions shape: (501000, 6)\n",
      "observations shape: (1000, 17) , actions shape: (1000, 1, 6)\n",
      "train input : train_observations shape: (400800, 17) , train_actions shape: (400800, 6)\n",
      "saved dir: model_HalfCheetah-v2\n",
      "Learning starts. It will take some time... 2019-07-25 09:30:20.509413\n",
      "Epoch: 0000 average training cost = 0.110685512 validation cost = 0.058326729 validation measure = 0.893301785 2019-07-25 09:30:33.796974\n",
      "Epoch: 0004 average training cost = 0.110264391 validation cost = 0.058330651 validation measure = 0.893294573 2019-07-25 09:31:23.815414\n",
      "Training(learning) Finished! 2019-07-25 09:31:23.816411\n",
      "Training took          63  seconds.\n",
      "train_observations shape: (502000, 17) , train_actions shape: (502000, 6)\n",
      "observations shape: (2000, 17) , actions shape: (2000, 1, 6)\n",
      "train input : train_observations shape: (401600, 17) , train_actions shape: (401600, 6)\n",
      "saved dir: model_dagger_HalfCheetah-v2\n",
      "Learning starts. It will take some time... 2019-07-25 09:31:55.013868\n",
      "Epoch: 0000 average training cost = 0.110458620 validation cost = 0.058191486 validation measure = 0.893820465 2019-07-25 09:32:08.464578\n",
      "Epoch: 0004 average training cost = 0.110786006 validation cost = 0.058138736 validation measure = 0.893916726 2019-07-25 09:32:58.854807\n",
      "Training(learning) Finished! 2019-07-25 09:32:58.855805\n",
      "Training took          63  seconds.\n",
      "train_observations shape: (503000, 17) , train_actions shape: (503000, 6)\n",
      "observations shape: (3000, 17) , actions shape: (3000, 1, 6)\n",
      "train input : train_observations shape: (402400, 17) , train_actions shape: (402400, 6)\n",
      "saved dir: model_dagger_HalfCheetah-v2\n",
      "Learning starts. It will take some time... 2019-07-25 09:33:29.873242\n",
      "Epoch: 0000 average training cost = 0.111063175 validation cost = 0.058156725 validation measure = 0.892942846 2019-07-25 09:33:42.297053\n",
      "Epoch: 0004 average training cost = 0.110591136 validation cost = 0.058126904 validation measure = 0.892997742 2019-07-25 09:34:28.698256\n",
      "Training(learning) Finished! 2019-07-25 09:34:28.699255\n",
      "Training took          58  seconds.\n",
      "train_observations shape: (504000, 17) , train_actions shape: (504000, 6)\n",
      "observations shape: (4000, 17) , actions shape: (4000, 1, 6)\n",
      "train input : train_observations shape: (403200, 17) , train_actions shape: (403200, 6)\n",
      "saved dir: model_dagger_HalfCheetah-v2\n",
      "Learning starts. It will take some time... 2019-07-25 09:34:59.693670\n",
      "Epoch: 0000 average training cost = 0.110577047 validation cost = 0.057993237 validation measure = 0.893710077 2019-07-25 09:35:13.190032\n",
      "Epoch: 0004 average training cost = 0.110436544 validation cost = 0.058007207 validation measure = 0.893684447 2019-07-25 09:36:03.434596\n",
      "Training(learning) Finished! 2019-07-25 09:36:03.434596\n",
      "Training took          63  seconds.\n",
      "train_observations shape: (505000, 17) , train_actions shape: (505000, 6)\n",
      "observations shape: (5000, 17) , actions shape: (5000, 1, 6)\n",
      "train input : train_observations shape: (404000, 17) , train_actions shape: (404000, 6)\n",
      "saved dir: model_dagger_HalfCheetah-v2\n",
      "Learning starts. It will take some time... 2019-07-25 09:36:35.451823\n",
      "Epoch: 0000 average training cost = 0.110728957 validation cost = 0.058239296 validation measure = 0.893351316 2019-07-25 09:36:48.688918\n",
      "Epoch: 0004 average training cost = 0.110518850 validation cost = 0.058187481 validation measure = 0.893446207 2019-07-25 09:37:39.250292\n",
      "Training(learning) Finished! 2019-07-25 09:37:39.250292\n",
      "Training took          63  seconds.\n",
      "saved dir: model_dagger_HalfCheetah-v2\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-25 09:37:39.772849  and finished at  2019-07-25 09:37:39.829737\n",
      "shape of returns (5,)\n",
      "mean return 2638.2008821687677\n",
      "std of return 140.33568978630402\n",
      "saved dir: model_dagger_HalfCheetah-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Creating window glfw\n",
      "shape of returns (5,)\n",
      "mean return 2604.9674767930237\n",
      "std of return 171.31381009704205\n",
      "Rollout result. env: HalfCheetah-v2 , policy_type: dagger , returns: 5 / 2604.9674767930237 / 171.31381009704205\n",
      "HalfCheetah-v2 - dagger . It took         534  seconds.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and building expert policy\n",
      "obs (1, 376) (1, 376)\n",
      "loaded and built\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Creating window glfw\n",
      "shape of returns (5,)\n",
      "mean return 10401.182975388374\n",
      "std of return 22.169635619860017\n",
      "Rollout result. env: Humanoid-v2 , policy_type: expert , returns: 5 / 10401.182975388374 / 22.169635619860017\n",
      "Humanoid-v2 - expert . It took          34  seconds.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved dir: model_Humanoid-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Creating window glfw\n",
      "shape of returns (5,)\n",
      "mean return 284.027115344265\n",
      "std of return 12.980992706710655\n",
      "Rollout result. env: Humanoid-v2 , policy_type: learned , returns: 5 / 284.027115344265 / 12.980992706710655\n",
      "Humanoid-v2 - learned . It took           4  seconds.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and building learned policy\n",
      "saved dir: model_Humanoid-v2\n",
      "loaded and built\n",
      "starting dagger  Humanoid-v2 2019-07-25 09:39:38.169911\n",
      "Humanoid-v2  observation shape:  (499084, 376) , actions shape: (499084, 17)\n",
      "loading and building expert policy for DAgger\n",
      "obs (1, 376) (1, 376)\n",
      "loaded and built for DAgger\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "train_observations shape: (499142, 376) , train_actions shape: (499142, 17)\n",
      "observations shape: (58, 376) , actions shape: (58, 1, 17)\n",
      "train input : train_observations shape: (399313, 376) , train_actions shape: (399313, 17)\n",
      "saved dir: model_Humanoid-v2\n",
      "Learning starts. It will take some time... 2019-07-25 09:40:08.528859\n",
      "Epoch: 0000 average training cost = 0.181445137 validation cost = 0.104215570 validation measure = 0.891152859 2019-07-25 09:40:22.868708\n",
      "Epoch: 0004 average training cost = 0.181161374 validation cost = 0.104181349 validation measure = 0.891188622 2019-07-25 09:41:15.957682\n",
      "Training(learning) Finished! 2019-07-25 09:41:15.957682\n",
      "Training took          67  seconds.\n",
      "train_observations shape: (499194, 376) , train_actions shape: (499194, 17)\n",
      "observations shape: (110, 376) , actions shape: (110, 1, 17)\n",
      "train input : train_observations shape: (399355, 376) , train_actions shape: (399355, 17)\n",
      "saved dir: model_dagger_Humanoid-v2\n",
      "Learning starts. It will take some time... 2019-07-25 09:41:43.325098\n",
      "Epoch: 0000 average training cost = 0.181257486 validation cost = 0.104020372 validation measure = 0.891323686 2019-07-25 09:41:58.813928\n",
      "Epoch: 0004 average training cost = 0.181230560 validation cost = 0.103990741 validation measure = 0.891354680 2019-07-25 09:42:57.245496\n",
      "Training(learning) Finished! 2019-07-25 09:42:57.245496\n",
      "Training took          73  seconds.\n",
      "train_observations shape: (499254, 376) , train_actions shape: (499254, 17)\n",
      "observations shape: (170, 376) , actions shape: (170, 1, 17)\n",
      "train input : train_observations shape: (399403, 376) , train_actions shape: (399403, 17)\n",
      "saved dir: model_dagger_Humanoid-v2\n",
      "Learning starts. It will take some time... 2019-07-25 09:43:27.856059\n",
      "Epoch: 0000 average training cost = 0.180547714 validation cost = 0.105050460 validation measure = 0.890620410 2019-07-25 09:43:42.101472\n",
      "Epoch: 0004 average training cost = 0.180752113 validation cost = 0.105170861 validation measure = 0.890495062 2019-07-25 09:44:38.648864\n",
      "Training(learning) Finished! 2019-07-25 09:44:38.648864\n",
      "Training took          70  seconds.\n",
      "train_observations shape: (499309, 376) , train_actions shape: (499309, 17)\n",
      "observations shape: (225, 376) , actions shape: (225, 1, 17)\n",
      "train input : train_observations shape: (399447, 376) , train_actions shape: (399447, 17)\n",
      "saved dir: model_dagger_Humanoid-v2\n",
      "Learning starts. It will take some time... 2019-07-25 09:45:07.023083\n",
      "Epoch: 0000 average training cost = 0.181028366 validation cost = 0.104538120 validation measure = 0.890778601 2019-07-25 09:45:22.997718\n",
      "Epoch: 0004 average training cost = 0.181040242 validation cost = 0.104503103 validation measure = 0.890815198 2019-07-25 09:46:24.016494\n",
      "Training(learning) Finished! 2019-07-25 09:46:24.017492\n",
      "Training took          76  seconds.\n",
      "train_observations shape: (499361, 376) , train_actions shape: (499361, 17)\n",
      "observations shape: (277, 376) , actions shape: (277, 1, 17)\n",
      "train input : train_observations shape: (399488, 376) , train_actions shape: (399488, 17)\n",
      "saved dir: model_dagger_Humanoid-v2\n",
      "Learning starts. It will take some time... 2019-07-25 09:46:51.265566\n",
      "Epoch: 0000 average training cost = 0.181279421 validation cost = 0.104572102 validation measure = 0.890429318 2019-07-25 09:47:05.978482\n",
      "Epoch: 0004 average training cost = 0.181328341 validation cost = 0.104539491 validation measure = 0.890463471 2019-07-25 09:48:00.601797\n",
      "Training(learning) Finished! 2019-07-25 09:48:00.602795\n",
      "Training took          69  seconds.\n",
      "saved dir: model_dagger_Humanoid-v2\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-25 09:48:01.310855  and finished at  2019-07-25 09:48:01.644009\n",
      "shape of returns (5,)\n",
      "mean return 299.175576958036\n",
      "std of return 18.442137388705994\n",
      "saved dir: model_dagger_Humanoid-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Creating window glfw\n",
      "shape of returns (5,)\n",
      "mean return 288.76425126703907\n",
      "std of return 14.721469881940964\n",
      "Rollout result. env: Humanoid-v2 , policy_type: dagger , returns: 5 / 288.76425126703907 / 14.721469881940964\n",
      "Humanoid-v2 - dagger . It took         509  seconds.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and building expert policy\n",
      "obs (1, 17) (1, 17)\n",
      "loaded and built\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Creating window glfw\n",
      "shape of returns (5,)\n",
      "mean return 5519.144750657926\n",
      "std of return 62.62180384508529\n",
      "Rollout result. env: Walker2d-v2 , policy_type: expert , returns: 5 / 5519.144750657926 / 62.62180384508529\n",
      "Walker2d-v2 - expert . It took          23  seconds.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved dir: model_Walker2d-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Creating window glfw\n",
      "shape of returns (5,)\n",
      "mean return 552.7175979122823\n",
      "std of return 409.13333953431726\n",
      "Rollout result. env: Walker2d-v2 , policy_type: learned , returns: 5 / 552.7175979122823 / 409.13333953431726\n",
      "Walker2d-v2 - learned . It took           9  seconds.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading and building learned policy\n",
      "saved dir: model_Walker2d-v2\n",
      "loaded and built\n",
      "starting dagger  Walker2d-v2 2019-07-25 09:56:26.375748\n",
      "Walker2d-v2  observation shape:  (499696, 17) , actions shape: (499696, 6)\n",
      "loading and building expert policy for DAgger\n",
      "obs (1, 17) (1, 17)\n",
      "loaded and built for DAgger\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "train_observations shape: (500696, 17) , train_actions shape: (500696, 6)\n",
      "observations shape: (1000, 17) , actions shape: (1000, 1, 6)\n",
      "train input : train_observations shape: (400556, 17) , train_actions shape: (400556, 6)\n",
      "saved dir: model_Walker2d-v2\n",
      "Learning starts. It will take some time... 2019-07-25 09:56:54.025758\n",
      "Epoch: 0000 average training cost = 0.319572836 validation cost = 0.194618776 validation measure = 0.803772926 2019-07-25 09:57:08.080753\n",
      "Epoch: 0004 average training cost = 0.319365203 validation cost = 0.194537848 validation measure = 0.803854465 2019-07-25 09:58:01.656037\n",
      "Training(learning) Finished! 2019-07-25 09:58:01.658019\n",
      "Training took          67  seconds.\n",
      "train_observations shape: (500875, 17) , train_actions shape: (500875, 6)\n",
      "observations shape: (1179, 17) , actions shape: (1179, 1, 6)\n",
      "train input : train_observations shape: (400700, 17) , train_actions shape: (400700, 6)\n",
      "saved dir: model_dagger_Walker2d-v2\n",
      "Learning starts. It will take some time... 2019-07-25 09:58:08.276314\n",
      "Epoch: 0000 average training cost = 0.320089459 validation cost = 0.192778975 validation measure = 0.805357218 2019-07-25 09:58:21.929794\n",
      "Epoch: 0004 average training cost = 0.319626570 validation cost = 0.192757517 validation measure = 0.805378914 2019-07-25 09:59:13.745930\n",
      "Training(learning) Finished! 2019-07-25 09:59:13.747934\n",
      "Training took          65  seconds.\n",
      "train_observations shape: (501047, 17) , train_actions shape: (501047, 6)\n",
      "observations shape: (1351, 17) , actions shape: (1351, 1, 6)\n",
      "train input : train_observations shape: (400837, 17) , train_actions shape: (400837, 6)\n",
      "saved dir: model_dagger_Walker2d-v2\n",
      "Learning starts. It will take some time... 2019-07-25 09:59:20.213080\n",
      "Epoch: 0000 average training cost = 0.320434541 validation cost = 0.193656698 validation measure = 0.804194033 2019-07-25 09:59:32.203158\n",
      "Epoch: 0004 average training cost = 0.319963038 validation cost = 0.193565458 validation measure = 0.804286301 2019-07-25 10:00:03.982222\n",
      "Training(learning) Finished! 2019-07-25 10:00:03.983219\n",
      "Training took          43  seconds.\n",
      "train_observations shape: (501220, 17) , train_actions shape: (501220, 6)\n",
      "observations shape: (1524, 17) , actions shape: (1524, 1, 6)\n",
      "train input : train_observations shape: (400976, 17) , train_actions shape: (400976, 6)\n",
      "saved dir: model_dagger_Walker2d-v2\n",
      "Learning starts. It will take some time... 2019-07-25 10:00:10.388086\n",
      "Epoch: 0000 average training cost = 0.319788933 validation cost = 0.193888217 validation measure = 0.804766655 2019-07-25 10:00:20.395535\n",
      "Epoch: 0004 average training cost = 0.319703251 validation cost = 0.193902731 validation measure = 0.804751992 2019-07-25 10:00:51.685935\n",
      "Training(learning) Finished! 2019-07-25 10:00:51.686933\n",
      "Training took          41  seconds.\n",
      "train_observations shape: (502220, 17) , train_actions shape: (502220, 6)\n",
      "observations shape: (2524, 17) , actions shape: (2524, 1, 6)\n",
      "train input : train_observations shape: (401776, 17) , train_actions shape: (401776, 6)\n",
      "saved dir: model_dagger_Walker2d-v2\n",
      "Learning starts. It will take some time... 2019-07-25 10:01:23.635046\n",
      "Epoch: 0000 average training cost = 0.321108341 validation cost = 0.193412259 validation measure = 0.804420114 2019-07-25 10:01:30.115957\n",
      "Epoch: 0004 average training cost = 0.320174783 validation cost = 0.193150088 validation measure = 0.804685235 2019-07-25 10:02:00.937520\n",
      "Training(learning) Finished! 2019-07-25 10:02:00.939515\n",
      "Training took          37  seconds.\n",
      "saved dir: model_dagger_Walker2d-v2\n",
      "Prediction took           0  seconds.\n",
      "Started at  2019-07-25 10:02:01.370365  and finished at  2019-07-25 10:02:01.425218\n",
      "shape of returns (5,)\n",
      "mean return 745.0056735059414\n",
      "std of return 511.4077744750482\n",
      "saved dir: model_dagger_Walker2d-v2\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n",
      "Creating window glfw\n",
      "shape of returns (5,)\n",
      "mean return 744.7198638776975\n",
      "std of return 480.094976747742\n",
      "Rollout result. env: Walker2d-v2 , policy_type: dagger , returns: 5 / 744.7198638776975 / 480.094976747742\n",
      "Walker2d-v2 - dagger . It took         347  seconds.\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue... \n"
     ]
    }
   ],
   "source": [
    "# rollout and check\n",
    "from time import sleep\n",
    "import pandas as pd\n",
    "import glfw\n",
    "\n",
    "max_timesteps = None\n",
    "num_rollouts = 5\n",
    "\n",
    "df2 = pd.DataFrame(columns=['gymenv', 'policy_type', 'rollouts', 'mean', 'std'])\n",
    "\n",
    "def close_mujoco_window(win) :\n",
    "    if win.unwrapped.viewer is not None :\n",
    "        glfw.destroy_window(win.unwrapped.viewer.window)\n",
    "        win.unwrapped.viewer = None\n",
    "    \n",
    "for gym_env in gym_envs :\n",
    "    for expert_policy in [ True, False] :\n",
    "        start_time = dt.datetime.now()\n",
    "        rollout_data, policy_type, opengym_win = rollout_by_policy(gym_env, max_timesteps, num_rollouts,\n",
    "                                                                  policy_fn=None if expert_policy else load_learned_policy_fn(gym_env),\n",
    "                                                                  render=True)\n",
    "        returns = rollout_data['returns']\n",
    "        end_time = dt.datetime.now()\n",
    "        print('Rollout result. env:', gym_env, ', policy_type:', policy_type, ', returns:', len(returns), '/', np.mean(returns), '/', np.std(returns))\n",
    "        df2 = df2.append({'gymenv':gym_env, 'policy_type':policy_type, 'rollouts': len(returns), 'mean': np.mean(returns), 'std': np.std(returns)}, ignore_index=True)\n",
    "        print(gym_env, '-', policy_type, '. It took ', '%10d' % ((end_time - start_time).total_seconds()), ' seconds.')\n",
    "        input(\"Press Enter to continue...\")\n",
    "        close_mujoco_window(opengym_win)\n",
    "\n",
    "    start_time = dt.datetime.now()\n",
    "    rollout_data, policy_type, opengym_win = rollout_by_dagger(gym_env, max_timesteps, num_rollouts, num_epochs=5,\n",
    "                                                              render=True)\n",
    "    returns = rollout_data['returns']\n",
    "    end_time = dt.datetime.now()\n",
    "    print('Rollout result. env:', gym_env, ', policy_type:', policy_type, ', returns:', len(returns), '/', np.mean(returns), '/', np.std(returns))\n",
    "    df2 = df2.append({'gymenv':gym_env, 'policy_type':policy_type, 'rollouts': len(returns), 'mean': np.mean(returns), 'std': np.std(returns)}, ignore_index=True)\n",
    "    print(gym_env, '-', policy_type, '. It took ', '%10d' % ((end_time - start_time).total_seconds()), ' seconds.')\n",
    "    input(\"Press Enter to continue...\")\n",
    "    close_mujoco_window(opengym_win)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gymenv</th>\n",
       "      <th>policy_type</th>\n",
       "      <th>rollouts</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ant-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>5</td>\n",
       "      <td>4236.738440</td>\n",
       "      <td>1082.665141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Ant-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>5</td>\n",
       "      <td>1456.429197</td>\n",
       "      <td>20.799161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ant-v2</td>\n",
       "      <td>dagger</td>\n",
       "      <td>5</td>\n",
       "      <td>1559.178639</td>\n",
       "      <td>84.840651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Hopper-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>5</td>\n",
       "      <td>3778.744257</td>\n",
       "      <td>3.245949</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Hopper-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>5</td>\n",
       "      <td>753.864344</td>\n",
       "      <td>319.446004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Hopper-v2</td>\n",
       "      <td>dagger</td>\n",
       "      <td>5</td>\n",
       "      <td>644.931829</td>\n",
       "      <td>276.004330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Reacher-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>5</td>\n",
       "      <td>-4.224733</td>\n",
       "      <td>1.484114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Reacher-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>5</td>\n",
       "      <td>-10.555401</td>\n",
       "      <td>2.896976</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Reacher-v2</td>\n",
       "      <td>dagger</td>\n",
       "      <td>5</td>\n",
       "      <td>-12.051469</td>\n",
       "      <td>2.349063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>HalfCheetah-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>5</td>\n",
       "      <td>4129.324865</td>\n",
       "      <td>48.397850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>HalfCheetah-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>5</td>\n",
       "      <td>2551.819694</td>\n",
       "      <td>183.526858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>HalfCheetah-v2</td>\n",
       "      <td>dagger</td>\n",
       "      <td>5</td>\n",
       "      <td>2604.967477</td>\n",
       "      <td>171.313810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Humanoid-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>5</td>\n",
       "      <td>10401.182975</td>\n",
       "      <td>22.169636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Humanoid-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>5</td>\n",
       "      <td>284.027115</td>\n",
       "      <td>12.980993</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Humanoid-v2</td>\n",
       "      <td>dagger</td>\n",
       "      <td>5</td>\n",
       "      <td>288.764251</td>\n",
       "      <td>14.721470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Walker2d-v2</td>\n",
       "      <td>expert</td>\n",
       "      <td>5</td>\n",
       "      <td>5519.144751</td>\n",
       "      <td>62.621804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Walker2d-v2</td>\n",
       "      <td>learned</td>\n",
       "      <td>5</td>\n",
       "      <td>552.717598</td>\n",
       "      <td>409.133340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Walker2d-v2</td>\n",
       "      <td>dagger</td>\n",
       "      <td>5</td>\n",
       "      <td>744.719864</td>\n",
       "      <td>480.094977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            gymenv policy_type rollouts          mean          std\n",
       "0           Ant-v2      expert        5   4236.738440  1082.665141\n",
       "1           Ant-v2     learned        5   1456.429197    20.799161\n",
       "2           Ant-v2      dagger        5   1559.178639    84.840651\n",
       "3        Hopper-v2      expert        5   3778.744257     3.245949\n",
       "4        Hopper-v2     learned        5    753.864344   319.446004\n",
       "5        Hopper-v2      dagger        5    644.931829   276.004330\n",
       "6       Reacher-v2      expert        5     -4.224733     1.484114\n",
       "7       Reacher-v2     learned        5    -10.555401     2.896976\n",
       "8       Reacher-v2      dagger        5    -12.051469     2.349063\n",
       "9   HalfCheetah-v2      expert        5   4129.324865    48.397850\n",
       "10  HalfCheetah-v2     learned        5   2551.819694   183.526858\n",
       "11  HalfCheetah-v2      dagger        5   2604.967477   171.313810\n",
       "12     Humanoid-v2      expert        5  10401.182975    22.169636\n",
       "13     Humanoid-v2     learned        5    284.027115    12.980993\n",
       "14     Humanoid-v2      dagger        5    288.764251    14.721470\n",
       "15     Walker2d-v2      expert        5   5519.144751    62.621804\n",
       "16     Walker2d-v2     learned        5    552.717598   409.133340\n",
       "17     Walker2d-v2      dagger        5    744.719864   480.094977"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2.head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "source": [
    "##  Bonus: Alternative Policy Architectures\n",
    "\n",
    "1. (Optional) Experiment with a different policy architecture, e.g. using recurrence or changing the size or nonlinearities used.\n",
    "\n",
    "Compare performance between your new and original policy architectures using behavioral cloning and/or DAgger,\n",
    "and report your results in the same form as above, with a caption describing what you did.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "autoscroll": false,
    "collapsed": false,
    "ein.hycell": false,
    "ein.tags": "worksheet-0",
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (tf1)",
   "language": "python",
   "name": "tf1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "name": "hw1.ipynb"
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
